,doc.id,title,citeulike.id,raw.title,raw.abstract
0,1,the metabolic world of escherichia coli is not small,42.0,The metabolic world of Escherichia coli is not small,"To elucidate the organizational and evolutionary principles of the metabolism of living organisms, recent studies have addressed the graph-theoretic analysis of large biochemical networks responsible for the synthesis and degradation of cellular building blocks [Jeong, H., Tombor, B., Albert, R., Oltvai, Z. N. \& Barab\{\'a\}si, A. L. (2000) Nature 407, 651-654; Wagner, A. \& Fell, D. A. (2001) Proc. R. Soc. London Ser. B 268, 1803-1810; and Ma, H.-W. \& Zeng, A.-P. (2003) Bioinformatics 19, 270-277]. In such studies, the global properties of the network are computed by considering enzymatic reactions as links between metabolites. However, the pathways computed in this manner do not conserve their structural moieties and therefore do not correspond to biochemical pathways on the traditional metabolic map. In this work, we reassessed earlier results by digitizing carbon atomic traces in metabolic reactions annotated for Escherichia coli. Our analysis revealed that the average path length of its metabolism is much longer than previously thought and that the metabolic world of this organism is not small in terms of biosynthesis and degradation."
1,2,reverse engineering of biological complexity,43.0,Reverse Engineering of Biological Complexity,"Advanced technologies and biology have extremely different physical implementations, but they are far more alike in systems-level organization than is widely appreciated. {C}onvergent evolution in both domains produces modular architectures that are composed of elaborate hierarchies of protocols and layers of feedback regulation, are driven by demand for robustness to uncertain environments, and use often imprecise components. {T}his complexity may be largely hidden in idealized laboratory settings and in normal operation, becoming conspicuous only when contributing to rare cascading failures. {T}hese puzzling and paradoxical features are neither accidental nor artificial, but derive from a deep and necessary interplay between complexity and robustness, modularity, feedback, and fragility. {T}his review describes insights from engineering theory and practice that can shed some light on biological complexity."
2,3,exploring complex networks,44.0,Exploring complex networks,"The study of networks pervades all of science, from neurobiology to statistical physics. {T}he most basic issues are structural: how does one characterize the wiring diagram of a food web or the {I}nternet or the metabolic network of the bacterium {E}scherichia coli? {A}re there any unifying principles underlying their topology? {F}rom the perspective of nonlinear dynamics, we would also like to understand how an enormous network of interacting dynamical systems-be they neurons, power stations or lasers-will behave collectively, given their individual dynamics and coupling architecture. {R}esearchers are only now beginning to unravel the structure and dynamics of complex networks."
3,4,comparative assessment of largescale data sets of proteinprotein interactions,46.0,Comparative assessment of large-scale data sets of protein-protein interactions.,"Comprehensive protein protein interaction maps promise to reveal many aspects of the complex regulatory network underlying cellular function. Recently, large-scale approaches have predicted many new protein interactions in yeast. To measure their accuracy and potential as well as to identify biases, strengths and weaknesses, we compare the methods with each other and with a reference set of previously reported protein interactions."
4,5,navigation in a small world,47.0,Navigation in a small world,"The small-world phenomenon — the principle that most of us are linked by short chains of acquaintances — was first investigated as a question in sociology1, 2 and is a feature of a range of networks arising in nature and technology3, 4, 5. Experimental study of the phenomenon1 revealed that it has two fundamental components: first, such short chains are ubiquitous, and second, individuals operating with purely local information are very adept at finding these chains. The first issue has been analysed2, 3, 4, and here I investigate the second by modelling how individuals can find short chains in a large social network."
5,6,random graphs with arbitrary degree distributions and their applications,48.0,Random graphs with arbitrary degree distributions and their applications.,"Recent work on the structure of social networks and the internet has focussed attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph."
6,7,artificial gene networks for objective comparison of analysis algorithms,49.0,Artificial gene networks for objective comparison of analysis  algorithms,"Motivation: Large-scale gene expression profiling generates data sets that are rich in observed features but poor in numbers of observations. The analysis of such data sets is a challenge that has been object of vigorous research. The algorithms in use for this purpose have been poorly documented and rarely compared objectively, posing a problem of uncertainty about the outcomes of the analyses. One way to objectively test such analysis algorithms is to apply them on computational gene network models for which the mechanisms are completely know.  Results: We present a system that generates random artificial   gene networks according to well-defined topological and kinetic   properties. These are used to run in silico experiments simulating   real laboratory microarray experiments. Noise with controlled   properties is added to the simulation results several times   emulating measurement replicates, before expression ratios are calculated.  Availability: The data sets and kinetic models described here are available from http://www.vbi.vt.edu/~mendes/AGN/as  biochemical dynamic models in SBML and Gepasi formats.  Contact: mendes@vt.edu 10.1093/bioinformatics/btg1069"
7,8,the segment polarity network is a robust developmental module,50.0,The segment polarity network is a robust developmental module,"All insects possess homologous segments, but segment specification differs radically among insect orders. {I}n {D}rosophila, maternal morphogens control the patterned activation of gap genes, which encode transcriptional regulators that shape the patterned expression of pair-rule genes. {T}his patterning cascade takes place before cellularization. {P}air-rule gene products subsequently 'imprint' segment polarity genes with reiterated patterns, thus defining the primordial segments. {T}his mechanism must be greatly modified in insect groups in which many segments emerge only after cellularization. {I}n beetles and parasitic wasps, for instance, pair-rule homologues are expressed in patterns consistent with roles during segmentation, but these patterns emerge within cellular fields. {I}n contrast, although in locusts pair-rule homologues may not control segmentation, some segment polarity genes and their interactions are conserved. {P}erhaps segmentation is modular, with each module autonomously expressing a characteristic intrinsic behaviour in response to transient stimuli. {I}f so, evolution could rearrange inputs to modules without changing their intrinsic behaviours. {H}ere we suggest, using computer simulations, that the {D}rosophila segment polarity genes constitute such a module, and that this module is resistant to variations in the kinetic constants that govern its behaviour."
8,9,the evolutionary origin of complex features,52.0,The evolutionary origin of complex features.,"A long-standing challenge to evolutionary theory has been whether it can explain the origin of complex organismal features. {W}e examined this issue using digital organisms--computer programs that self-replicate, mutate, compete and evolve. {P}opulations of digital organisms often evolved the ability to perform complex logic functions requiring the coordinated execution of many genomic instructions. {C}omplex functions evolved by building on simpler functions that had evolved earlier, provided that these were also selectively favoured. {H}owever, no particular intermediate stage was essential for evolving complex functions. {T}he first genotypes able to perform complex functions differed from their non-performing parents by only one or two mutations, but differed from the ancestor by many mutations that were also crucial to the new functions. {I}n some cases, mutations that were deleterious when they appeared served as stepping-stones in the evolution of complex features. {T}hese findings show how complex functions can originate by random mutation and natural selection."
9,10,early language acquisition cracking the speech code,60.0,Early language acquisition: cracking the speech code.,"Infants learn language with remarkable speed, but how they do it remains a mystery. New data show that infants use computational strategies to detect the statistical and prosodic patterns in language input, and that this leads to the discovery of phonemes and words. Social interaction with another human being affects speech learning in a way that resembles communicative learning in songbirds. The brain's commitment to the statistical and prosodic patterns that are experienced early in life might help to explain the long-standing puzzle of why infants are better language learners than adults. Successful learning by infants, as well as constraints on that learning, are changing theories of language acquisition."
10,11,organization development and function of complex brain networks,61.0,"Organization, development and function of complex brain networks"," Recent research has revealed general principles in the structural and functional organization of complex networks which are shared by various natural, social and technological systems. This review examines these principles as applied to the organization, development and function of complex brain networks. Specifically, we examine the structural properties of large-scale anatomical and functional brain networks and discuss how they might arise in the course of network growth and rewiring. Moreover, we examine the relationship between the structural substrate of neuroanatomy and more dynamic functional and effective connectivity patterns that underlie human cognition. We suggest that network analysis offers new fundamental insights into global and integrative aspects of brain function, including the origin of flexible and coherent cognitive states within the neural architecture."
11,12,motifs in brain networks,62.0,Motifs in brain networks.,"Complex brains have evolved a highly efficient network architecture whose structural connectivity is capable of generating a large repertoire of functional states. We detect characteristic network building blocks ( structural and functional motifs) in neuroanatomical data sets and identify a small set of structural motifs that occur in significantly increased numbers. Our analysis suggests the hypothesis that brain networks maximize both the number and the diversity of functional motifs, while the repertoire of structural motifs remains small. Using functional motif number as a cost function in an optimization algorithm, we obtain network topologies that resemble real brain networks across a broad spectrum of structural measures, including small-world attributes. These results are consistent with the hypothesis that highly evolved neural architectures are organized to maximize functional repertoires and to support highly efficient integration of information."
12,13,diffusion on complex networks a way to probe their large scale topological structures,97.0,Diffusion on Complex Networks : A way to probe their large scale topological structures,"A diffusion process on complex networks is introduced in order to uncover their large scale topological structures. This is achieved by focusing on the slowest decaying diffusive modes of the network. The proposed procedure is applied to real-world networks like a friendship network of known modular structure, and an Internet routing network. For the friendship network, its known structure is well reproduced. In case of the Internet, where the structure is far less well-known, one indeed finds a modular structure, and modules can roughly be associated with individual countries. Quantitatively the modular structure of the Internet manifests itself in an approximately 10 times larger participation ratio of its slowest decaying modes as compared to the null model -- a random scale-free network. The extreme edges of the Internet are found to correspond to Russian and US military sites."
13,14,topological generalizations of network motifs,98.0,Topological Generalizations of network motifs,"Biological and technological networks contain patterns, termed network motifs, which occur far more often than in randomized networks. {N}etwork motifs were suggested to be elementary building blocks that carry out key functions in the network. {I}t is of interest to understand how network motifs combine to form larger structures. {T}o address this, we present a systematic approach to define ""motif generalizations"": families of motifs of different sizes that share a common architectural theme. {T}o define motif generalizations, we first define ""roles"" in a subgraph according to structural equivalence. {F}or example, the feedforward loop triad--a motif in transcription, neuronal, and some electronic networks--has three roles: an input node, an output node, and an internal node. {T}he roles are used to define possible generalizations of the motif. {T}he feedforward loop can have three simple generalizations, based on replicating each of the three roles and their connections. {W}e present algorithms for efficiently detecting motif generalizations. {W}e find that the transcription networks of bacteria and yeast display only one of the three generalizations, the multi-output feedforward generalization. {I}n contrast, the neuronal network of {C}. elegans mainly displays the multi-input generalization. {F}orward-logic electronic circuits display a multi-input, multi-output hybrid. {T}hus, networks which share a common motif can have very different generalizations of that motif. {U}sing mathematical modeling, we describe the information processing functions of the different motif generalizations in transcription, neuronal, and electronic networks."
14,15,collective dynamics of smallworld networks,99.0,Collective dynamics of 'small-world' networks.,"Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices."
15,16,network motifs simple building blocks of complex networks,101.0,Network motifs: simple building blocks of complex networks.,"Complex networks are studied across many fields of science. {T}o uncover their structural design principles, we defined ""network motifs,"" patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. {W}e found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. {T}he motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of {E}scherichia coli and {S}accharomyces cerevisiae or from those found in the {W}orld {W}ide {W}eb. {S}imilar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in {C}aenorhabditis elegans. {M}otifs may thus define universal classes of networks. {T}his approach may uncover the basic building blocks of most networks."
16,17,authoritative sources in a hyperlinked environment,102.0,Authoritative sources in a hyperlinked environment,"The network structure of a hyperlinked environment can be a rich source of in- formation about the content of the environment, provided we have eective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their eectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of \authoritative"" information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of \hub pages"" that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.  We began with the goal of discovering authoritative pages, but our approach in fact identies a more complex pattern of social organization on the www, in which hub pages link densely to a set of thematically related authorities. This equilibrium between hubs and authorities is a phenomenon that recurs in the context of a wide variety of topics on the www. Measures of impact and influence in bibliometrics have typically lacked, and arguably not required, an analogous formulation of the role that hubs play; the www is very dierent from the scientic literature, and our framework seems appropriate as a model of the way in which authority is conferred in an environment such as the Web."
17,18,the anatomy of a largescale hypertextual web search engine,114.0,The anatomy of a large-scale hypertextual Web search engine,"Abstract In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http:// google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical largescale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want."
18,19,spectra and eigenvectors of scalefree networks,122.0,Spectra and eigenvectors of scale-free networks.,"We study the spectra and eigenvectors of the adjacency matrices of scale-free networks when bidirectional interaction is allowed, so that the adjacency matrix is real and symmetric. The spectral density shows an exponential decay around the center, followed by power-law long tails at both spectrum edges. The largest eigenvalue lambda1 depends on system size N as lambda1 approximately N1/4 for large N, and the corresponding eigenfunction is strongly localized at the hub, the vertex with largest degree. The component of the normalized eigenfunction at the hub is of order unity. We also find that the mass gap scales as N(-0.68)."
20,21,functional discovery via a compendium of expression profiles,124.0,Functional discovery via a compendium of expression profiles.,"Ascertaining the impact of uncharacterized perturbations on the cell is a fundamental problem in biology. Here, we describe how a single assay can be used to monitor hundreds of different cellular functions simultaneously. We constructed a reference database or “compendium” of expression profiles corresponding to 300 diverse mutations and chemical treatments in S. cerevisiae , and we show that the cellular pathways affected can be determined by pattern matching, even among very subtle profiles. The utility of this approach is validated by examining profiles caused by deletions of uncharacterized genes: we identify and experimentally confirm that eight uncharacterized open reading frames encode proteins required for sterol metabolism, cell wall function, mitochondrial respiration, or protein synthesis. We also show that the compendium can be used to characterize pharmacological perturbations by identifying a novel target of the commonly used drug dyclonine."
21,22,growing and navigating the small world web by local content,127.0,Growing and navigating the small world Web by local content.,"Can we model the scale-free distribution of Web hypertext degree under realistic assumptions about the behavior of page authors? Can a Web crawler efficiently locate an unknown relevant page? These questions are receiving much attention due to their potential impact for understanding the structure of the Web and for building better search engines. Here I investigate the connection between the linkage and content topology of Web pages. The relationship between a text-induced distance metric and a link-based neighborhood probability distribution displays a phase transition between a region where linkage is not determined by content and one where linkage decays according to a power law. This relationship is used to propose a Web growth model that is shown to accurately predict the distribution of Web page degree, based on textual content and assuming only local knowledge of degree for existing pages. A qualitatively similar phase transition is found between linkage and semantic distance, with an exponential decay tail. Both relationships suggest that efficient paths can be discovered by decentralized Web navigation algorithms based on textual and/or categorical cues."
22,23,the structure of scientific collaboration networks,128.0,The structure of scientific collaboration networks,"The structure of scientific collaboration networks is investigated. Two scientists are considered connected if they have authored a paper together and explicit networks of such connections are constructed by using data drawn from a number of databases, including MEDLINE (biomedical research), the Los Alamos e-Print Archive (physics), and NCSTRL (computer science). I show that these collaboration networks form âsmall worlds,â in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances. I further give results for mean and distribution of numbers of collaborators of authors, demonstrate the presence of clustering in the networks, and highlight a number of apparent differences in the patterns of collaboration between the fields studied."
23,24,classes of smallworld networks,129.0,Classes of small-world networks,"We study the statistical properties of a variety of diverse real-world networks. We present evidence of the occurrence of three classes of small-world networks: (a) scale-free networks, characterized by a vertex connectivity distribution that decays as a power law; (b) broad-scale networks, characterized by a connectivity distribution that has a power law regime followed by a sharp cutoff; and (c) single-scale networks, characterized by a connectivity distribution with a fast decaying tail. Moreover. we note for the classes of broad-scale and single-scale networks that there are constraints limiting the addition of new links. Our results suggest that the nature of such constraints may be the controlling factor for the emergence of different classes of networks."
24,25,spectra of realworld graphs beyond the semicircle law,139.0,"Spectra of ""real-world"" graphs: beyond the semicircle law.","Many natural and social systems develop complex networks, that are usually modelled as random graphs. The eigenvalue spectrum of these graphs provides information about their structural properties. While the semi-circle law is known to describe the spectral density of uncorrelated random graphs, much less is known about the eigenvalues of real-world graphs, describing such complex systems as the Internet, metabolic pathways, networks of power stations, scientific collaborations or movie actors, which are inherently correlated and usually very sparse. An important limitation in addressing the spectra of these systems is that the numerical determination of the spectra for systems with more than a few thousand nodes is prohibitively time and memory consuming. Making use of recent advances in algorithms for spectral characterization, here we develop new methods to determine the eigenvalues of networks comparable in size to real systems, obtaining several surprising results on the spectra of adjacency matrices corresponding to models of real-world graphs. We find that when the number of links grows as the number of nodes, the spectral density of uncorrelated random graphs does not converge to the semi-circle law. Furthermore, the spectral densities of real-world graphs have specific features depending on the details of the corresponding models. In particular, scale-free graphs develop a triangle-like spectral density with a power law tail, while small-world graphs have a complex spectral density function consisting of several sharp peaks. These and further results indicate that the spectra of correlated graphs represent a practical tool for graph classification and can provide useful insight into the relevant structural properties of real networks."
25,26,multistability in the lactose utilization network of escherichia coli,148.0,Multistability in the lactose utilization network of Escherichia coli,"Multistability, the capacity to achieve multiple internal states in response to a single set of external inputs, is the defining characteristic of a switch. Biological switches are essential for the determination of cell fate in multicellular organisms1, the regulation of cell-cycle oscillations during mitosis2, 3 and the maintenance of epigenetic traits in microbes4. The multistability of several natural1, 2, 3, 4, 5, 6 and synthetic7, 8, 9 systems has been attributed to positive feedback loops in their regulatory networks10. However, feedback alone does not guarantee multistability. The phase diagram of a multistable system, a concise description of internal states as key parameters are varied, reveals the conditions required to produce a functional switch11, 12. Here we present the phase diagram of the bistable lactose utilization network of Escherichia coli13. We use this phase diagram, coupled with a mathematical model of the network, to quantitatively investigate processes such as sugar uptake and transcriptional regulation in vivo. We then show how the hysteretic response of the wild-type system can be converted to an ultrasensitive graded response14, 15. The phase diagram thus serves as a sensitive probe of molecular interactions and as a powerful tool for rational network design."
26,27,protein complexes and functional modules in molecular networks,151.0,Protein complexes and functional modules in molecular networks,"Proteins, nucleic acids, and small molecules form a dense network of molecular interactions in a cell. Molecules are nodes of this network, and the interactions between them are edges. The architecture of molecular networks can reveal important principles of cellular organization and function, similarly to the way that protein structure tells us about the function and organization of a protein. Computational analysis of molecular networks has been primarily concerned with node degree [Wagner, A. & Fell, D. A. (2001) Proc. R. Soc. London Ser. B 268, 1803-1810; Jeong, H., Tombor, B., Albert, R., Oltvai, Z. N. & Barabasi, A. L. (2000) Nature 407, 651-654] or degree correlation [Maslov, S. & Sneppen, K. (2002) Science 296, 910-913], and hence focused on single/two-body properties of these networks. Here, by analyzing the multibody structure of the network of protein-protein interactions, we discovered molecular modules that are densely connected within themselves but sparsely connected with the rest of the network. Comparison with experimental data and functional annotation of genes showed two types of modules: (i) protein complexes (splicing machinery, transcription factors, etc.) and (ii) dynamic functional units (signaling cascades, cell-cycle regulation, etc.). Discovered modules are highly statistically significant, as is evident from comparison with random graphs, and are robust to noise in the data. Our results provide strong support for the network modularity principle introduced by Hartwell et al. [Hartwell, L. H., Hopfield, J. J., Leibler, S. & Murray, A. W. (1999) Nature 402, C47-C52], suggesting that found modules constitute the ""building blocks"" of molecular networks."
27,28,finding and evaluating community structure in networks,154.0,Finding and evaluating community structure in networks,"We propose and study a set of algorithms for discovering community structure in networksnatural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible ""betweenness"" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.We propose and study a set of algorithms for discovering community structure in networksnatural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible ""betweenness"" measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems."
28,29,the structure and function of complex networks,155.0,The structure and function of complex networks,"Inspired by empirical studies of networked systems such as the Internet, social networks, and bio- logical networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks."
29,30,matching words and pictures,156.0,Matching words and pictures,"We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-moda  and correspondence extensions to Hofmann’s hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
30,31,an overview of audio information retrieval,175.0,An overview of audio information retrieval,"Abstract.   The problem of audio information retrieval is familiar to anyone who has returned from vacation to find an answering machine full of messages. While there is not yet an ?AltaVista? for the audio data type, many workers are finding ways to automatically locate, index, and browse audio using recent advances in speech recognition and machine listening. This paper reviews the state of the art in audio information retrieval, and presents recent advances in automatic speech recognition, word spotting, speaker and music identification, and audio similarity with a view towards making audio less ?opaque?. A special section addresses intelligent interfaces for navigating and browsing audio and multimedia documents, using automatically derived information to go beyond the tape recorder metaphor."
31,32,programmed population control by cellcell communication and regulated killing,186.0,Programmed population control by cell–cell communication and regulated killing,"De novo engineering of gene circuits inside cells is extremely difficult1, 2, 3, 4, 5, 6, 7, 8, 9, and efforts to realize predictable and robust performance must deal with noise in gene expression and variation in phenotypes between cells10, 11, 12. Here we demonstrate that by coupling gene expression to cell survival and death using cell–cell communication, we can programme the dynamics of a population despite variability in the behaviour of individual cells. Specifically, we have built and characterized a 'population control' circuit that autonomously regulates the density of an Escherichia coli population. The cell density is broadcasted and detected by elements from a bacterial quorum-sensing system13, 14, which in turn regulate the death rate. As predicted by a simple mathematical model, the circuit can set a stable steady state in terms of cell density and gene expression that is easily tunable by varying the stability of the cell–cell communication signal. This circuit incorporates a mechanism for programmed death in response to changes in the environment, and allows us to probe the design principles of its more complex natural counterparts."
32,33,mobility in collaboration,205.0,Mobility in collaboration,"This paper addresses an issue that has received little attention within CSCW - the requirements to support mobility within collaborative activities. By examining three quite different settings each with differing technological support, we examine the ways in which mobility is critical to collaborative work. We suggest that taking mobility seriously may not only contribute our understanding of current support for collaboration, but raise more general issues concerning the requirements for mobile and other technologies."
33,34,the sociocognitive psychology of computermediated communication the present and future of technologybased interactions,214.0,The sociocognitive psychology of computer-mediated communication: the present and future of technology-based interactions.,"The increased diffusion of the Internet has made computer-mediated communication (CMC) very popular. However, a difficult question arises for psychologists and communication researchers: ""What are the communicative characteristics of CMC?"" According to the ""cues-filtered-out"" approach, CMC lacks the specifically relational features (social cues), which enable the interlocutors to identify correctly the kind of interpersonal situations they find themselves in. This paper counters this vision by integrating in its theoretical frame the different psycho-social approaches available in current literature. In particular, the paper describes the characteristics of the socio-cognitive processes-emotional expression, context definition, and identity creation-used by the interlocutors to make order and create relationships out of the miscommunication processes typical of CMC. Moreover, it presents the emerging forms of CMC-instant messaging, shared hypermedia, weblogs, and graphical chats-and their possible social and communicative effects."
34,35,oscillations in nfkappab signaling control the dynamics of gene expression,223.0,Oscillations in NF-{kappa}B Signaling Control the Dynamics of Gene Expression,"Signaling by the transcription factor nuclear factor kappa B (NF-{kappa}B) involves its release from inhibitor kappa B (I{kappa}B) in the cytosol, followed by translocation into the nucleus. NF-{kappa}B regulation of I{kappa}B{alpha} transcription represents a delayed negative feedback loop that drives oscillations in NF-{kappa}B translocation. Single-cell time-lapse imaging and computational modeling of NF-{kappa}B (RelA) localization showed asynchronous oscillations following cell stimulation that decreased in frequency with increased I{kappa}B{alpha} transcription. Transcription of target genes depended on oscillation persistence, involving cycles of RelA phosphorylation and dephosphorylation. The functional consequences of NF-{kappa}B signaling may thus depend on number, period, and amplitude of oscillations. 10.1126/science.1099962"
35,36,design of artificial cellcell communication using gene and metabolic networks,226.0,Design of artificial cell-cell communication using gene and metabolic networks.,"Artificial transcriptional networks have been used to achieve novel, nonnative behavior in bacteria. {T}ypically, these artificial circuits are isolated from cellular metabolism and are designed to function without intercellular communication. {T}o attain concerted biological behavior in a population, synchronization through intercellular communication is highly desirable. {H}ere we demonstrate the design and construction of a gene-metabolic circuit that uses a common metabolite to achieve tunable artificial cell-cell communication. {T}his circuit uses a threshold concentration of acetate to induce gene expression by acetate kinase and part of the nitrogen-regulation two-component system. {A}s one application of the cell-cell communication circuit we created an artificial quorum sensor. {E}ngineering of carbon metabolism in {E}scherichia coli made acetate secretion proportional to cell density and independent of oxygen availability. {I}n these cells the circuit induced gene expression in response to a threshold cell density. {T}his threshold can be tuned effectively by controlling {D}eltap{H} over the cell membrane, which determines the partition of acetate between medium and cells. {M}utagenesis of the enhancer sequence of the gln{A}p2 promoter produced variants of the circuit with changed sensitivity demonstrating tunability of the circuit by engineering of its components. {T}he behavior of the circuit shows remarkable predictability based on a mathematical design model."
36,37,initial sequencing and comparative analysis of the mouse genome,229.0,Initial sequencing and comparative analysis of the mouse genome.,"The sequence of the mouse genome is a key informational tool for understanding the contents of the human genome and a key experimental tool for biomedical research. Here, we report the results of an international collaboration to produce a high-quality draft sequence of the mouse genome. We also present an initial comparative analysis of the mouse and human genomes, describing some of the insights that can be gleaned from the two sequences. We discuss topics including the analysis of the evolutionary forces shaping the size, structure and sequence of the genomes; the conservation of large-scale synteny across most of the genomes; the much lower extent of sequence orthology covering less than half of the genomes; the proportions of the genomes under selection; the number of protein-coding genes; the expansion of gene families related to reproduction and immunity; the evolution of proteins; and the identification of intraspecies polymorphism."
37,38,dynamic conditional random fields,230.0,Dynamic conditional random fields,"Conditional random fields (CRFs) for sequence modeling have several  advantages over joint models such as HMMs, including the ability to  relax strong independence assumptions made in those models, and the  ability to incorporate arbitrary overlapping features. Previous work has  focused on linear-chain CRFs, which correspond to finite-state machines,  and have efficient exact inference algorithms. Often, however, we wish  to label sequence data in multiple interacting ways---for example,..."
38,39,large n field theories string theory and gravity,231.0,"Large N Field Theories, String Theory and Gravity","We review the holographic correspondence between field theories and string/M theory, focusing on the relation between compactifications of string/M theory on Anti-de Sitter spaces and conformal field theories. We review the background for this correspondence and discuss its motivations and the evidence for its correctness. We describe the main results that have been derived from the correspondence in the regime that the field theory is approximated by classical or semiclassical gravity. We focus on the case of the N=4 supersymmetric gauge theory in four dimensions, but we discuss also field theories in other dimensions, conformal and non-conformal, with or without supersymmetry, and in particular the relation to QCD. We also discuss some implications for black hole physics."
39,40,the largescale organization of metabolic networks,238.0,The large-scale organization of metabolic networks.,"In a cell or microorganism, the processes that generate mass, energy, information transfer and cell-fate specification are seamlessly integrated through a complex network of cellular constituents and reactions. {H}owever, despite the key role of these networks in sustaining cellular functions, their large-scale structure is essentially unknown. {H}ere we present a systematic comparative mathematical analysis of the metabolic networks of 43 organisms representing all three domains of life. {W}e show that, despite significant variation in their individual constituents and pathways, these metabolic networks have the same topological scaling properties and show striking similarities to the inherent organization of complex non-biological systems. {T}his may indicate that metabolic organization is not only identical for all living organisms, but also complies with the design principles of robust and error-tolerant scale-free networks, and may represent a common blueprint for the large-scale organization of interactions among all cellular constituents."
40,41,functional and topological characterization of protein interaction networks,239.0,Functional and topological characterization of protein interaction networks.,"The elucidation of the cell's large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. We compare four available databases that approximate the protein interaction network of the yeast, Saccharomyces cerevisiae, aiming to uncover the network's generic large-scale properties and the impact of the proteins' function and cellular localization on the network topology. We show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. We also find strong correlations between the network's structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. The uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies."
41,42,xml bioinformatics and data integration,242.0,"XML, bioinformatics and data integration.","Motivation: The eXtensible Markup Language (XML) is an emerging standard for structuring documents, notably for the World Wide Web. In this paper, the authors present XML and examine its use as a data language for bioinformatics. In particular, XML is compared to other languages, and some of the potential uses of XML in bioinformatics applications are presented. The authors propose to adopt XML for data interchange between databases and other sources of data. Finally the discussion is illustrated by a test case of a pedigree data model in XML. Contact: Emmanuel.Barillot@infobiogen.fr"
42,43,systems biology a brief overview,248.0,Systems Biology: A Brief Overview,"To understand biology at the system level, we must examine the structure and dynamics of cellular and organismal function, rather than the char- acteristics of isolated parts of a cell or organism. Properties of systems, such as robustness, emerge as central issues, and understanding these properties may have an impact on the future of medicine. However, many breakthroughs in experimental devices, advanced software, and analytical methods are required before the achievements of systems biology can live up to their much-touted potential."
43,44,computational systems biology,249.0,Computational systems biology,"Book Description Systems Biology is concerned with the quantitative study of complex biosystems at the molecular, cellular, tissue, and systems scales. Its focus is on the function of the system as a whole, rather than on individual parts. This exciting new arena applies mathematical modeling and engineering methods to the study of biological systems. This book is the first of its kind to focus on the newly emerging field of systems biology with an emphasis on computational approaches. The work covers new concepts, methods for information storage, mining and knowledge extraction, reverse engineering of gene and metabolic networks, as well as modelling and simulation of multi-cellular systems. Central themes include strategies for predicting biological properties and methods for elucidating structure-function relationships. From the Back Cover Computational systems biology is a field that aims to develop a systems level understanding of biological processes using computational techniques. This book presents a timely compendium of the state-of-the-art in this new era of biological understanding. A distinguished group of contributors reviews bioinformation engineering and data integration technologies along with biomedical applications. Different computational approaches to model and simulate biological systems are discussed. Current computational research efforts, comprehensively covered in this volume, are focused on regulatory, signaling and metabolic networks. As an exciting outlook new concepts for computer representations on multi-scales predicting emergent properties of biological systems are introduced. Andres Kriete is Associate Professor for Bioinformation Engineering at Drexel University, Philadelphia and Director of the Biocomputing Laboratory at the Coriell Institute for Medical Research. Roland Eils is Professor of Bioinformatics at the University of Heidelberg and Director of the Division of Theoretical Bioinformatics at the German Cancer Research Center (DKFZ) in Heidelberg."
44,45,mapping weblog communities,260.0,Mapping weblog communities,"Websites of a particular class form increasingly complex networks, and new tools are needed to map and understand them. A way of visualizing this complex network is by mapping it. A map highlights which members of the community have similar interests, and reveals the underlying social network. In this paper, we will map a network of websites using Kohonen's self-organizing map (SOM), a neural-net like method generally used for clustering and visualization of complex data sets. The set of websites considered has been the Blogalia weblog hosting site (based at <A HREF=""http://www.blogalia.com/"">this http URL</A>), a thriving community of around 200 members, created in January 2002. In this paper we show how SOM discovers interesting community features, its relation with other community-discovering algorithms, and the way it highlights the set of communities formed over the network."
45,46,understanding mobile contexts,263.0,Understanding mobile contexts,"Mobile urban environments present a challenge for context-aware computers because they differ from fixed indoor contexts such as offices, meeting rooms, and lecture halls in many important ways. Internal factors such as tasks and goals are different—external factors such as social resources are dynamic and unpredictable. An empirical, user-centred approach is needed to understand mobile contexts. In this paper, we present insights from an ethnomethodologically inspired study of 25 adult urbanites in Helsinki. The results describe typical phenomena in mobility: how situational and planned acts intermesh in navigation, how people construct personal and group spaces, and how temporal tensions develop and dissolve. Furthermore, we provide examples of social solutions to navigation problems, examine mobile multitasking, and consider design implications for mobile and context-aware human–computer interaction."
47,48,network biology understanding the cells functional organization,268.0,Network biology: understanding the cell's functional organization.,"A key aim of postgenomic biomedical research is to systematically catalogue all molecules and their interactions within a living cell. There is a clear need to understand how these molecules and the interactions between them determine the function of this enormously complex machinery, both in isolation and when surrounded by other cells. Rapid advances in network biology indicate that cellular networks are governed by universal laws and offer a new conceptual framework that could potentially revolutionize our view of biology and disease pathologies in the twenty-first century."
48,49,the small world of human language,270.0,The small world of human language.,"Words in human language interact in sentences in non-random ways, and allow humans to construct an astronomic variety of sentences from a limited number of discrete units. This construction process is extremely fast and robust. The co-occurrence of words in sentences reflects language organization in a subtle manner that can be described in terms of a graph of word interactions. Here, we show that such graphs display two important features recently found in a disparate number of complex systems. (i) The so called small-world effect. In particular, the average distance between two words, d (i.e. the average minimum number of links to be crossed from an arbitrary word to another), is shown to be d approximately equal to 2-3, even though the human brain can store many thousands. (ii) A scale-free distribution of degrees. The known pronounced effects of disconnecting the most connected vertices in such networks can be identified in some language disorders. These observations indicate some unexpected features of language organization that might reflect the evolutionary and social history of lexicons and the origins of their flexibility and combinatorial nature."
49,50,metabolomics and systems biology making sense of the soup,271.0,Metabolomics and systems biology: making sense of the soup.,"Novel techniques for acquiring metabolomics data continue to emerge. Such data require proper storage in suitably configured databases, which then permit one to establish the size of microbial metabolomes (hundreds of major metabolites) and allow the nature, organisation and control of metabolic networks to be investigated. A variety of algorithms for metabolic network reconstruction coupled to suitable modelling algorithms are the ground substances for the development of metabolic network and systems biology. Even qualitative models of metabolic networks, when subject to stoichiometric constraints, can prove highly informative, and are the first step to the quantitative models, which alone can allow the true representation of complex biochemical systems."
50,51,genomic analysis of regulatory network dynamics reveals large topological changes,272.0,Genomic analysis of regulatory network dynamics reveals large topological changes,"Network analysis has been applied widely, providing a unifying language to describe disparate systems ranging from social interactions to power grids. It has recently been used in molecular biology, but so far the resulting networks have only been analysed statically1, 2, 3, 4, 5, 6, 7, 8. Here we present the dynamics of a biological network on a genomic scale, by integrating transcriptional regulatory information9, 10, 11 and gene-expression data12, 13, 14, 15, 16 for multiple conditions in Saccharomyces cerevisiae. We develop an approach for the statistical analysis of network dynamics, called SANDY, combining well-known global topological measures, local motifs and newly derived statistics. We uncover large changes in underlying network architecture that are unexpected given current viewpoints and random simulations. In response to diverse stimuli, transcription factors alter their interactions to varying degrees, thereby rewiring the network. A few transcription factors serve as permanent hubs, but most act transiently only during certain conditions. By studying sub-network structures, we show that environmental responses facilitate fast signal propagation (for example, with short regulatory cascades), whereas the cell cycle and sporulation direct temporal progression through multiple stages (for example, with highly inter-connected transcription factors). Indeed, to drive the latter processes forward, phase-specific transcription factors inter-regulate serially, and ubiquitously active transcription factors layer above them in a two-tiered hierarchy. We anticipate that many of the concepts presented here—particularly the large-scale topological changes and hub transience—will apply to other biological networks, including complex sub-systems in higher eukaryotes."
51,52,fluctuations in network dynamics,280.0,Fluctuations in network dynamics.,"Most complex networks serve as conduits for various dynamical processes, ranging from mass transfer by chemical reactions in the cell to packet transfer on the Internet. We collected data on the time dependent activity of five natural and technological networks, finding that for each the coupling of the flux fluctuations with the total flux on individual nodes obeys a unique scaling law. We show that the observed scaling can explain the competition between the system's internal collective dynamics and changes in the external environment, allowing us to predict the relevant scaling exponents."
52,53,analysis of weighted networks,281.0,Analysis of weighted networks,"The connections in many networks are not merely binary entities, either present or not, but have associated weights that record their strengths relative to one another. Recent studies of networks have, by and large, steered clear of such weighted networks, which are often perceived as being harder to analyze than their unweighted counterparts. Here we point out that weighted networks can in many cases be analyzed using a simple mapping from a weighted network to an unweighted multigraph, allowing us to apply standard techniques for unweighted graphs to weighted ones as well. We give a number of examples of the method, including an algorithm for detecting community structure in weighted networks and a simple proof of the maximum-flow–minimum-cut theorem."
53,54,inferring network mechanisms the drosophila melanogaster protein interaction network,282.0,Inferring Network Mechanisms: The Drosophila melanogaster Protein Interaction Network,"Naturally occurring networks exhibit quantitative features revealing underlying growth mechanisms. Numerous network mechanisms have recently been proposed to reproduce specific properties such as degree distributions or clustering coefficients. We present a method for inferring the mechanism most accurately capturing a given network topology, exploiting discriminative tools from machine learning. The Drosophila melanogaster protein network is confidently and robustly (to noise and training data subsampling) classified as a duplication{\^a}mutation{\^a}complementation network over preferential attachment, small-world, and a duplication{\^a}mutation mechanism without complementation. Systematic classification, rather than statistical study of specific properties, provides a discriminative approach to understand the design of complex networks."
54,55,metabolomics by numbers acquiring and understanding global metabolite data,289.0,Metabolomics by numbers: acquiring and understanding global metabolite data,"In this postgenomic era, there is a specific need to assign function to orphan genes in order to validate potential targets for drug therapy and to discover new biomarkers of disease. Metabolomics is an emerging field that is complementary to the other ‘omics and proving to have unique advantages. As in transcriptomics or proteomics, a typical metabolic fingerprint or metabolomic experiment is likely to generate thousands of data points, of which only a handful might be needed to describe the problem adequately. Extracting the most meaningful elements of these data is thus key to generating useful new knowledge with mechanistic or explanatory power."
55,56,functional genomic hypothesis generation and experimentation by a robot scientist,292.0,Functional genomic hypothesis generation and experimentation by a robot scientist,"The question of whether it is possible to automate the scientific process is of both great theoretical interest1, 2 and increasing practical importance because, in many scientific areas, data are being generated much faster than they can be effectively analysed. We describe a physically implemented robotic system that applies techniques from artificial intelligence3, 4, 5, 6, 7, 8 to carry out cycles of scientific experimentation. The system automatically originates hypotheses to explain observations, devises experiments to test these hypotheses, physically runs the experiments using a laboratory robot, interprets the results to falsify hypotheses inconsistent with the data, and then repeats the cycle. Here we apply the system to the determination of gene function using deletion mutants of yeast (Saccharomyces cerevisiae) and auxotrophic growth experiments9. We built and tested a detailed logical model (involving genes, proteins and metabolites) of the aromatic amino acid synthesis pathway. In biological experiments that automatically reconstruct parts of this model, we show that an intelligent experiment selection strategy is competitive with human performance and significantly outperforms, with a cost decrease of 3-fold and 100-fold (respectively), both cheapest and random-experiment selection."
56,57,here is the evidence now what is the hypothesis the complementary roles of inductive and hypothesisdriven science in the postgenomic era,293.0,"Here is the evidence, now what is the hypothesis? The complementary roles of inductive and hypothesis-driven science in the post-genomic era","Abstract It is considered in some quarters that hypothesis-driven methods are the only valuable, reliable or significant means of scientific advance. Data-driven or ‘inductive’ advances in scientific knowledge are then seen as marginal, irrelevant, insecure or wrong-headed, while the development of technology—which is not of itself ‘hypothesis-led’ (beyond the recognition that such tools might be of value)—must be seen as equally irrelevant to the hypothetico-deductive scientific agenda. We argue here that data- and technology-driven programmes are not alternatives to hypothesis-led studies in scientific knowledge discovery but are complementary and iterative partners with them. Many fields are data-rich but hypothesis-poor. Here, computational methods of data analysis, which may be automated, provide the means of generating novel hypotheses, especially in the post-genomic era. BioEssays 26:99–105, 2004. © 2003 Wiley Periodicals, Inc."
57,58,highthroughput classification of yeast mutants for functional genomics using metabolic footprinting,296.0,High-throughput classification of yeast mutants for functional genomics using metabolic footprinting.,"Many technologies have been developed to help explain the function of genes discovered by systematic genome sequencing. At present, transcriptome and proteome studies dominate large-scale functional analysis strategies. Yet the metabolome, because it is 'downstream', should show greater effects of genetic or physiological changes and thus should be much closer to the phenotype of the organism. We earlier presented a functional analysis strategy that used metabolic fingerprinting to reveal the phenotype of silent mutations of yeast genes. However, this is difficult to scale up for high-throughput screening. Here we present an alternative that has the required throughput (2 min per sample). This 'metabolic footprinting' approach recognizes the significance of 'overflow metabolism' in appropriate media. Measuring intracellular metabolites is time-consuming and subject to technical difficulties caused by the rapid turnover of intracellular metabolites and the need to quench metabolism and separate metabolites from the extracellular space. We therefore focused instead on direct, noninvasive, mass spectrometric monitoring of extracellular metabolites in spent culture medium. Metabolic footprinting can distinguish between different physiological states of wild-type yeast and between yeast single-gene deletion mutants even from related areas of metabolism. By using appropriate clustering and machine learning techniques, the latter based on genetic programming, we show that metabolic footprinting is an effective method to classify 'unknown' mutants by genetic defect."
58,59,a systematic approach to modeling capturing and disseminating proteomics experimental data,297.0,"A systematic approach to modeling, capturing, and disseminating proteomics experimental data.","Both the generation and the analysis of proteome data are becoming increasingly widespread, and the field of proteomics is moving incrementally toward high-throughput approaches. Techniques are also increasing in complexity as the relevant technologies evolve. A standard representation of both the methods used and the data generated in proteomics experiments, analogous to that of the MIAME (minimum information about a microarray experiment) guidelines for transcriptomics, and the associated MAGE (microarray gene expression) object model and XML (extensible markup language) implementation, has yet to emerge. This hinders the handling, exchange, and dissemination of proteomics data. Here, we present a UML (unified modeling language) approach to proteomics experimental data, describe XML and SQL (structured query language) implementations of that model, and discuss capture, storage, and dissemination strategies. These make explicit what data might be most usefully captured about proteomics experiments and provide complementary routes toward the implementation of a proteome repository."
59,60,schemes of flux control in a model of saccharomyces cerevisiae glycolysis,306.0,Schemes of flux control in a model of Saccharomyces cerevisiae glycolysis.,"We used parameter scanning to emulate changes to the limiting rate for steps in a fitted model of glucose-derepressed yeast glycolysis. Three flux-control regimes were observed, two of which were under the dominant control of hexose transport, in accordance with various experimental studies and other model predictions. A third control regime in which phosphofructokinase exerted dominant glycolytic flux control was also found, but it appeared to be physiologically unreachable by this model, and all realistically obtainable flux control regimes featured hexose transport as a step involving high flux control."
60,61,motifs modules and games in bacteria,309.0,"Motifs, modules and games in bacteria.","Global explorations of regulatory network dynamics, organization and evolution have become tractable thanks to high-throughput sequencing and molecular measurement of bacterial physiology. {F}rom these, a nascent conceptual framework is developing, that views the principles of regulation in term of motifs, modules and games. {M}otifs are small, repeated, and conserved biological units ranging from molecular domains to small reaction networks. {T}hey are arranged into functional modules, genetically dissectible cellular functions such as the cell cycle, or different stress responses. {T}he dynamical functioning of modules defines the organism's strategy to survive in a game, pitting cell against cell, and cell against environment. {P}lacing pathway structure and dynamics into an evolutionary context begins to allow discrimination between those physical and molecular features that particularize a species to its surroundings, and those that provide core physiological function. {T}his approach promises to generate a higher level understanding of cellular design, pathway evolution and cellular bioengineering."
61,62,network dynamics and cell physiology,310.0,Network dynamics and cell physiology,"Complex assemblies of interacting proteins carry out most of the interesting jobs in a cell, such as metabolism, DNA synthesis, movement and information processing. These physiological properties play out as a subtle molecular dance, choreographed by underlying regulatory networks. To understand this dance, a new breed of theoretical molecular biologists reproduces these networks in computers and in the mathematical language of dynamical systems."
62,63,a synthetic oscillatory network of transcriptional regulators,311.0,A synthetic oscillatory network of transcriptional regulators.,"Networks of interacting biomolecules carry out many essential functions in living cells, but the 'design principles' underlying the functioning of such intracellular networks remain poorly understood, despite intensive efforts including quantitative analysis of relatively simple systems. {H}ere we present a complementary approach to this problem: the design and construction of a synthetic network to implement a particular function. {W}e used three transcriptional repressor systems that are not part of any natural biological clock to build an oscillating network, termed the repressilator, in {E}scherichia coli. {T}he network periodically induces the synthesis of green fluorescent protein as a readout of its state in individual cells. {T}he resulting oscillations, with typical periods of hours, are slower than the cell-division cycle, so the state of the oscillator has to be transmitted from generation to generation. {T}his artificial clock displays noisy behaviour, possibly because of stochastic fluctuations of its components. {S}uch 'rational network design may lead both to the engineering of new cellular behaviours and to an improved understanding of naturally occurring networks."
63,64,construction of a genetic toggle switch in escherichia coli,312.0,Construction of a genetic toggle switch in Escherichia coli.,"It has been proposed' that gene-regulatory circuits with virtually any desired property can be constructed from networks of simple regulatory elements. {T}hese properties, which include multistability and oscillations, have been found in specialized gene circuits such as the bacteriophage lambda switch and the {C}yanobacteria circadian oscillator. {H}owever, these behaviours have not been demonstrated in networks of non-specialized regulatory components. {H}ere we present the construction of a genetic toggle switch-a synthetic, bistable gene-regulatory network-in {E}scherichia coli and provide a simple theory that predicts the conditions necessary for bistability. {T}he toggle is constructed from any two repressible promoters arranged in a mutually inhibitory network. {I}t is flipped between stable states using transient chemical or thermal induction and exhibits a nearly ideal switching threshold. {A}s a practical device, the toggle switch forms a synthetic, addressable cellular memory unit and has implications for biotechnology, biocomputing and gene therapy."
64,65,engineering stability in gene networks by autoregulation,314.0,Engineering stability in gene networks by autoregulation.,"The genetic and biochemical networks which underlie such things as homeostasis in metabolism and the developmental programs of living cells, must withstand considerable variations and random perturbations of biochemical parameters. {T}hese occur as transient changes in, for example, transcription, translation, and {RNA} and protein degradation. {T}he intensity and duration of these perturbations differ between cells in a population. {T}he unique state of cells, and thus the diversity in a population, is owing to the different environmental stimuli the individual cells experience and the inherent stochastic nature of biochemical processes (for example, refs 5 and 6). {I}t has been proposed, but not demonstrated, that autoregulatory, negative feedback loops in gene circuits provide stability, thereby limiting the range over which the concentrations of network components fluctuate. {H}ere we have designed and constructed simple gene circuits consisting of a regulator and transcriptional repressor modules in {E}scherichia coli and we show the gain of stability produced by negative feedback."
65,66,a functional genomics strategy that uses metabolome data to reveal the phenotype of silent mutations,326.0,A functional genomics strategy that uses metabolome data to reveal the phenotype of silent mutations.,"A large proportion of the 6,000 genes present in the genome of Saccharomyces cerevisiae, and of those sequenced in other organisms, encode proteins of unknown function. Many of these genes are ""silent,"" that is, they show no overt phenotype, in terms of growth rate or other fluxes, when they are deleted from the genome. We demonstrate how the intracellular concentrations of metabolites can reveal phenotypes for proteins active in metabolic regulation. Quantification of the change of several metabolite concentrations relative to the concentration change of one selected metabolite can reveal the site of action, in the metabolic network, of a silent gene. In the same way, comprehensive analyses of metabolite concentrations in mutants, providing ""metabolic snapshots,"" can reveal functions when snapshots from strains deleted for unstudied genes are compared to those deleted for known genes. This approach to functional analysis, using comparative metabolomics, we call FANCY—an abbreviation for functional analysis by co-responses in yeast."
66,67,document coorganization in an online knowledge community,431.0,Document co-organization in an online knowledge community,"We introduce the concept of ""document co-organization"" and describe such a system. By document co-organization we mean that individuals are allowed to hierarchically organize documents personally and share their hierarchies with others, while the system generates a ""consensus"" hierarchy from these personal hierarchies, which provides a full, common, and emergent view of all documents. By allowing users to retrieve documents from their own organization (hierarchy), another user's, the consensus hierarchy, or a time-based hierarchy, we provide access corresponding to different characteristics of knowledge tasks: they are personal, collective, social, and time-sensitive. In a class website experiment, we show that for a complex knowledge task, hierarchies are used more frequently than search. One surprising finding is how often students use others' personal hierarchies."
67,68,inferring web communities from link topology,444.0,Inferring Web Communities from Link Topology,"dag&amp;x.berkeley.edu kleinberQcs.cornell.edu The World Wide Web grows through a decentralized, almost anarchic process, and this has resulted in a large hyperlinked corpus without the kind of logical organiza-tion that can be built into more tradit,ionally-created hy-permedia. To extract, meaningful structure under such circumstances, we develop a notion of hyperlinked com-munities on the www t,hrough an analysis of the link topology. By invoking a simple, mathematically clean method for defining and exposing the structure of these communities, we are able to derive a number of themes: The communities can be viewed as containing a core of central, “authoritative ” pages linked tog&amp;her by “hub pages ” ; and they exhibit a natural type of hierarchical topic generalization that can be inferred directly from the pat,t,ern of linkage. Our investigation shows that al-though the process by which users of the Web create pages and links is very difficult to understand at a “lo-cal ” level, it results in a much greater degree of orderly high-level structure than has typically been assumed."
68,69,footprints historyrich tools for information foraging,446.0,Footprints: History-Rich Tools for Information Foraging,"Inspired by Hill and Hollan's original work [6], we have been developing a theory of interaction history and building tools to apply this theory to navigation in a complex information space. We have built a series of tools --- map, trails, annotations and signposts --- based on a physical-world navigation metaphor. These tools have been in use for over a year. Our user study involved a controlled browse task and showed that users were able to get the same amount of work done with significantly less effort."
69,70,information foraging,448.0,Information foraging,"Information foraging theory is an approach to understanding how strategies and technologies for information seeking, gathering, and consumption are adapted to the flux of information in the environment. The theory assumes that people, when possible, will modify their strategies or the structure of the environment to maximize their rate of gaining valuable information. The theory is developed by (a) adaptation (rational) analysis of information foraging problems and (b) a detailed process model (adaptive control of thought in information foraging [ACT-IF]). The adaptation analysis develops (a) information patch models, which deal with time allocation and information filtering and enrichment activities in environments in which information is encountered in clusters; (b) information scent models, which address the identification of information value from proximal cues; and (c) information diet models, which address decisions about the selection and pursuit of information items. ACT-IF is instantiated as a production system model of people interacting with complex information technology. Humans actively seek, gather, share, and consume information to a degree unapproached by other organisms. Ours might properly be characterized as a species of informavores (Dennett, 1991). Our adaptive success depends to a large extent on a vast and complex"
70,71,exploiting generative models in discriminative classifiers,449.0,Exploiting generative models in discriminative classifiers,"Generative probability models such as hidden Markov models providea principled way of treating missing information and dealingwith variable length sequences. On the other hand, discriminativemethods such as support vector machines enable us to constructflexible decision boundaries and often result in classification performancesuperior to that of the model based approaches. An idealclassifier should combine these two complementary approaches. Inthis paper, we develop a natural way of achieving this combinationby deriving kernel functions for use in discriminative methodssuch as support vector machines from generative probability models.We provide a theoretical justification for this combination aswell as demonstrate a substantial improvement in the classificationperformance in the context of DNA and protein sequence analysis.1 IntroductionSpeech, vision, text and biosequence data can be difficult to deal with in the contextof simple statistical classification problems. B..."
71,72,spreading activation models for trust propagation,457.0,Spreading Activation Models for Trust Propagation,"Semantic Web endeavors have mainly focused on issues pertaining to knowledge representation and ontology design. However, besides understanding information metadata stated by subjects, knowing about their credibility becomes equally crucial. Hence, trust and trust metrics, conceived as computational means to evaluate trust relationships between individuals, come into play. Our major contributions to semantic Web trust management are twofold. First, we introduce our classification scheme for trust metrics along various axes and discuss advantages and drawbacks of existing approaches for semantic Web scenarios. Hereby, we devise our advocacy for local group trust metrics, guiding us to the second part which presents Appleseed, our novel proposal for local group trust computation. Compelling in its simplicity, Appleseed borrows many ideas from spreading activation models in psychology and relates their concepts to trust evaluation in an intuitive fashion."
72,73,design and implementation of microarray gene expression markup language mageml,459.0,Design and implementation of microarray gene expression markup language (MAGE-ML).,"BACKGROUND: Meaningful exchange of microarray data is currently difficult because it is rare that published data provide sufficient information depth or are even in the same format from one publication to another. Only when data can be easily exchanged will the entire biological community be able to derive the full benefit from such microarray studies. RESULTS: To this end we have developed three key ingredients towards standardizing the storage and exchange of microarray data. First, we have created a minimal information for the annotation of a microarray experiment (MIAME)-compliant conceptualization of microarray experiments modeled using the unified modeling language (UML) named MAGE-OM (microarray gene expression object model). Second, we have translated MAGE-OM into an XML-based data format, MAGE-ML, to facilitate the exchange of data. Third, some of us are now using MAGE (or its progenitors) in data production settings. Finally, we have developed a freely available software tool kit (MAGE-STK) that eases the integration of MAGE-ML into end users' systems. CONCLUSIONS: MAGE will help microarray data producers and users to exchange information by providing a common platform for data exchange, and MAGE-STK will make the adoption of MAGE easier."
73,74,network reachability of realworld contact sequences,482.0,Network reachability of real-world contact sequences,"We use real-world contact sequences, time-ordered lists of contacts from one person to another, to study how fast information or disease can spread across network of contacts. Specifically we measure the reachability time -- the average shortest time for a series of contacts to spread information between a reachable pair of vertices (a pair where a chain of contacts exists leading from one person to the other) -- and the reachability ratio -- the fraction of reachable vertex pairs. These measures are studied using conditional uniform graph tests. We conclude, among other things, that the network reachability depends much on a core where the path lengths are short and communication frequent, that clustering of the contacts of an edge in time tend to decrease the reachability, and that the order of the contacts really do make sense for dynamical spreading processes."
75,76,sticky information and the locus of problem solving implications for innovation,490.0,Sticky Information and the Locus of Problem Solving: Implications for Innovation,"To solve a problem, needed information and problem-solving capabilities must be brought together. Often the information used in technical problem solving is costly to acquire, transfer, and use in a new location-is, in our terms, sticky. In this paper we explore the impact of information stickiness on the locus of innovation-related problem solving. We find, first, that when sticky information needed by problem solvers is held at one site only, problem solving will be carried out at that locus, other things being equal. Second, when more than one locus of sticky information is called upon by problem solvers, the locus of problem solving may iterate among these sites as problem solving proceeds. When the costs of such iteration are high, then, third, problems that draw upon multiple sites of sticky information will sometimes be task partitioned into subproblems that each draw on only one such locus, and/or, fourth, investments will be made to reduce the stickiness of information at some locations. Information stickiness appears to affect a number of issues of importance to researchers and practitioners. Among these are patterns in the diffusion of information, the specialization of firms, the locus of innovation, and the nature of problems selected by problem solvers."
76,77,information diffusion through blogspace,491.0,Information diffusion through blogspace,"We study the dynamics of information propagation in environments of low-overhead personal publishing, using a large collection of weblogs over time as our example domain. We characterize and model this collection at two levels. First, we present a macroscopic characterization of topic propagation through our corpus, formalizing the notion of long-running “chatter” topics consisting recursively of “spike” topics generated by outside world events, or more rarely, by resonances within the community. Second, we present a microscopic characterization of propagation from individual to individual, drawing on the theory of infectious diseases to model the flow. We propose, validate, and employ an algorithm to induce the underlying propagation network from a sequence of posts, and report on the results."
77,78,problems with fitting to the powerlaw distribution,492.0,Problems with Fitting to the Power-Law Distribution,"Abstract. This short communication uses a simple experiment to show that fitting to a power law distribution by using graphical methods based on linear fit on the log-log scale is biased and inaccurate. It shows that using maximum likelihood estimation (MLE) is far more robust. Finally, it presents a new table for performing the Kolmogorov-Smirnov test for goodness-of-fit tailored to power-law distributions in which the power-law exponent is estimated using MLE. The techniques presented here will advance the application of complex network theory by allowing reliable estimation of power-law models from data and further allowing quantitative assessment of goodness-of-fit of proposed power-law models to empirical data. PACS. 02.50.Ng Distribution theory and Monte Carlo studies -- 05.10.Ln Monte Carlo methods -- 89.75.-k"
78,79,the shortest path to complex networks,494.0,The shortest path to complex networks,1. The birth of network science. 2. What are random networks? 3. Adjacency matrix. 4. Degree distribution. 5. What are simple networks? Classical random graphs. 6. Birth of the giant component. 7. Topology of the Web. 8.Uncorrelated networks. 9. What are small worlds? 10. Real networks are mesoscopic objects. 11. What are complex networks? 12. The configuration model. 13. The absence of degree--degree correlations. 14.Networks with correlated degrees.15.Clustering. 16. What are small-world networks? 17. `Small worlds' is not the same as `small-world networks'. 18. Fat-tailed degree distributions. 19.Reasons for the fat-tailed degree distributions. 20. Preferential linking. 21. Condensation of edges. 22. Cut-offs of degree distributions. 23. Reasons for correlations in networks. 24. Classical random graphs cannot be used for comparison with real networks. 25. How to measure degree--degree correlations. 26. Assortative and disassortative mixing. 27. Disassortative mixing does not mean that vertices of high degrees rarely connect to each other. 28. Reciprocal links in directed nets. 29. Ultra-small-world effect. 30. Tree ansatz. 31.Ultraresilience against random failures. 32. When correlated nets are ultraresilient. 33. Vulnerability of complex networks. 34. The absence of an epidemic threshold. 35. Search based on local information. 36.Ultraresilience disappears in finite nets. 37.Critical behavior of cooperative models on networks. 38. Berezinskii-Kosterlitz-Thouless phase transitions in networks. 39.Cascading failures. 40.Cliques &amp; communities. 41. Betweenness. 42.Extracting communities. 43. Optimal paths. 44.Distributions of the shortest-path length &amp; of the loop's length are narrow. 45. Diffusion on networks. 46. What is modularity? 47.Hierarchical organization of networks. 48. Convincing modelling of real-world networks:Is it possible? 49. The small Web..
79,80,coevolution of dynamical states and interactions in dynamic networks,496.0,Coevolution of dynamical states and interactions in dynamic networks,"We explore the coupled dynamics of the internal states of a set of interacting elements and the network of interactions among them. Interactions are modeled by a spatial game and the network of interaction links evolves adapting to the outcome of the game. As an example, we consider a model of cooperation in which the adaptation is shown to facilitate the formation of a hierarchical interaction network that sustains a highly cooperative stationary state. The resulting network has the characteristics of a small world network when a mechanism of local neighbor selection is introduced in the adaptive network dynamics. The highly connected nodes in the hierarchical structure of the network play a leading role in the stability of the network. Perturbations acting on the state of these special nodes trigger global avalanches leading to complete network reorganization."
80,81,coarsegraining and selfdissimilarity of complex networks,500.0,Coarse-Graining and Self-Dissimilarity of Complex Networks,"Can complex engineered and biological networks be coarse-grained into smaller and more understandable versions in which each node represents an entire pattern in the original network? To address this, we define coarse-graining units as connectivity patterns which can serve as the nodes of a coarse-grained network and present algorithms to detect them. We use this approach to systematically reverse-engineer electronic circuits, forming understandable high-level maps from incomprehensible transistor wiring: first, a coarse-grained version in which each node is a gate made of several transistors is established. Then the coarse-grained network is itself coarse-grained, resulting in a high-level blueprint in which each node is a circuit module made of many gates. We apply our approach also to a mammalian protein signal-transduction network, to find a simplified coarse-grained network with three main signaling channels that resemble multi-layered perceptrons made of cross-interacting MAP-kinase cascades. We find that both biological and electronic networks are ""self-dissimilar,"" with different network motifs at each level. The present approach may be used to simplify a variety of directed and nondirected, natural and designed networks."
81,82,mips a database for genomes and protein sequences,501.0,MIPS: a database for genomes and protein sequences,"The Munich Information Center for Protein Sequences (MIPS-GSF, Neuherberg, Germany) continues to provide genome-related information in a systematic way. MIPS supports both national and European sequencing and functional analysis projects, develops and maintains automatically generated and manually annotated genome-specific databases, develops systematic classification schemes for the functional annotation of protein sequences, and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the databases for the comprehensive set of genomes (PEDANT genomes), the database of annotated human EST clusters (HIB), the database of complete cDNAs from the DHGP (German Human Genome Project), as well as the project specific databases for the GABI (Genome Analysis in Plants) and HNB (Helmholtz-Netzwerk Bioinformatik) networks. The Arabidospsis thaliana database (MATDB), the database of mitochondrial proteins (MITOP) and our contribution to the PIR International Protein Sequence Database have been described elsewhere [Schoof et al. (2002) Nucleic Acids Res., 30, 91-93; Scharfe et al. (2000) Nucleic Acids Res., 28, 155-158; Barker et al. (2001) Nucleic Acids Res., 29, 29-32]. All databases described, the protein analysis tools provided and the detailed descriptions of our projects can be accessed through the MIPS World Wide Web server (http://mips.gsf.de)."
82,83,mips analysis and annotation of proteins from whole genomes,509.0,MIPS: analysis and annotation of proteins from whole genomes,"The Munich Information Center for Protein Sequences (MIPSâGSF), Neuherberg, Germany, provides protein sequenceârelated information based on wholeâgenome analysis. The main focus of the work is directed toward the systematic organization of sequenceârelated attributes as gathered by a variety of algorithms, primary information from experimental data together with information compiled from the scientific literature. MIPS maintains automatically generated and manually annotated genomeâspecific databases, develops systematic classification schemes for the functional annotation of protein sequences and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the database of complete cDNAs (German Human Genome Project, NGFN), the database of mammalian proteinâprotein interactions (MPPI), the database of FASTA homologies (SIMAP), and the interface for the fast retrieval of proteinâassociated information (QUIPOS). The Arabidopsis thaliana database, the rice database, the plant EST databases (MATDB, MOsDB, SPUTNIK), as well as the databases for the comprehensive set of genomes (PEDANT genomes) are described elsewhere in the 2003 and 2004 NAR database issues, respectively. All databases described, and the detailed descriptions of our projects can be accessed through the MIPS web server (http://mips.gsf.de)."
83,84,evolving a stigmergic selforganized datamining,514.0,Evolving a Stigmergic Self-Organized Data-Mining,"Self-organizing complex systems typically are comprised of a large number of frequently similar components or events. Through their process, a pattern at the global-level of a system emerges solely from numerous interactions among the lower-level components of the system. Moreover, the rules specifying interactions among the system’s components are executed using only local information, without reference to the global pattern, which, as in many real-world problems is not easily accessible or possible to be found. Stigmergy, a kind of indirect communication and learning by the environment found in social insects is a well know example of self-organization, providing not only vital clues in order to understand how the components can interact to produce a complex pattern, as can pinpoint simple biological non-linear rules and methods to achieve improved artificial intelligent adaptive categorization systems, critical for Data-Mining. On the present work it is our intention to show that a new type of Data-Mining can be designed based on Stigmergic paradigms, taking profit of several natural features of this phenomenon. By hybridizing bio-inspired Swarm Intelligence with Evolutionary Computation we seek for an entire distributed, adaptive, collective and cooperative self-organized Data-Mining. As a real-world/real-time test bed for our proposal, World-Wide-Web Mining will be used.  Having that purpose in mind, Web usage Data was collected from the Monash University’s Web site (Australia), with over 7 million hits every week. Results are compared to other recent systems, showing that the system presented is by far promising."
84,85,random networks with tunable degree distribution and clustering,516.0,Random Networks with Tunable Degree Distribution and Clustering,"We present an algorithm for generating random networks with arbitrary degree distribution and Clustering (frequency of triadic closure). We use this algorithm to generate networks with exponential, power law, and poisson degree distributions with variable levels of clustering. Such networks may be used as models of social networks and as a testable null hypothesis about network structure. Finally, we explore the effects of clustering on the point of the phase transition where a giant component forms in a random network, and on the size of the giant component. Some analysis of these effects is presented."
85,86,the statistical mechanics of networks,525.0,The statistical mechanics of networks,"We study the family of network models derived by requiring the expected properties of a graph ensemble to match a given set of measurements of a real-world network, while maximizing the entropy of the ensemble. Models of this type play the same role in the study of networks as is played by the Boltzmann distribution in classical statistical mechanics; they offer the best prediction of network properties subject to the constraints imposed by a given set of observations. We give exact solutions of models within this class that incorporate arbitrary degree distributions and arbitrary but independent edge probabilities. We also discuss some more complex examples with correlated edges that can be solved approximately or exactly by adapting various familiar methods, including mean-field theory, perturbation theory, and saddle-point expansions."
86,87,bridging epistemologies the generative dance between organizational knowledge and organizational knowing,538.0,Bridging Epistemologies: The Generative Dance between Organizational Knowledge and Organizational Knowing,"Much current work on organizational knowledge, intellectual capital, knowledge-creating organizations, knowledge work, and the like rests on a single, traditional understanding of the nature of knowledge. We call this understanding the ""epistemology of possession,"" since it treats knowledge as something people possess. Yet, this epistemology cannot account for the knowing found in individual and group practice. Knowing as action calls for an ""epistemology of practice."" Moreover, the epistemology of possession tends to privilege explicit over tacit knowledge, and knowledge possessed by individuals over that possessed by groups. Current work on organizations is limited by this privileging and by the scant attention given to knowing in its own right. Organizations are better understood if explicit, tacit, individual and group knowledge are treated as four distinct and coequal forms of knowledge (each doing work the others cannot), and if knowledge and knowing are seen as mutually enabling (not competing). We hold that knowledge is a tool of knowing, that knowing is an aspect of our interaction with the social and physical world, and that the interplay of knowledge and knowing can generate new knowledge and new ways of knowing. We believe this generative dance between knowledge and knowing is a powerful source of organizational innovation. Harnessing this innovation calls for organizational and technological infrastructures that support the interplay of knowledge and knowing. Ultimately, these concepts make possible a more robust framing of such epistemologically-centered concerns as core competencies, the management of intellectual capital, etc. We explore these views through three brief case studies drawn from recent research. 10.1287/orsc.10.4.381"
87,88,sgd saccharomyces genome database,540.0,SGD: Saccharomyces Genome Database.,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/"
88,89,on the uniform generation of random graphs with prescribed degree sequences,548.0,On the uniform generation of random graphs with prescribed degree sequences,"Random graphs with prescribed degree sequences have been widely used as a model of complex networks. Comparing an observed network to an ensemble of such graphs allows one to detect deviations from randomness in network properties. Here we briefly review two existing methods for the generation of random graphs with arbitrary degree sequences, which we call the ``switching'' and ``matching'' methods, and present a new method based on the ``go with the winners'' Monte Carlo method. The matching method may suffer from non uniform sampling, while the switching method has no general theoretical bound on its mixing time. The ``go with the winners'' method has neither of these drawbacks, but is slow. It can however be used to evaluate there liability of the other two methods and, by doing this, we demonstrate that the deviations of the switching and matching algorithms under realistic conditions are small compared to the ``go with the winners'' algorithm. Because of its combination of speed and accuracy we recommend the use of the switching method for most calculations."
89,90,how can we think the complex,553.0,How can we think the complex?,"This chapter does not deal with specific tools and techniques for managing complex systems, but proposes some basic concepts that help us to think and speak about complexity. We review classical thinking and its intrinsic drawbacks when dealing with complexity. We then show how complexity forces us to take build models with indeterminacy and unpredictability. However, we can still deal with the problems created in this way by being adaptive, and profiting from a complex system’s capability for selforganization, and the distributed intelligence this may produce."
90,91,whats in a name,556.0,What's in a name?,"Among the several findings deriving from the application of complex network formalism to the investigation of natural phenomena, the fact that linguistic constructions follow power laws presents special interest for its potential implications for psychology and brain science. By corresponding to one of the most essentially human manifestations, such language-related properties suggest that similar dynamics may also be inherent to the brain areas related to language and associative memory, and perhaps even consciousness. The present work reports a preliminary experimental investigation aimed at characterizing and modeling the flow of sequentially induced associations between words from the English language in terms of complex networks. The data is produced through a psychophysical experiment where a word is presented to the subject, who is requested to associate another word. Complex network and graph theory formalism and measurements are applied in order to characterize the experimental data. Several interesting results are identified, including the characterization of attraction basins, association asymmetries, context biasing, as well as a possible power-law underlying word associations, which could be explained by the appearance of strange loops along the hierarchical structure underlying word categories."
91,92,zipfs law and the creation of musical context,570.0,Zipf's law and the creation of musical context,"This article discusses the extension of the notion of context from linguistics to the domain of music. In language, the statistical regularity known as Zipf's law - which concerns the frequency of usage of different words - has been quantitatively related to the process of text generation. This connection is established by Simon's model, on the basis of a few assumptions regarding the accompanying creation of context. Here, it is shown that the statistics of note usage in musical compositions are compatible with the predictions of Simon's model. This result, which gives objective support to the conceptual likeness of context in language and music, is obtained through automatic analysis of the digital versions of several compositions. As a by-product, a quantitative measure of context definiteness is introduced and used to compare tonal and atonal works."
92,93,modeling the evolution of weighted networks,576.0,Modeling the evolution of weighted networks,"We present a general model for the growth of weighted networks in which the structural growth is coupled with the edges’ weight dynamical evolution. The model is based on a simple weight-driven dynamics and a weights’ reinforcement mechanism coupled to the local network growth. That coupling can be generalized in order to include the effect of additional randomness and nonlinearities which can be present in real-world networks. The model generates weighted graphs exhibiting the statistical properties observed in several real-world systems. In particular, the model yields a nontrivial time evolution of vertices’ properties and scale-free behavior with exponents depending on the microscopic parameters characterizing the coupling rules. Very interestingly, the generated graphs spontaneously achieve a complex hierarchical architecture characterized by clustering and connectivity correlations varying as a function of the vertices’ degree."
93,94,social structure and opinion formation,644.0,Social Structure and Opinion Formation,"We present a dynamical theory of opinion formation that takes explicitly into account the structure of the social network in which in- dividuals are embedded. The theory predicts the evolution of a set of opinions through the social network and establishes the existence of a martingale property, i.e. that the expected weighted fraction of the population that holds a given opinion is constant in time. Most importantly, this weighted fraction is not either zero or one, but corresponds to a non-trivial distribution of opinions in the long time limit. This co-existence of opinions within a social network is in agreement with the often observed locality effect, in which an opinion or a fad is localized to given groups without infecting the whole society. We verified these predictions, as well as those concerning the fragility of opinions and the importance of highly connected individuals in opinion formation, by performing computer experiments on a number of social networks."
94,95,conservation and evolution of cisregulatory systems in ascomycete fungi,679.0,Conservation and Evolution of Cis-Regulatory Systems in Ascomycete Fungi,"Relatively little is known about the mechanisms through which gene expression regulation evolves. To investigate this, we systematically explored the conservation of regulatory networks in fungi by examining the cis -regulatory elements that govern the expression of coregulated genes. We first identified groups of coregulated Saccharomyces cerevisiae genes enriched for genes with known upstream or downstream cis -regulatory sequences. Reasoning that many of these gene groups are coregulated in related species as well, we performed similar analyses on orthologs of coregulated S. cerevisiae genes in 13 other ascomycete species. We find that many species-specific gene groups are enriched for the same flanking regulatory sequences as those found in the orthologous gene groups from S. cerevisiae , indicating that those regulatory systems have been conserved in multiple ascomycete species. In addition to these clear cases of regulatory conservation, we find examples of cis -element evolution that suggest multiple modes of regulatory diversification, including alterations in transcription factor-binding specificity, incorporation of new gene targets into an existing regulatory system, and cooption of regulatory systems to control a different set of genes. We investigated one example in greater detail by measuring the in vitro activity of the S. cerevisiae transcription factor Rpn4p and its orthologs from Candida albicans and Neurospora crassa . Our results suggest that the DNA binding specificity of these proteins has coevolved with the sequences found upstream of the Rpn4p target genes and suggest that Rpn4p has a different function in N. crassa ."
95,96,computational identification of developmental enhancers conservation and function of transcription factor bindingsite clusters in drosophila melanogaster and drosophila pseudoobscura,680.0,Computational identification of developmental enhancers: conservation and function of transcription factor binding-site clusters in Drosophila melanogaster and Drosophila pseudoobscura.,"BACKGROUND: The identification of sequences that control transcription in metazoans is a major goal of genome analysis. In a previous study, we demonstrated that searching for clusters of predicted transcription factor binding sites could discover active regulatory sequences, and identified 37 regions of the Drosophila melanogaster genome with high densities of predicted binding sites for five transcription factors involved in anterior-posterior embryonic patterning. Nine of these clusters overlapped known enhancers. Here, we report the results of in vivo functional analysis of 27 remaining clusters. RESULTS: We generated transgenic flies carrying each cluster attached to a basal promoter and reporter gene, and assayed embryos for reporter gene expression. Six clusters are enhancers of adjacent genes: giant, fushi tarazu, odd-skipped, nubbin, squeeze and pdm2; three drive expression in patterns unrelated to those of neighboring genes; the remaining 18 do not appear to have enhancer activity. We used the Drosophila pseudoobscura genome to compare patterns of evolution in and around the 15 positive and 18 false-positive predictions. Although conservation of primary sequence cannot distinguish true from false positives, conservation of binding-site clustering accurately discriminates functional binding-site clusters from those with no function. We incorporated conservation of binding-site clustering into a new genome-wide enhancer screen, and predict several hundred new regulatory sequences, including 85 adjacent to genes with embryonic patterns. CONCLUSIONS: Measuring conservation of sequence features closely linked to function--such as binding-site clusterin--makes better use of comparative sequence data than commonly used methods that examine only sequence identity."
97,98,shannon information and kolmogorov complexity,682.0,Shannon Information and Kolmogorov Complexity,"We compare the elementary theories of Shannon information and Kolmogorov complexity, the extent to which they have a common purpose, and where they are fundamentally different. We discuss and relate the basic notions of both theories: Shannon entropy versus Kolmogorov complexity, the relation of both to universal coding, Shannon mutual information versus Kolmogorov (`algorithmic') mutual information, probabilistic sufficient statistic versus algorithmic sufficient statistic (related to lossy compression in the Shannon theory versus meaningful information in the Kolmogorov theory), and rate distortion theory versus Kolmogorov's structure function. Part of the material has appeared in print before, scattered through various publications, but this is the first comprehensive systematic comparison. The last mentioned relations are new."
98,99,muscle a multiple sequence alignment method with reduced time and space complexity,692.0,MUSCLE: a multiple sequence alignment method with reduced time and space complexity,"BACKGROUND: In a previous paper, we introduced MUSCLE, a new program for creating multiple alignments of protein sequences, giving a brief summary of the algorithm and showing MUSCLE to achieve the highest scores reported to date on four alignment accuracy benchmarks. Here we present a more complete discussion of the algorithm, describing several previously unpublished techniques that improve biological accuracy and / or computational complexity. We introduce a new option, MUSCLE-fast, designed for high-throughput applications. We also describe a new protocol for evaluating objective functions that align two profiles. RESULTS: We compare the speed and accuracy of MUSCLE with CLUSTALW, Progressive POA and the MAFFT script FFTNS1, the fastest previously published program known to the author. Accuracy is measured using four benchmarks: BAliBASE, PREFAB, SABmark and SMART. We test three variants that offer highest accuracy (MUSCLE with default settings), highest speed (MUSCLE-fast), and a carefully chosen compromise between the two (MUSCLE-prog). We find MUSCLE-fast to be the fastest algorithm on all test sets, achieving average alignment accuracy similar to CLUSTALW in times that are typically two to three orders of magnitude less. MUSCLE-fast is able to align 1,000 sequences of average length 282 in 21 seconds on a current desktop computer. CONCLUSIONS: MUSCLE offers a range of options that provide improved speed and / or alignment accuracy compared with currently available programs. MUSCLE is freely available at http://www.drive5.com/muscle."
99,100,edutella a pp networking infrastructure based on rdf,722.0,EDUTELLA: A P2P Networking Infrastructure Based on RDF,"Metadata for the World Wide Web is important, but metadata for Peer-to-Peer (P2P) networks is absolutely crucial. In this paper we discuss the open source project Edutella which builds upon metadata standards defined for the WWW and aims to provide an RDF-based metadata infrastructure for P2P applications, building on the recently announced JXTA Framework. We describe the goals and main services this infrastructure will provide and the architecture to connect Edutella Peers based on exchange of RDF metadata. As the query service is one of the core services of Edutella, upon which other services are built, we specify in detail the Edutella Common Data Model (ECDM) as basis for the Edutella query exchange language (RDF-QEL-i) and format implementing distributed queries over the Edutella network. Finally, we shortly discuss registration and mediation services, and introduce the prototype and application scenario for our current Edutella aware peers."
100,101,the long memory of the efficient market,730.0,The long memory of the efficient market,"For the London Stock Exchange we demonstrate that the signs of orders obey a long-memory process. The autocorrelation function decays roughly as $\tau^{-\alpha}$ with $\alpha \approx 0.6$, corresponding to a Hurst exponent $H \approx 0.7$. This implies that the signs of future orders are quite predictable from the signs of past orders; all else being equal, this would suggest a very strong market inefficiency. We demonstrate, however, that fluctuations in order signs are compensated for by anti-correlated fluctuations in transaction size and liquidity, which are also long-memory processes. This tends to make the returns whiter. We show that some institutions display long-range memory and others don't."
101,102,complexity vs stability in smallworld networks,732.0,Complexity vs stability in small-world networks,"According to the May-Wigner stability theorem, increasing the complexity of a network inevitably leads to its destabilization, such that a small perturbation will be able to disrupt the entire system. One of the principal arguments against this observation is that it is valid only for random networks, and therefore does not apply to real-world networks, which presumably are structured. Here, we examine how the introduction of small-world topological structure into networks affects their stability. Our results indicate that, in structured networks, the parameter values at which the stability-instability transition occurs with increasing complexity is identical to that predicted by the May-Wigner criteria. However, the nature of the transition, as measured by the finite-size scaling exponent, appears to change as the network topology transforms from regular to random, with the small-world regime as the cross-over region. This behavior is related to the localization of the largest eigenvalues along the real axis in the eigenvalue plain with increasing regularity in the network."
102,103,the spatial structure of networks,736.0,The spatial structure of networks,"Abstract.&nbsp;&nbsp; We study networks that connect points in geographic space, such as transportation networks and the Internet. We ﬁnd that there are strong signatures in these networks of topography and use patterns, giving the networks shapes that are quite distinct from one another and from non-geographic networks. We oﬀer an explanation of these diﬀerences in terms of the costs and beneﬁts of transportation and communication, and give a simple model based on the Monte Carlo optimization of these costs and beneﬁts that reproduces well the qualitative features of the networks studied."
103,104,citation statistics from more than a century of physical review,741.0,Citation Statistics From More Than a Century of Physical Review,"We study the statistics of citations from all Physical Review journals for the 110-year period 1893 until 2003. In addition to characterizing the citation distribution and identifying publications with the highest citation impact, we investigate how citations evolve with time. There is a positive correlation between the number of citations to a paper and the average age of citations. Citations from a publication have an exponentially decaying age distribution; that is, old papers tend to not get cited. In contrast, the citations to a publication are consistent with a power-law age distribution, with an exponent close to -1 over a time range of 2 -- 20 years. We also identify a number of strongly-correlated citation bursts and other dramatic features in the time history of citations to individual publications."
104,105,analysis of relative gene expression data using realtime quantitative pcr and the ct method,756.0,Analysis of Relative Gene Expression Data Using Real-Time Quantitative PCR and the 2−ΔΔCT Method,"The two most commonly used methods to analyze data from real-time, quantitative PCR experiments are absolute quantification and relative quantification. Absolute quantification determines the input copy number, usually by relating the PCR signal to a standard curve. Relative quantification relates the PCR signal of the target transcript in a treatment group to that of another sample such as an untreated control. The 2−ΔΔCT method is a convenient way to analyze the relative changes in gene expression from real-time quantitative PCR experiments. The purpose of this report is to present the derivation, assumptions, and applications of the 2−ΔΔCT method. In addition, we present the derivation and applications of two variations of the 2−ΔΔCT method that may be useful in the analysis of real-time, quantitative PCR data."
105,106,document similarity using a phrase indexing graph model,762.0,Document Similarity Using a Phrase Indexing Graph Model,"Document clustering techniques mostly rely on single term analysis of text, such as the vector space model. To better capture the structure of documents, the underlying data model should be able to represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes Web documents based on phrases rather than on single terms only. The semistructured Web documents help in identifying potential phrases that when matched with other documents indicate strong similarity between the documents. The Document Index Graph captures this information, and finding significant matching phrases between documents becomes easy and efficient with such model. The model is flexible in that it could revert to a compact representation of the vector space model if we choose not to index phrases. However, using phrase indexing yields more accurate document similarity calculations. The similarity between documents is based on both single term weights and matching phrase weights. The combined similarities are used with standard document clustering techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, gives a more accurate measure of document similarity and thus significantly enhances Web document clustering quality. [PUBLICATION ABSTRACT]"
106,107,from kuramoto to crawford exploring the onset of synchronization in populations of coupled oscillators,769.0,From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators,"The Kuramoto model describes a large population of coupled limit-cycle oscillators whose natural frequencies are drawn from some prescribed distribution. If the coupling strength exceeds a certain threshold, the system exhibits a phase transition: some of the oscillators spontaneously synchronize, while others remain incoherent. The mathematical analysis of this bifurcation has proved both problematic and fascinating. We review 25 years of research on the Kuramoto model, highlighting the false turns as well as the successes, but mainly following the trail leading from Kuramoto's work to Crawford's recent contributions. It is a lovely winding road, with excursions through mathematical biology, statistical physics, kinetic theory, bifurcation theory, and plasma physics. (C) 2000 Elsevier Science B.V. All rights reserved."
107,108,the systems biology markup language sbml a medium for representation and exchange of biochemical network models,781.0,The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models,"Motivation: Molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.  Results: We summarize the Systems Biology Markup Language (SBML) Level 1, a free, open, XML-based format for representing biochemical reaction networks. SBML is a software-independent language for describing models common to research in many areas of computational biology, including cell signaling pathways, metabolic pathways, gene regulation, and others.  Availability: The specification of SBML Level 1 is freely available from http://www.sbml.org/  Contact: sysbio-team@caltech.edu 10.1093/bioinformatics/btg015"
108,109,a fast elitist nondominated sorting genetic algorithm for multiobjective optimization nsgaii,783.0,A Fast Elitist Non-Dominated Sorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II,"Multi-objective evolutionary algorithms which use non-dominated sorting and sharing have been mainly criticized for their (i) O(MN 3) computational complexity (where M is the number of objectives and N is the population size), (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In this paper, we suggest a non-dominated sorting based multi-objective evolutionary algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which alleviates all the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN 2) computational complexity is presented. Second, a selection operator is presented which creates a mating pool by combining the parent and child populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on five difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to PAES and SPEAâtwo other elitist multi-objective EAs which pay special attention towards creating a diverse Pareto-optimal front. Because of NSGA-IIâs low computational requirements, elitist approach, and parameter-less sharing approach, NSGA-II should find increasing applications in the years to come."
109,110,the similarity metric,808.0,The similarity metric,"A new class of metrics appropriate for measuring effective similarity relations between sequences, say one type of similarity per metric, is studied. We propose a new ""normalized information distance"", based on the noncomputable notion of Kolmogorov complexity, and show that it minorizes every metric in the class (that is, it is universal in that it discovers all effective similarities). We demonstrate that it too is a metric and takes values in [0, 1]; hence it may be called the  similarity metric . This is a theory foundation for a new general practical tool. We give two distinctive applications in widely divergent areas (the experiments by necessity use just computable approximations to the target notions). First, we computationally compare whole mitochondrial genomes and infer their evolutionary history. This results in a first completely automatic computed whole mitochondrial phylogeny tree. Secondly, we give fully automatically computed language tree of 52 different language based on translated versions of the ""Universal Declaration of Human Rights""."
110,111,introduction to random boolean networks,816.0,Introduction to Random Boolean Networks,"The goal of this tutorial is to promote interest in the study of random Boolean networks (RBNs). These can be very interesting models, since one does not have to assume any functionality or particular connectivity of the networks to study their generic properties. Like this, RBNs have been used for exploring the configurations where life could emerge. The fact that RBNs are a generalization of cellular automata makes their research a very important topic. The tutorial, intended for a broad audience, presents the state of the art in RBNs, spanning over several lines of research carried out by different groups. We focus on research done within artificial life, as we cannot exhaust the abundant research done over the decades related to RBNs."
111,112,a generalized approach to complex networks,823.0,A Generalized Approach to Complex Networks,"This work reports on how the formalization of complex network concepts in terms of discrete mathematics, especially mathematical morphology, allows a series of generalizations and important results ranging from new measurements of the network topology to new network growth models. First, the concepts of node degree and clustering coefficient are extended in order to characterize not only specific nodes, but any generic subnetwork. Second, the consideration of distance transform and rings are used to further extend those concepts in order to obtain a signature, instead of a single scalar measurement, ranging from the single node to whole graph scales. The use of the closing operation is also proposed as a means to analyze the proximity of 3-cycles along a complex network. The potential of such concepts is illustrated with respect to the random and Barab\'asi-Albert models, as well as distance-based preferential-attachment networks. A generalization of the Barab\'asi-Albert network, characterized by preferential attachment with respect to the generalized degree, is also proposed and shown to exhibit properties which are intermediate between the Barab\'asi-Albert and random models. Each of these cases is characterized and analyzed with respect to particularly relevant subnetworks contained in the original complex network, namely hubs, 3-cycles and outmost nodes."
112,113,generation of uncorrelated random scalefree networks,826.0,Generation of uncorrelated random scale-free networks,"Uncorrelated random scale-free networks are useful null models to check the accuracy and the analytical solutions of dynamical processes defined on complex networks. We propose and analyze a model capable of generating random uncorrelated scale-free networks with no multiple and self-connections. The model is based on the classical configuration model, with an additional restriction on the maximum possible degree of the vertices. We check numerically that the proposed model indeed generates scale-free networks with no two- and three-vertex correlations, as measured by the average degree of the nearest neighbors and the clustering coefficient of the vertices of degree k , respectively."
113,114,robustness in bacterial chemotaxis,841.0,Robustness in bacterial chemotaxis.,"Networks of interacting proteins orchestrate the responses of living cells to a variety of external stimuli, but how sensitive is the functioning of these protein networks to variations in their biochemical parameters? One possibility is that to achieve appropriate function, the reaction rate constants and enzyme concentrations need to be adjusted in a precise manner, and any deviation from these 'fine-tuned' values ruins the network's performance. An alternative possibility is that key properties of biochemical networks are robust; that is, they are insensitive to the precise values of the biochemical parameters. Here we address this issue in experiments using chemotaxis of Escherichia coli, one of the best-characterized sensory systems. We focus on how response and adaptation to attractant signals vary with systematic changes in the intracellular concentration of the components of the chemotaxis network. We find that some properties, such as steady-state behaviour and adaptation time, show strong variations in response to varying protein concentrations. In contrast, the precision of adaptation is robust and does not vary with the protein concentrations. This is consistent with a recently proposed molecular mechanism for exact adaptation, where robustness is a direct consequence of the network's architecture."
114,115,global prevalence of diabetes estimates for the year and projections for,865.0,Global prevalence of diabetes: estimates for the year 2000 and projections for 2030.,"OBJECTIVE: The goal of this study was to estimate the prevalence of diabetes and the number of people of all ages with diabetes for years 2000 and 2030. RESEARCH DESIGN AND METHODS: Data on diabetes prevalence by age and sex from a limited number of countries were extrapolated to all 191 World Health Organization member states and applied to United Nations' population estimates for 2000 and 2030. Urban and rural populations were considered separately for developing countries. RESULTS: The prevalence of diabetes for all age-groups worldwide was estimated to be 2.8% in 2000 and 4.4% in 2030. The total number of people with diabetes is projected to rise from 171 million in 2000 to 366 million in 2030. The prevalence of diabetes is higher in men than women, but there are more women with diabetes than men. The urban population in developing countries is projected to double between 2000 and 2030. The most important demographic change to diabetes prevalence across the world appears to be the increase in the proportion of people >65 years of age. CONCLUSIONS: These findings indicate that the ""diabetes epidemic"" will continue even if levels of obesity remain constant. Given the increasing prevalence of obesity, it is likely that these figures provide an underestimate of future diabetes prevalence."
115,116,what makes biochemical networks tick,881.0,What makes biochemical networks tick?,"In view of the increasing number of reported concentration oscillations in living cells, methods are needed that can identify the causes of these oscillations. These causes always derive from the influences that concentrations have on reaction rates. The influences reach over many molecular reaction steps and are defined by the detailed molecular topology of the network. So-called ‘autoinfluence paths’, which quantify the influence of one molecular species upon itself through a particular path through the network, can have positive or negative values. The former bring a tendency towards instability. In this molecular context a new graphical approach is presented that enables the classification of network topologies into oscillophoretic and nonoscillophoretic, i.e. into ones that can and ones that cannot induce concentration oscillations. The network topologies are formulated in terms of a set of uni-molecular and bi-molecular reactions, organized into branched cycles of directed reactions, and presented as graphs. Subgraphs of the network topologies are then classified as negative ones (which can) and positive ones (which cannot) give rise to oscillations. A subgraph is oscillophoretic (negative) when it contains more positive than negative autoinfluence paths. Whether the former generates oscillations depends on the values of the other subgraphs, which again depend on the kinetic parameters. An example shows how this can be established. By following the rules of our new approach, various oscillatory kinetic models can be constructed and analyzed, starting from the classified simplest topologies and then working towards desirable complications. Realistic biochemical examples are analyzed with the new method, illustrating two new main classes of oscillophore topologies."
116,117,robustness of cellular functions,889.0,Robustness of Cellular Functions,"Robustness, the ability to maintain performance in the face of perturbations and uncertainty, is a long-recognized key property of living systems. Owing to intimate links to cellular complexity, however, its molecular and cellular basis has only recently begun to be understood. Theoretical approaches to complex engineered systems can provide guidelines for investigating cellular robustness because biology and engineering employ a common set of basic mechanisms in different combinations. Robustness may be a key to understanding cellular complexity, elucidating design principles, and fostering closer interactions between experimentation and theory."
117,118,the protein folding network,890.0,The Protein Folding Network,"The conformation space of a 20 residue antiparallel beta-sheet peptide, sampled by molecular dynamics simulations, is mapped to a network. Snapshots saved along the trajectory are grouped according to secondary structure into nodes of the network and the transitions between them are links. The conformation space network describes the significant free energy minima and their dynamic connectivity without requiring arbitrarily chosen reaction coordinates. As previously found for the Internet and the World-Wide Web as well as for social and biological networks, the conformation space network is scale-free and contains highly connected hubs like the native state which is the most populated free energy basin. Furthermore, the native basin exhibits a hierarchical organization, which is not found for a random heteropolymer lacking a predominant free-energy minimum. The network topology is used to identify conformations in the folding transition state (TS) ensemble, and provides a basis for understanding the heterogeneity of the TS and denatured state ensemble as well as the existence of multiple pathways."
118,119,efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs,891.0,Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs,"Summary: Biological and engineered networks have recently been shown to display network motifs: a small set of characteristic patterns that occur much more frequently than in randomized networks with the same degree sequence. Network motifs were demonstrated to play key information processing roles in biological regulation networks. Existing algorithms for detecting network motifs act by exhaustively enumerating all subgraphs with a given number of nodes in the network. The runtime of such algorithms increases strongly with network size. Here, we present a novel algorithm that allows estimation of subgraph concentrations and detection of network motifs at a runtime that is asymptotically independent of the network size. This algorithm is based on random sampling of subgraphs. Network motifs are detected with a surprisingly small number of samples in a wide variety of networks. Our method can be applied to estimate the concentrations of larger subgraphs in larger networks than was previously possible with exhaustive enumeration algorithms. We present results for high-order motifs in several biological networks and discuss their possible functions.  Availability: A software tool for estimating subgraph concentrations and detecting network motifs (mfinder 1.1) and further information is available at http://www.weizmann.ac.il/mcb/UriAlon/ 10.1093/bioinformatics/bth163"
120,121,semantic blogging and decentralized knowledge management,938.0,Semantic blogging and decentralized knowledge management,"In the Semantic Web research group at Hewlett-Packard Laboratories, Bristol, we frequently circulate items of interest (such as news articles, software tools, links to Web sites, and competitor information). We call them snippets, or information nuggets, we would like to store, annotate, and share. Email is not the ideal medium for these tasks; its transient nature means the snippets are effectively lost over time. Yet the risk from using a more formal process, like a centralized database, is that it is both cumbersome to use (a barrier to entry) and overly rigid in its data model (not amenable to storing different types of information). Our need illustrates what I call decentralized, informal knowledge management."
122,123,reappraising information seeking behaviour in a digital environment bouncers checkers returnees and the like,945.0,"Re-appraising information seeking behaviour in a digital environment: Bouncers, checkers, returnees and the like","Collating data from a number of log and questionnaire studies conducted largely into the use of a range of consumer health digital information platforms, Centre for Information Behaviour and the Evaluation of Research (Ciber) researchers describe some new thoughts on characterising (and naming) information seeking behaviour in the digital environment, and in so doing, suggest a new typology of digital users. The characteristic behaviour found is one of bouncing in which users seldom penetrate a site to any depth, tend to visit a number of sites for any given information need and seldom return to sites they once visited. They tend to feed for information horizontally, and whether they search a site of not depends heavily on digital visibility, which in turn creates all the conditions for bouncing. The question whether this type of information seeking represents a form of dumbing down or up, and what it all means for publishers, librarians and information providers, who might be working on other, possible outdated usage paradigms, is discussed."
123,124,detection of multistability bifurcations and hysteresis in a large class of biological positivefeedback systems,946.0,"Detection of multistability, bifurcations, and hysteresis in a large class of biological positive-feedback systems.","It is becoming increasingly clear that bistability (or, more generally, multistability) is an important recurring theme in cell signaling. {B}istability may be of particular relevance to biological systems that switch between discrete states, generate oscillatory responses, or ""remember"" transitory stimuli. {S}tandard mathematical methods allow the detection of bistability in some very simple feedback systems (systems with one or two proteins or genes that either activate each other or inhibit each other), but realistic depictions of signal transduction networks are invariably much more complex. {H}ere, we show that for a class of feedback systems of arbitrary order the stability properties of the system can be deduced mathematically from how the system behaves when feedback is blocked. {P}rovided that this open-loop, feedback-blocked system is monotone and possesses a sigmoidal characteristic, the system is guaranteed to be bistable for some range of feedback strengths. {W}e present a simple graphical method for deducing the stability behavior and bifurcation diagrams for such systems and illustrate the method with two examples taken from recent experimental studies of bistable systems: a two-variable {C}dc2/{W}ee1 system and a more complicated five-variable mitogen-activated protein kinase cascade."
124,125,emergence of complex dynamics in a simple model of signaling networks,979.0,Emergence of Complex Dynamics in a Simple Model of Signaling Networks,"10.1073/pnas.0404843101 Various physical, social, and biological systems generate complex fluctuations with correlations across multiple time scales. In physiologic systems, these long-range correlations are altered with disease and aging. Such correlated fluctuations in living systems have been attributed to the interaction of multiple control systems; however, the mechanisms underlying this behavior remain unknown. Here, we show that a number of distinct classes of dynamical behaviors, including correlated fluctuations characterized by 1/ scaling of their power spectra, can emerge in networks of simple signaling units. We found that, under general conditions, complex dynamics can be generated by systems fulfilling the following two requirements, () a “small-world” topology and () the presence of noise. Our findings support two notable conclusions. First, complex physiologic-like signals can be modeled with a minimal set of components; and second, systems fulfilling conditions  and  are robust to some degree of degradation (i.e., they will still be able to generate 1/ dynamics)."
125,126,semantic web services,986.0,Semantic Web services,"hose properties, capabilities,  interfaces, and effects are encoded in an unambiguous,  machine-understandable form.  The realization of the Semantic Web is underway  with the development of new AI-inspired content  markup languages, such as OIL,  3  DAML+OIL  (www.daml.org/2000/10/daml-oil), and DAML-L (the  last two are members of the DARPA Agent Markup  Language (DAML) family of languages).  4  These languages  have a well-defined semantics and enable the  markup and manipulation of complex taxonomic and  logical relations between entities on the Web. A fundamental  component of the Semantic Web will be the markup of Web services to make them computer-interpretable, use-apparent, and agent-ready. This article addresses precisely this component.  We present an approach to Web service markup that provides an agent-independent declarative API capturing the data and metadata associated with a service together with specifications of its pro"
126,127,genomewide detection of tissuespecific alternative splicing in the human transcriptome,1008.0,Genomeâwide detection of tissueâspecific alternative splicing in the human transcriptome,"We have developed an automated method for discovering tissueâspecific regulation of alternative splicing through a genomeâwide analysis of expressed sequence tags (ESTs). Using this approach, we have identified 667 tissueâspecific alternative splice forms of human genes. We validated our muscleâspecific and brainâspecific splice forms for known genes. A high fraction (8/10) were reported to have a matching tissue specificity by independent studies in the published literature. The number of tissueâspecific alternative splice forms is highest in brain, while eye_retina, muscle, skin, testis and lymph have the greatest enrichment of tissueâspecific splicing. Overall, 10â30% of human alternatively spliced genes in our data show evidence of tissueâspecific splice forms. Seventyâeight percent of our tissueâspecific alternative splices appear to be novel discoveries. We present bioinformatics analysis of several tissueâspecific splice forms, including automated protein isoform sequence and domain prediction, showing how our data can provide valuable insights into gene function in different tissues. For example, we have discovered a novel kidneyâspecific alternative splice form of the WNK1 gene, which appears to specifically disrupt its Nâterminal kinase domain and may play a role in PHAII hypertension. Our database greatly expands knowledge of tissueâspecific alternative splicing and provides a comprehensive dataset for investigating its functional roles and regulation in different human tissues."
127,128,inferring quantitative models of regulatory networks from expression data,1019.0,Inferring quantitative models of regulatory networks from expression data.,"Motivation: Genetic networks regulate key processes in living cells. Various methods have been suggested to reconstruct network architecture from gene expression data. However, most approaches are based on qualitative models that provide only rough approximations of the underlying events, and lack the quantitative aspects that are critical for understanding the proper function of biomolecular systems.  Results: We present fine-grained dynamical models of gene transcription and develop methods for reconstructing them from gene expression data within the framework of a generative probabilistic model. Unlike previous works, we employ quantitative transcription rates, and simultaneously estimate both the kinetic parameters that govern these rates, and the activity levels of unobserved regulators that control them. We apply our approach to expression datasets from yeast and show that we can learn the unknown regulator activity profiles, as well as the binding affinity parameters. We also introduce a novel structure learning algorithm, and demonstrate its power to accurately reconstruct the regulatory network from those datasets. 10.1093/bioinformatics/bth941"
128,129,using bayesian networks to analyze expression data,1021.0,Using Bayesian networks to analyze expression data.,"DNA hybridization arrays simultaneously measure the expression level for thousands of genes. These measurements provide a “snapshot ” of transcription levels within the cell. A major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. In this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. This framework builds on the use of Bayesian networks for representing statistical dependencies. A Bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. Such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. We start by showing how Bayesian networks can describe interactions between genes. We then describe a method for recovering gene interactions from microarray data using tools for learning Bayesian networks. Finally, we demonstrate this method on the S. cerevisiae cell-cycle measurements of Spellman et al. (1998). Key words: gene expression, microarrays, Bayesian methods. 1."
129,130,combining location and expression data for principled discovery of genetic regulatory network models,1022.0,Combining location and expression data for principled discovery of genetic regulatory network models.,"We develop principled methods for the automatic induction (discovery) of genetic regulatory network models from multiple data sources and data modalities. Models of regulatory networks are represented as Bayesian networks, allowing the models to compactly and robustly capture probabilistic multivariate statistical dependencies between the various cellular factors in these networks. We build on previous Bayesian network validation results by extending the validation framework to the context of model induction, leveraging heuristic simulated annealing search algorithms and posterior model averaging. Using expression data in isolation yields results inconsistent with location data so we incorporate genomic location data to guide the model induction process. We combine these two data modalities by allowing location data to influence the model prior and expression data to influence the model likelihood. We demonstrate the utility of this approach by discovering genetic regulatory models of thirty-three variables involved in S. cerevisiae pheromone response. The models we automatically generate are consistent with the current understanding regarding this regulatory network, but also suggest new directions for future experimental investigation. 1"
130,131,using graphical models and genomic expression data to statistically validate models of genetic regulatory networks,1023.0,Using graphical models and genomic expression data to statistically validate models of genetic regulatory networks.,"We propose a model-driven approach for analyzing genomic expression data that permits genetic regulatory networks to be represented in a biologically interpretable computational form. Our models permit latent variables capturing unobserved factors, describe arbitrarily complex (more than pair-wise) relationships at varying levels of refinement, and can be scored rigorously against observational data. The models that we use are based on Bayesian networks and their extensions. As a demonstration of this approach, we utilize 52 genomes worth of Affymetrix GeneChip expression data to correctly differentiate between alternative hypotheses of the galactose regulatory network in S. cerevisiae. When we extend the graph semantics to permit annotated edges, we are able to score models describing relationships at a finer degree of specification. 1 Introduction The vast quantity of data generated by genomic expression arrays affords researchers a significant opportunity to transform biology, medicine, and pharmacology using systematic computational methods. The availability of genomic (and eventually proteomic) expression data promises to have a profound impact on the understanding of basic cellular processes, the diagnosis and treatment of disease, and the efficacy of designing and delivering targeted therapeutics. Particularly relevant to these objectives is the development of a deeper understanding of the various mechanisms by which cells control and regulate the transcription of their genes. In this paper, we present a principled method for using genomic expression data to elucidate these genetic regulatory networks. While the potential utility of expression data is immense, some obstacles"
131,132,advances to bayesian network inference for generating causal networks from observational biological data,1024.0,Advances to Bayesian network inference for generating causal networks from observational biological data,"Motivation: Network inference algorithms are powerful computational tools for identifying putative causal interactions among variables from observational data. Bayesian network inference algorithms hold particular promise in that they can capture linear, non-linear, combinatorial, stochastic and other types of relationships among variables across multiple levels of biological organization. However, challenges remain when applying these algorithms to limited quantities of experimental data collected from biological systems. Here, we use a simulation approach to make advances in our dynamic Bayesian network (DBN) inference algorithm, especially in the context of limited quantities of biological data.  Results: We test a range of scoring metrics and search heuristics to find an effective algorithm configuration for evaluating our methodological advances. We also identify sampling intervals and levels of data discretization that allow the best recovery of the simulated networks. We develop a novel influence score for DBNs that attempts to estimate both the sign (activation or repression) and relative magnitude of interactions among variables. When faced with limited quantities of observational data, combining our influence score with moderate data interpolation reduces a significant portion of false positive interactions in the recovered networks. Together, our advances allow DBN inference algorithms to be more effective in recovering biological networks from experimentally collected data.  Availability: Source code and simulated data are available upon request.  Supplementary information: http://www.jarvislab.net/Bioinformatics/BNAdvances/ 10.1093/bioinformatics/bth448"
132,133,a survey of approaches to automatic schema matching,1039.0,A survey of approaches to automatic schema matching,"Abstract. Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component."
133,134,cowl contextualizing ontologies,1043.0,C-OWL: Contextualizing ontologies,"Ontologies are shared models of a domain that encode a view which is common to a set of different parties. Contexts are local models that encode a party's subjective view of a domain. In this paper we show how ontologies can be contextualized, thus acquiring certain useful properties that a pure shared approach cannot provide. We say that an ontology is contextualized or, also, that it is a contextual ontology, when its contents are kept local, and therefore not shared with other ontologies, and mapped with the contents of other ontologies via explicit (context) mappings. The result is Context OWL (C-OWL), a language whose syntax and semantics have been obtained by extending the OWL syntax and semantics to allow for the representation of contextual ontologies."
134,135,integrated semanticsyntactic video modeling for search and browsing,1064.0,Integrated Semantic-Syntactic Video Modeling for Search and Browsing,"Video processing and computer vision communities usually employ shot-based or object-based structural video models and associate low-level (color, texture, shape, and motion) and semantic descriptions (textual annotations) with these structural (syntactic) elements. Database and information retrieval communities, on the other hand, employ entity-relation or object-oriented models to model the semantics of multimedia documents. This paper proposes a new generic integrated semantic-syntactic video model to include all of these elements within a single framework to enable structured video search and browsing combining textual and low-level descriptors. The proposed model includes semantic entities (video objects and events) and the relations between them. We introduce a new ""actor"" entity to enable grouping of object roles in specific events. This context-dependent classification of attributes of an object allows for more efficient browsing and retrieval. The model also allows for decomposition of events into elementary motion units and elementary reaction/interaction units in order to access mid-level semantics and low-level video features. The instantiations of the model are expressed as graphs. Users can formulate flexible queries that can be translated into such graphs. Alternatively, users can input query graphs by editing an abstract model (model template). Search and retrieval is accomplished by matching the query graph with those instantiated models in the database. Examples and experimental results are provided to demonstrate the effectiveness of the proposed integrated modeling and querying framework."
136,137,time as essence for photo browsing through personal digital libraries,1100.0,Time as essence for photo browsing through personal digital libraries,"We developed two photo browsers for collections with thousands of time-stamped digital images. Modern digital cameras record photo shoot times, and semantically related photos tend to occur in bursts. Our browsers exploit the timing information to structure the collections and to automatically generate meaningful summaries. The browsers differ in how users navigate and view the structured collections. We conducted user studies to compare the two browsers and an un-summarized image browser. Our results show that exploiting the time dimension and appropriately summarizing collections can lead to significant improvements. For example, for one task category, one of our browsers enabled a 33\\ improvement in speed of finding given images compared to the commercial browser. Similarly, users were able to complete 29% more tasks when using this same browser."
137,138,automatic organization for digital photographs with geographic coordinates,1101.0,Automatic organization for digital photographs with geographic coordinates,"Uses time and location information in the EXIF header to automatically organise a personal photograph collection. Foruses on auto-genereating a structure that can be used WITHOUT A MAP. Makes a nice point about the inneficient use of screen real estate when using maps, and that zooming an paning could be cumbersome. The paper presents the algorithm for grouping by time and location for event detection. The tool generates a text caption to describe (by location) each grouping--this is crucial since no map. Description/evaluation of the user interface is left for future work. Evaluation: the algorithms are tested on three image collections (2580, 1192, 1823 images) . This could be interesting to look at again when writing the machine learning stuff."
138,139,two supervised learning approaches for name disambiguation in author citations,1108.0,Two supervised learning approaches for name disambiguation in author citations,"Due to name abbreviations, identical names, name misspellings,  and pseudonyms in publications or bibliographies (citations), an  author may have multiple names and multiple authors may share  the same name. Such name ambiguity affects the performance  of document retrieval, web search, database integration, and may  cause improper attribution to authors. This paper investigates two  supervised learning approaches to disambiguate authors in the citations.   One approach uses the naive Bayes probability model, a  generative model; the other uses Support Vector Machines(SVMs)  [The Nature of Statistical Learning Theory] and the vector space representation of citations,   a discriminative model. Both approaches utilize three types of citation attributes:  co-author names, the title of the paper, and the title of  the journal or proceeding. We illustrate these two approaches on  two types of data, one collected from the web, mainly publication  lists from homepages, the other collected from the DBLP citation databases."
139,140,toward an understanding of the motivation of open source software developers,1109.0,Toward an Understanding of the Motivation of Open Source Software Developers,"An Open Source Software (OSS) project is unlikely to  be successful unless there is an accompanied community  that provides the platform for developers and users to  collaborate. Members of such communities are volunteers  whose motivation to participate and contribute is of  essential importance to the success of OSS projects. In  this paper, we aim to create an understanding of what  motivates people to participate in OSS communities. We  theorize that learning is one of the motivational forces.  Our theory is grounded in the learning theory of  Legitimate Peripheral Participation, and is supported by  analyzing the social structure of OSS communities and the  co-evolution between OSS systems and communities. We  also discuss practical implications of our theory for  creating and maintaining sustainable OSS communities as  well as for software engineering research and education."
140,141,bibliometrics and beyond some thoughts on webbased citation analysis,1113.0,Bibliometrics and beyond: some thoughts on web-based citation analysis,"The idea of a unified citation index to the literature of science was first outlined by Eugene Garfield [1] in 1955 in the journal Science. Science  Citation Index has since established itself as the gold standard for scientific information retrieval. It has also become the database of choice for citation analysts and evaluative bibliometricians worldwide. As scientific publication moves to the web, and novel approaches to scholarly communication  and peer review establish themselves, new methods of citation and link analysis will emerge to capture often liminal expressions of peer esteem, influence and approbation. The web thus affords bibliometricians rich opportunities to apply   and adapt their techniques to new contexts and content: the age of  bibliometric spectroscopy' [2] is dawning. 10.1177/016555150102700101"
141,142,adding semantics to web services standards,1125.0,Adding semantics to web services standards,"With the increasing growth in popularity of Web services, discovery of relevant Web services becomes a significant challenge. One approach is to develop semantic Web services where by the Web services are annotated based on shared ontologies, and use these annotations for semantics-based discovery of relevant Web services. We discuss one such approach that involves adding semantics to WSDL using DAML+OIL ontologies. Our approach also uses UDDI to store these semantic annotations and search for..."
142,143,analysis of a very large web search engine query log,1126.0,Analysis of a very large web search engine query log,"In this paper we present an analysis of an AltaVista Search Engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. This represents almost 285 million user sessions, each an attempt to fill a single information need. We present an analysis of individual queries, query duplication, and query sessions. We also present results of a correlation analysis of the log entries, studying the interaction of terms within queries. Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. This suggests that traditional information retrieval techniques may not work well for answering web search requests. The correlation analysis showed that the most highly correlated items are constituents of phrases. This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such."
143,144,relevant term suggestion in interactive web search based on contextual information in query session logs,1130.0,Relevant term suggestion in interactive web search based on contextual information in query session logs,"Abstract 10.1002/asi.10256.abs This paper proposes an effective term suggestion approach to interactive Web search. Conventional approaches to making term suggestions involve extracting co-occurring keyterms from highly ranked retrieved documents. Such approaches must deal with term extraction difficulties and interference from irrelevant documents, and, more importantly, have difficulty extracting terms that are conceptually related but do not frequently co-occur in documents. In this paper, we present a new, effective log-based approach to relevant term extraction and term suggestion. Using this approach, the relevant terms suggested for a user query are those that co-occur in similar query sessions from search engine logs, rather than in the retrieved documents. In addition, the suggested terms in each interactive search step can be organized according to its relevance to the entire query session, rather than to the most recent single query as in conventional approaches. The proposed approach was tested using a proxy server log containing about two million query transactions submitted to search engines in Taiwan. The obtained experimental results show that the proposed approach can provide organized and highly relevant terms, and can exploit the contextual information in a user's query session to make more effective suggestions."
144,145,agglomerative clustering of a search engine query log,1132.0,Agglomerative clustering of a search engine query log,"This paper introduces a technique for mining a collection of user transactions with an Internet search engine to discover clusters of similar queries and similar URLs. The information we exploit is clickthrough data: each record consists of a user's query to a search engine along with the URL which the user selected from among the candidates offered by the search engine. By viewing this dataset as a bipartite graph, with the vertices on one side corresponding to queries and on the other side..."
145,146,regionbased memory management,1134.0,Region-Based Memory Management,"This paper describes a memory management discipline for programs that perform dynamic memory allocation and de-allocation. At runtime, all values are put into regions. The store consists of a stack of regions. All points of region allocation and deallocation are inferred automatically, using a type and effect based program analysis. The scheme does not assume the presence of a garbage collector. The scheme was first presented by Tofte and Talpin (1994); subsequently, it has been tested in The ML Kit with Regions, a region-based, garbage-collection free implementation of the Standard ML Core language, which includes recursive datatypes, higher-order functions and updatable references (Birkedal et al. 96, Elsman and Hallenberg 95). This paper defines a region-based dynamic semantics for a skeletal programming language extracted from Standard ML. We present the inference system which specifies where regions can be allocated and de-allocated and a detailed proof that the system is sound wi..."
146,147,language support for regions,1136.0,Language Support for Regions,"Region-based memory management systems structure memory by grouping objects in regions under program control. Memory is reclaimed by deleting regions, freeing all objects stored therein. Our compiler for C with regions, RC, prevents unsafe region deletions by keeping a count of references to each region. Using type annotations that make the structure of a program's regions more explicit, we reduce the overhead of reference counting from a maximum of 27\% to a maximum of 11\% on a suite of realistic benchmarks. We generalise these annotations in a region type system whose main novelty is the use of existentially quantified abstract regions to represent pointers to objects whose region is partially or totally unknown. A distribution of RC is available at http://www.cs.berkeley.edu/~dgay/rc.tar.gz."
148,149,expertise recommender a flexible recommendation system and architecture,1159.0,Expertise recommender: a flexible recommendation system and architecture,"Locating the expertise necessary to solve difficult problems is a nuanced social and collaborative problem. In organizations, some people assist others in locating expertise by making referrals. People who make referrals fill key organizational roles that have been identified by CSCW and affiliated research. Expertise locating systems are not designed to replace people who fill these key organizational roles. Instead, expertise locating systems attempt to decrease workload and support people who have no other options. Recommendation systems are collaborative software that can be applied to expertise locating. This work describes a general recommendation architecture that is grounded in a field study of expertise locating. Our expertise recommendation system details the work necessary to fit expertise recommendation to a work setting. The architecture and implementation begin to tease apart the technical aspects of providing good recommendations from social and collaborative concerns."
149,150,fotofile a consumer multimedia organization and retrieval system,1161.0,FotoFile: a consumer multimedia organization and retrieval system,"FotoFile blends automatic and human annotation methods. Users express frustration re: organizing and retrieving digital images. Conducted focus groups to gain insight into the percieved tradeoffs between: manual vs. automated annotattion and direct search vs. browsing. Different focus groups were held for business and home participants. Keyword based search easiest to grasp, but time requuirements seen as daunting. Automated indexing (content based) desired, but home users thought they'd do keywords more, browsing more desired by home users than business. The primary orgaizational metaphor is an album, and useres can specify the representative image to aid in recognition. Batch annotation is supported, annotation and search use the same basic mechanism. Storytelling is discussed as a means of facilitating annotation--allows for small grouping of photos, part of a story--called scraplets, to be named and annotated. Using the same photos in mutliple scraplets links them implicitly--links are shown during album playback to remind of multiple possible story lines. Face recognition is implemented and black rectangles are placed on thumbnails to indicate recognized faces--users are prompted to confirm person's identity, all images auto annoted w/ the person's name. Metadata is visualized using the hyperbolic tree. Good discussion of the difficulties in designing meaningful studies in this area."
150,151,composable and compilable macros you want it when,1164.0,Composable and compilable macros:: you want it when?,"Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme---the language of the PLT Scheme tool suite---addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros."
152,153,a deoxyribozymebased molecular automaton,1181.0,A deoxyribozyme-based molecular automaton,"We describe a molecular automaton, called MAYA, which encodes a version of the game of tic-tac-toe and interactively competes against a human opponent. The automaton is a Boolean network of deoxyribozymes that incorporates 23 molecular-scale logic gates and one constitutively active deoxyribozyme arrayed in nine wells (3x3) corresponding to the game board. To make a move, MAYA carries out an analysis of the input oligonucleotide keyed to a particular move by the human opponent and indicates a move by fluorescence signaling in a response well. The cycle of human player input and automaton response continues until there is a draw or a victory for the automaton. The automaton cannot be defeated because it implements a perfect strategy."
153,154,refactoring objectoriented frameworks,1189.0,Refactoring object-oriented frameworks,"This thesis denes a set of program restructuring operations (refactorings) that support the design, evolution and reuse of object-oriented application frameworks. The focus of the thesis is on automating the refactorings in a way that preserves the behavior of a program. The refactorings are dened to be behavior preserving, provided that their preconditions are met. Most of the refactorings are simple to implement and it is almost trivial to show that they are behavior preserving. However, for a few refactorings, one or more of their preconditions are in general undecidable. Fortunately, for some cases it can be determined whether these refactorings can be applied safely. Three of the most complex refactorings are dened in detail: generalizing the inheritance hierarchy, specializing the inheritance hierarchy and using aggregations to model the relationships among classes. These operations are decomposed into more primitive parts, and the power of these operations is discussed from the perspectives of automatability and usefulness in supporting design. Two design constraints needed in refactoring are class invariants and exclusive components. These constraints are needed to ensure that behavior is preserved across some refactorings. This thesis gives some conservative algorithms for determining whether a program satises these constraints, and describes how to use this design information to refactor a program."
157,158,five reasons for scenariobased design,1265.0,Five reasons for scenario-based design,"Scenarios of human-computer interaction help us to understand and to create computer systems and applications as artifacts of human activity&#209; as things to learn from, as tools to use in one's work, as media for interacting with other people. Scenario-based design of information technology addresses five technical challenges: Scenarios evoke reflection in the content of design work, helping developers coordinate design action and reflection. Scenarios are at once concrete and flexible, helping developers manage the fluidy of design situations. Scenarios afford multiple views of an interaction, diverse kinds and amounts of detailing, helping developers manage the many consequences entailed by any given design move. Scenarios can also be abstracted and categorized, helping designers to recognize, capture, and reuse generalizations, and to address the challenge that technical knowledge often lags the needs of technical design. Finally, scenarios promote work-oriented communication among stakeholders, helping to make design activities more accessible to the great variety of expertise that can contribute to design, and addressing the challenge that external constraints designers and clients often distract attention from the needs and concerns of the people who will use the technology."
158,159,citeseerapi towards seamless resource location and interlinking for digital libraries,1266.0,CiteSeer-API: towards seamless resource location and interlinking for digital libraries,"We introduce CiteSeer-API, a public API to CiteSeer-like services. CiteSeer-API is SOAP/WSDL based and allows for easy programmatical access to all the specific functionalities offered by CiteSeer services, including full text search of documents and citations and citation-based document discovery. In order to enable operability and interlinking with arbitrary software agents and digital library systems, CiteSeer-API uses digital content signatures to create system-independent handles for the..."
159,160,modeling network dynamics the lac operon a case study,1268.0,"Modeling network dynamics: the lac operon, a case study","We use the lac operon in Escherichia coli as a prototype system to illustrate the current state, applicability, and limitations of modeling the dynamics of cellular networks. We integrate three different levels of description (molecular, cellular, and that of cell population) into a single model, which seems to capture many experimental aspects of the system. [Journal Article, Review, Review, Tutorial; 26 Refs; In English; United States]"
161,162,robust perfect adaptation in bacterial chemotaxis through integral feedback control,1277.0,Robust perfect adaptation in bacterial chemotaxis through integral feedback control,"Integral feedback control is a basic engineering strategy for ensuring that the output of a system robustly tracks its desired value independent of noise or variations in system parameters. In biological systems, it is common for the response to an extracellular stimulus to return to its prestimulus value even in the continued presence of the signal—a process termed adaptation or desensitization. Barkai, Alon, Surette, and Leibler have provided both theoretical and experimental evidence that the precision of adaptation in bacterial chemotaxis is robust to dramatic changes in the levels and kinetic rate constants of the constituent proteins in this signaling network [Alon, U., Surette, M. G., Barkai, N. & Leibler, S. (1998) Nature (London) 397, 168–171]. Here we propose that the robustness of perfect adaptation is the result of this system possessing the property of integral feedback control. Using techniques from control and dynamical systems theory, we demonstrate that integral control is structurally inherent in the Barkai–Leibler model and identify and characterize the key assumptions of the model. Most importantly, we argue that integral control in some form is necessary for a robust implementation of perfect adaptation. More generally, integral control may underlie the robustness of many homeostatic mechanisms."
169,170,robustness in simple biochemical networks,1303.0,Robustness in simple biochemical networks.,"Cells use complex networks of interacting molecular components to transfer and process information. These ""computational devices of living cells""1 are responsible for many important cellular processes, including cell-cycle regulation and signal transduction. Here we address the issue of the sensitivity of the networks to variations in their biochemical parameters. We propose a mechanism for robust adaptation in simple signal transduction networks. We show that this mechanism applies in particular to bacterial chemotaxis2, 3, 4, 5, 6, 7. This is demonstrated within a quantitative model which explains, in a unified way, many aspects of chemotaxis, including proper responses to chemical gradients8, 9, 10, 11, 12. The adaptation property10, 13, 14, 15, 16 is a consequence of the network's connectivity and does not require the 'fine-tuning' of parameters. We argue that the key properties of biochemical networks should be robust in order to ensure their proper functioning."
170,171,association of genes to genetically inherited diseases using data mining,1309.0,Association of genes to genetically inherited diseases using data mining,"Although approximately one-quarter of the roughly 4,000 genetically inherited diseases currently recorded in respective databases (LocusLink1, OMIM2) are already linked to a region of the human genome, about 450 have no known associated gene. Finding disease-related genes requires laborious examination of hundreds of possible candidate genes (sometimes, these are not even annotated; see, for example, refs 3,4). The public availability of the human genome5 draft sequence has fostered new strategies to map molecular functional features of gene products to complex phenotypic descriptions, such as those of genetically inherited diseases. Owing to recent progress in the systematic annotation of genes using controlled vocabularies6, we have developed a scoring system for the possible functional relationships of human genes to 455 genetically inherited diseases that have been mapped to chromosomal regions without assignment of a particular gene. In a benchmark of the system with 100 known disease-associated genes, the disease-associated gene was among the 8 best-scoring genes with a 25% chance, and among the best 30 genes with a 50% chance, showing that there is a relationship between the score of a gene and its likelihood of being associated with a particular disease. The scoring also indicates that for some diseases, the chance of identifying the underlying gene is higher."
172,173,template metaprogramming for haskell,1342.0,Template meta-programming for Haskell,"We propose a new extension to the purely functional programming language Haskell that supports  compile-time meta-programming.  The purpose of the system is to support the  algorithmic  construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc."
174,175,proper tail recursion and space efficiency,1350.0,Proper tail recursion and space efficiency,"The IEEE/ANSI standard for Scheme requires implementations to be properly tail recursive. This ensures that portable code can rely upon the space efficiency of continuation-passing style and other idioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within a tail context. When examined closely, proper tail recursion also depends upon the fact that garbage collection can be asymptotically more space-efficient than Algol-like stack allocation. Proper..."
175,176,control flow analysis in scheme,1354.0,Control flow analysis in scheme,"Traditional flow analysis techniques, such as the ones typically employed by optimizing Fortran compilers, do not work for Scheme-like languages. This paper presents a flow analysis technique &mdash;  control flow analysis  &mdash; which is applicable to Scheme-like languages. As a demonstration application, the information gathered by control flow analysis is used to perform a traditional flow analysis problem, induction variable elimination. Extensions and limitations are discussed.   The techniques presented in this paper are backed up by working code. They are applicable not only to Scheme, but also to related languages, such as Common Lisp and ML."
177,178,contracts for higherorder functions,1362.0,Contracts for higher-order functions,"Assertions play an important role in the construction of robust software. Their use in programming languages dates back to the 1970s. Eiffel, an object-oriented programming language, wholeheartedly adopted assertions and developed the ""Design by Contract"" philosophy. Indeed, the entire object-oriented community recognizes the value of assertion-based contracts on methods.In contrast, languages with higher-order functions do not support assertion-based contracts. Because predicates on functions are, in general, undecidable, specifying such predicates appears to be meaningless. Instead, the functional languages community developed type systems that statically approximate interesting predicates.In this paper, we show how to support higher-order function contracts in a theoretically well-founded and practically viable manner. Specifically, we introduce &#955; con , a typed lambda calculus with assertions for higher-order functions. The calculus models the assertion monitoring system that we employ in DrScheme. We establish basic properties of the model (type soundness, etc.) and illustrate the usefulness of contract checking with examples from DrScheme's code base.We believe that the development of an assertion system for higher-order functions serves two purposes. On one hand, the system has strong practical potential because existing type systems simply cannot express many assertions that programmers would like to state. On the other hand, an inspection of a large base of invariants may provide inspiration for the direction of practical future type system research."
178,179,units cool modules for hot languages,1379.0,Units: cool modules for HOT languages,"A module system ought to enable  assembly-line programming  using separate compilation and an expressive linking language. Separate compilation allows programmers to develop parts of a program independently. A linking language gives programmers precise control over the assembly of parts into a whole. This paper presents models of  program units , MzScheme's module language for assembly-line programming. Units support separate compilation, independent module reuse, cyclic dependencies, hierarchical structuring, and dynamic linking. The models explain how to integrate units with untyped and typed languages such as Scheme and ML."
179,180,a callbyneed lambda calculus,1380.0,A call-by-need lambda calculus,"The mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of call-by-need and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies,  e.g. , the call-by-need continuation passing transformation and the realization of sharing via assignments."
180,181,on the expressive power of programming languages,1381.0,On the expressive power of programming languages,"The literature on programming languages contains an abundance of informal claims on the relative expressive power of programming languages, but there is no framework for formalizing such statements nor for deriving interesting consequences. As a first step in this direction, we develop a formal notion of expressiveness and investigate its properties. To validate the theory, we analyze some widely held beliefs about the expressive power of several extensions of functional languages. Based on these results, we believe that our system correctly captures many of the informal ideas on expressiveness, and that it constitutes a foundation for further research in this direction. 1 Comparing Programming Languages The literature on programming languages contains an abundance of informal claims on the expressive power of programming languages. Arguments in these contexts typically assert the expressibility or non-expressibility of programming constructs relative to a language. Unfortunately, pro..."
181,182,catching bugs in the web of program invariants,1382.0,Catching bugs in the web of program invariants,"MrSpidey is a user-friendly, interactive static debugger for Scheme. A static debugger supplements the standard debugger by analyzing the program and pinpointing those program operations that may cause run-time errors such as dereferencing the null pointer or applying non-functions. The program analysis of MrSpidey computes value set descriptions for each term in the program and constructs a value flow graph connecting the set descriptions. Using the set descriptions, MrSpidey can identify and highlight potentially erroneous program operations, whose cause the programmer can then explore by selectively exposing portions of the value flow graph."
182,183,synthesizing objectoriented and functional design to promote reuse,1385.0,Synthesizing Object-Oriented and Functional Design to Promote Re-Use,"Many problems require recursively specified types of data and a collection of tools that operate on those data. Over time, these problems evolve so that the programmer must extend the toolkit or extend the types and adjust the existing tools accordingly. Ideally, this should be done without modifying existing code. Unfortunately, the prevailing program design strategies do not support both forms of extensibility: functional programming accommodates the addition of tools, while object-oriented programming supports either adding new tools or extending the data set, but not both. In this paper, we present a composite design pattern that synthesizes the best of both approaches and in the process resolves the tension between the two design strategies. We also show how this protocol suggests a new set of linguistic facilities for languages that support class systems."
183,184,modular objectoriented programming with units and mixins,1393.0,Modular object-oriented programming with units and mixins,"Module and class systems have evolved to meet the demand for reuseable software components. Considerable effort has been invested in developing new module and class systems, and in demonstrating how each promotes code reuse. However, relatively little has been said about the interaction of these constructs, and how using modules and classes together  can improve programs. In this paper, we demonstrate the synergy of a particular form of modules and classes---called units and mixins,..."
184,185,how to design programs an introduction to programming and computing,1396.0,How to design programs: an introduction to programming and computing,"{This introduction to programming places computer science in the core of a liberal arts education. Unlike other introductory books, it focuses on the program design process. This approach fosters a variety of skills--critical reading, analytical thinking, creative synthesis, and attention to detail--that are important for everyone, not just future computer programmers.<br /> <br /> The book exposes readers to two fundamentally new ideas. First, it presents program design guidelines that show the reader how to analyze a problem statement; how to formulate concise goals; how to make up examples; how to develop an outline of the solution, based on the analysis; how to finish the program; and how to test. Each step produces a well-defined intermediate product. Second, the book comes with a novel programming environment, the first one explicitly designed for beginners. The environment grows with the readers as they master the material in the book until it supports a full-fledged language for the whole spectrum of programming tasks.<br /> <br /> All the book's support materials are available for free on the Web. The Web site includes the environment, teacher guides, exercises for all levels, solutions, and additional projects.}"
185,186,macros as multistage computations typesafe generative binding macros in macroml,1403.0,"Macros as multi-stage computations: type-safe, generative, binding macros in MacroML","With few exceptions, macros have traditionally been viewed as operations on syntax trees or even on plain strings. This view makes macros seem ad hoc, and is at odds with two desirable features of contemporary typed functional languages: static typing and static scoping. At a deeper level, there is a need for a simple, usable semantics for macros. This paper argues that these problems can be addressed by formally viewing macros as multi-stage computations. This view eliminates the need for freshness conditions and tests on variable names, and provides a compositional interpretation that can serve as a basis for designing a sound type system for languages supporting macros, or even for compilation. To illustrate our approach, we develop and present MacroML, an extension of ML that supports inlining, recursive macros, and the definition of new binding constructs. The latter is subtle, and is the most novel addition in a statically typed setting. The semantics of a core subset of MacroML is given by an interpretation into MetaML, a statically-typed multi-stage programming language. It is then easy to show that MacroML is stage- and type-safe: macro expansion does not depend on runtime evaluation, and both stages do not ""go wrong."
187,188,metabolic pathways in the postgenome era,1414.0,Metabolic pathways in the post-genome era.,"Metabolic pathways are a central paradigm in biology. Historically, they have been defined on the basis of their step-by-step discovery. However, the genome-scale metabolic networks now being reconstructed from annotation of genome sequences demand new network-based definitions of pathways to facilitate analysis of their capabilities and functions, such as metabolic versatility and robustness, and optimal growth rates. This demand has led to the development of a new mathematically based analysis of complex, metabolic networks that enumerates all their unique pathways that take into account all requirements for cofactors and byproducts. Applications include the design of engineered biological systems, the generation of testable hypotheses regarding network structure and function, and the elucidation of properties that can not be described by simple descriptions of individual components (such as product yield, network robustness, correlated reactions and predictions of minimal media). Recently, these properties have also been studied in genome-scale networks. Thus, network-based pathways are emerging as an important paradigm for analysis of biological systems."
188,189,a hybrid approach for searching in the semantic web,1417.0,A hybrid approach for searching in the semantic web,"This paper presents a search architecture that combines classical search techniques with spread activation techniques applied to a semantic model of a given domain. Given an ontology, weights are assigned to links based on certain properties of the ontology, so that they measure the strength of the relation. Spread activation techniques are used to find related concepts in the ontology given an initial set of concepts and corresponding initial activation values. These initial values are obtained from the results of classical search applied to the data associated with the concepts in the ontology. Two test cases were implemented, with very positive results. It was also observed that the proposed hybrid spread activation, combining the symbolic and the sub-symbolic approaches, achieved better results when compared to each of the approaches alone."
189,190,adaptive web search based on user profile constructed without any effort from users,1418.0,Adaptive web search based on user profile constructed without any effort from users,"Web search engines help users find useful information on the World Wide Web (WWW). However, when the same query is submitted by different users, typical search engines return the same result regardless of who submitted the query. Generally, each user has different information needs for his/her query. Therefore, the search result should be adapted to users with different information needs. In this paper, we first propose several approaches to adapting search results according to each user's need for relevant information without any user effort, and then verify the effectiveness of our proposed approaches. Experimental results show that search systems that adapt to each user's preferences can be achieved by constructing user profiles based on modified collaborative filtering with detailed analysis of user's browsing history in one day."
191,192,ngrambased text categorization,1430.0,N-Gram-Based Text Categorization,"Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8% correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80% correct classification rate. There are also several obvious directions for improving the system's classification performance in those cases where it did not do as well. The system is based on calculating and comparing profiles of N-gram frequencies. First, we use the system to compute profiles on training set data that represent the various categories, e.g.,language samples or newsgroup content samples. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document's profile and each of the category profiles. The system selects the category whose profile has the smallest distance to the document's profile. The profiles involved are quite small, typically 10K bytes for a category training set, and less than 4K bytes for an individual document. Using N-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks."
192,193,blogging by the rest of us,1433.0,Blogging by the rest of us,"Weblogs (or blogs) are frequently updated webpages with posts typically in reverse-chronological order. Blogging is the latest form of online communication to gain widespread popularity and it is rapidly becoming mainstream. Media attention tends to focus on ""heavy-hitting"" blogs devoted to politics, punditry and technology, but it has recently become apparent that vast majority of blogs are written by ordinary people for much smaller audiences, and on largely personal themes. Surprisingly little is known about this ""blogging by the rest of us"", especially from the blogger's point of view. This paper presents the preliminary results of an ethnographic study of blogging as a form of personal expression and communication. We characterize a number of blogging practices, and then consider blogging as personal journaling. We find blogging to be a surprisingly versatile medium, with uses similar to an online diary, personal chronicle or newsletter, and much more. The next few years should provide a fascinating opportunity for research and design as blogging tools improve and blog usage evolves and flourishes."
194,195,assessing the effects of library instruction,1477.0,Assessing the Effects of Library Instruction,"The current study sought to measure the influence of a one-hour library training/orientation session on college students' library use and library skill development. Data analysis revealed a statistically significant increase in student library use while there was no statistically significant increase in library skill development. Some issues, however, limited the utility of the findings. This paper also critiques the study's design with the intention of helping future researchers plan for occurrences that may limit the accuracy of their findings."
195,196,using blackboard in library instruction addressing the learning styles of generations x and y,1478.0,Using Blackboard in Library Instruction: Addressing the Learning Styles of Generations X and Y,"Studies show that recent generations of college students have a learning style with identifiable characteristics. Library instruction efforts must adapt to these learning styles. Course management software (CMS), such as Blackboard, is one resource available to academic librarians to meet the challenges posed by the ``Net Generation.'' At Stetson University, the use of Blackboard courseware in library instruction sessions successfully addressed the unique learning styles of students."
196,197,texttiling segmenting text into multiparagraph subtopic passages,1541.0,TextTiling: segmenting text into multi-paragraph subtopic passages,"TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization."
197,198,on defining library and information science as applied philosophy of information,1546.0,On Defining Library and Information Science as Applied Philosophy of Information,"This paper analyses the relations between philosophy of information (PI), library and information science (LIS) and social epistemology (SE). In the first section, it is argued that there is a natural relation between philosophy and LIS but that SE cannot provide a satisfactory foundation for LIS. SE should rather be seen as sharing with LIS a common ground, represented by the study of information, to be investigated by a new discipline, PI. In the second section, the nature of PI is outlined as the philosophical area that studies the conceptual nature of information, its dynamics and problems. In the third section, LIS is defined as a form of applied PI. The hypothesis supported is that PI should replace SE as the philosophical discipline that can best provide the conceptual foundation for LIS. In the conclusion, it is suggested that the &#039;identity&#039; crisis undergone by LIS has been the natural outcome of a justified but precocious search for a philosophical counterpart that has emerged only recently: namely, PI. The development of LIS should not rely on some borrowed, pre-packaged theory. As applied PI, LIS can fruitfully contribute to the growth of basic theoretical research in PI itself and thus provide its own foundation."
198,199,embodying information systems the contribution of phenomenology,1559.0,Embodying information systems: the contribution of phenomenology,"This paper presents a case for embodying information systems. That is, for recognizing the fundamental importance of the body in human cognition and social action, and exploring the consequences for information systems and artificial intelligence. Current work within philosophy, biology, cognitive science, and social theory demonstrates that the Cartesian dualism of mind and body is no longer tenable, and points to the embodied and enactive nature of thought and language. Three different approaches to cognition are identified and their underlying philosophies are exemplified by Husserl, Heidegger, and Merleau-Ponty respectively. Sections of the paper cover: a philosophical and biological framework for embodied cognition; the main arguments in favor of the approach; and the implications for information systems and artificial intelligence."
199,200,learning to cluster web search results,1597.0,Learning to cluster web search results,"Organizing Web search results into clusters facilitates users' quick browsing through search results. Traditional clustering techniques are inadequate since they don't generate clusters with highly readable names. In this paper, we reformalize the clustering problem as a salient phrase ranking problem. Given a query and the ranked list of documents (typically a list of titles and snippets) returned by a certain Web search engine, our method first extracts and ranks salient phrases as candidate cluster names, based on a regression model learned from human labeled training data. The documents are assigned to relevant salient phrases to form candidate clusters, and the final clusters are generated by merging these candidate clusters. Experimental results verify our method's feasibility and effectiveness."
200,201,a common open representation of mass spectrometry data and its application to proteomics research,1668.0,A common open representation of mass spectrometry data and its application to proteomics research,"A broad range of mass spectrometers are used in mass spectrometry (MS)-based proteomics research. Each type of instrument possesses a unique design, data system and performance specifications, resulting in strengths and weaknesses for different types of experiments. Unfortunately, the native binary data formats produced by each type of mass spectrometer also differ and are usually proprietary. The diverse, nontransparent nature of the data structure complicates the integration of new instruments into preexisting infrastructure, impedes the analysis, exchange, comparison and publication of results from different experiments and laboratories, and prevents the bioinformatics community from accessing data sets required for software development. Here, we introduce the 'mzXML' format, an open, generic XML (extensible markup language) representation of MS data. We have also developed an accompanying suite of supporting programs. We expect that this format will facilitate data management, interpretation and dissemination in proteomics research."
201,202,making sense of it all bacterial chemotaxis,1939.0,Making sense of it all: bacterial chemotaxis.,"Bacteria must be able to respond to a changing environment, and one way to respond is to move. The transduction of sensory signals alters the concentration of small phosphorylated response regulators that bind to the rotary flagellar motor and cause switching. This simple pathway has provided a paradigm for sensory systems in general. However, the increasing number of sequenced bacterial genomes shows that although the central sensory mechanism seems to be common to all bacteria, there is added complexity in a wide range of species."
203,204,generalized venn diagrams a new method of visualizing complex genetic set relations,2090.0,Generalized Venn diagrams: a new method of visualizing complex genetic set relations.,"MOTIVATION: Microarray experiments generate vast amounts of data. The unknown or only partially known functional context of differentially expressed genes may be assessed by querying the Gene Ontology database via GOMiner. Resulting tree representations are difficult to interpret and are not suited for visualization of this type of data. Methods are needed to effectively visualize these complex set relationships. RESULTS: We present a visualization approach for set relationships based on Venn diagrams. The proposed extension enhances the usual notion of Venn diagrams by incorporating set size information. The cardinality of the sets and intersection sets is represented by their corresponding circle (polygon) sizes. To avoid local minima, solutions to this problem are sought by evolutionary optimization. This generalized Venn diagram approach has been implemented as an interactive Java application (VennMaster) specifically designed for use with GOMiner in the context of the Gene Ontology database. AVAILABILITY: VennMaster is platform-independent (Java 1.4.2) and has been tested on Windows (XP, 2000), Mac OS X, and Linux. Supplementary information and the software (free for non-commercial use) are available at http://www.informatik.uni-ulm.de/ni/mitarbeiter/HKestler/vennm together with a user documentation. CONTACT: hans.kestler@medizin.uni-ulm.de."
204,205,integrated genomic and proteomic analyses of a systematically perturbed metabolic network,2100.0,Integrated Genomic and Proteomic Analyses of a Systematically Perturbed Metabolic Network,"We demonstrate an integrated approach to build, test, and refine a model of a cellular pathway, in which perturbations to critical pathway components are analyzed using DNA microarrays, quantitative proteomics, and databases of known physical interactions. Using this approach, we identify 997 messenger RNAs responding to 20 systematic perturbations of the yeast galactose-utilization pathway, provide evidence that approximately 15 of 289 detected proteins are regulated posttranscriptionally, and identify explicit physical interactions governing the cellular response to each perturbation. We refine the model through further iterations of perturbation and global measurements, suggesting hypotheses about the regulation of galactose utilization and physical interactions between this and a variety of other metabolic pathways."
205,206,transcriptional regulatory networks in saccharomyces cerevisiae,2107.0,Transcriptional Regulatory Networks in Saccharomyces cerevisiae,"We have determined how most of the transcriptional regulators encoded in the eukaryote {S}accharomyces cerevisiae associate with genes across the genome in living cells. {J}ust as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. {W}e use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. {O}ur results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators."
206,207,programmable syntax macros,2108.0,Programmable Syntax Macros,"Lisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP #the C preprocessor#, syntax macros operate on Abstract Syntax Trees #ASTs#. Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual..."
207,208,network structure and the diffusion of knowledge,2118.0,Network structure and the diffusion of knowledge,"This paper models knowledge diffusion as a barter process in which agents exchange different types of knowledge. This is intended to capture the observed practice of informal knowledge trading. Agents are located on a network and are directly connected with a small number of other agents. Agents repeatedly meet those with whom direct connections exist and trade if mutually profitable trades exist. In this way knowledge diffuses throughout the economy. We examine the relationship between network architecture and diffusion performance. We consider the space of structures that fall between, at one extreme, a network in which every agent is connected to  n  nearest neighbours, and at the other extreme a network with each agent being connected to, on average,  n  randomly chosen agents. We find that the performance of the system exhibits clear ‘small world’ properties, in that the steady-state level of average knowledge is maximal when the structure is a small world (that is, when most connections are local, but roughly 10 percent of them are long distance). The variance of knowledge levels among agents is maximal in the small world region, whereas the coefficient of variation is minimal. We explain these results as reflecting the dynamics of knowledge transmission as affected by the architecture of connections among agents."
208,209,gene regulatory network growth by duplication,2124.0,Gene regulatory network growth by duplication,"We are beginning to elucidate transcriptional regulatory networks on a large scale and to understand some of the structural principles of these networks, but the evolutionary mechanisms that form these networks are still mostly unknown. {H}ere we investigate the role of gene duplication in network evolution. {G}ene duplication is the driving force for creating new genes in genomes: at least 50\% of prokaryotic genes and over 90\% of eukaryotic genes are products of gene duplication. {T}he transcriptional interactions in regulatory networks consist of multiple components, and duplication processes that generate new interactions would need to be more complex. {W}e define possible duplication scenarios and show that they formed the regulatory networks of the prokaryote {E}scherichia coli and the eukaryote {S}accharomyces cerevisiae. {G}ene duplication has had a key role in network evolution: more than one-third of known regulatory interactions were inherited from the ancestral transcription factor or target gene after duplication, and roughly one-half of the interactions were gained during divergence after duplication. {I}n addition, we conclude that evolution has been incremental, rather than making entire regulatory circuits or motifs by duplication with inheritance of interactions."
209,210,a proposed framework for the description of plant metabolomics experiments and their results,2825.0,A proposed framework for the description of plant metabolomics experiments and their results,"The study of the metabolite complement of biological samples, known as metabolomics, is creating large amounts of data, and support for handling these data sets is required to facilitate meaningful analyses that will answer biological questions. We present a data model for plant metabolomics known as ArMet (architecture for metabolomics). It encompasses the entire experimental time line from experiment definition and description of biological source material, through sample growth and preparation to the results of chemical analysis. Such formal data descriptions, which specify the full experimental context, enable principled comparison of data sets, allow proper interpretation of experimental results, permit the repetition of experiments and provide a basis for the design of systems for data storage and transmission. The current design and example implementations are freely available (http://www.armet.org/). We seek to advance discussion and community adoption of a standard for metabolomics, which would promote principled collection, storage and transmission of experiment data."
210,211,improved monomeric red orange and yellow fluorescent proteins derived from discosoma sp red fluorescent protein,2837.0,"Improved monomeric red, orange and yellow fluorescent proteins derived from Discosoma sp. red fluorescent protein","Fluorescent proteins are genetically encoded, easily imaged reporters crucial in biology and biotechnology1, 2. When a protein is tagged by fusion to a fluorescent protein, interactions between fluorescent proteins can undesirably disturb targeting or function3. Unfortunately, all wild-type yellow-to-red fluorescent proteins reported so far are obligately tetrameric and often toxic or disruptive4, 5. The first true monomer was mRFP1, derived from the Discosoma sp. fluorescent protein “DsRed” by directed evolution first to increase the speed of maturation6, then to break each subunit interface while restoring fluorescence, which cumulatively required 33 substitutions7. Although mRFP1 has already proven widely useful, several properties could bear improvement and more colors would be welcome. We report the next generation of monomers. The latest red version matures more completely, is more tolerant of N-terminal fusions and is over tenfold more photostable than mRFP1. Three monomers with distinguishable hues from yellow-orange to red-orange have higher quantum efficiencies."
211,212,foundations for the study of software architecture,2960.0,Foundations for the study of software architecture,"The purpose of this paper is to build the foundation for software architecture. We first develop an intuition for software architecture by appealing to several well-established architectural disciplines. On the basis of this intuition, we present a model of software architec-ture that consists of three components: elements, form, and rationale. Elements are either processing, data, or connecting elements. Form is defined in terms of the properties of, and the relationships among, the elements-- that is, the constraints on the elements. The ratio-nale provides the underlying basis for the architecture in terms of the system constraints, which most often derive from the system:requirements. We discuss the compo-nents of the model in the context of both architectures and architectural styles and present an extended exam-ple to illustrate some important architecture and style considerations. We conclude by presenting some of the benefits of our approach to software architecture, sum-marizing our contributions, and relating our approach to other current work. 1"
212,213,locationaided routing lar in mobile ad hoc networks,3005.0,Location‐Aided Routing (LAR) in mobile ad hoc networks,"Abstract&nbsp;&nbsp;A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location‐Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms."
213,214,genetic networks with canalyzing boolean rules are always stable,3060.0,Genetic networks with canalyzing Boolean rules are always stable,"We determine stability and attractor properties of random Boolean genetic network models with canalyzing rules for a variety of architectures. For all power law, exponential, and flat in-degree distributions, we find that the networks are dynamically stable. Furthermore, for architectures with few inputs per node, the dynamics of the networks is close to critical. In addition, the fraction of genes that are active decreases with the number of inputs per node. These results are based upon investigating ensembles of networks using analytical methods. Also, for different in-degree distributions, the numbers of fixed points and cycles are calculated, with results intuitively consistent with stability analysis; fewer inputs per node implies more cycles, and vice versa. There are hints that genetic networks acquire broader degree distributions with evolution, and hence our results indicate that for single cells, the dynamics should become more stable with evolution. However, such an effect is very likely compensated for by multicellular dynamics, because one expects less stability when interactions among cells are included. We verify this by simulations of a simple model for interactions among cells."
215,216,traits composable units of behavior,3123.0,Traits: Composable Units of Behavior,"Inheritance is the fundamental reuse mechanism in object-oriented programming languages; its most prominent variants are single inheritance, multiple inheritance, and mixin inheritance. In the first part of this paper, we identify and illustrate the conceptual and practical reusability problems that arise with these forms of inheritance. We then present a simple compositional model for structuring object-oriented programs, which we call traits. Traits are essentially groups of methods that serve as building blocks for classes and are primitive units of code reuse. In this model, classes are composed from a set of traits by specifying glue code that connects the traits together and accesses the necessary state. We demonstrate how traits overcome the problems arising with the different variants of inheritance, we discuss how traits can be implemented effectively, and we summarize our experience applying traits to refactor an existing class hierarchy."
216,217,efficient identification of web communities,3128.0,Efficient Identification of Web Communities,"We define a community on the web as a set of sites that have more links (in either direction) to members of the community than to non-members. Members of such a community can be efficiently identified in a maximum flow / minimum cut framework, where the source is composed of known members, and the sink consists of well-known non-members. A focused crawler that crawls to a fixed depth can approximate community membership by augmenting the graph induced by the crawl with links to a virtual sink..."
217,218,real life real users and real needs a study and analysis of user queries on the web,3190.0,"Real life, real users, and real needs: a study and analysis of user queries on the web",". We analyzed transaction logs containing 51,473 queries posed by 18,113 users of Excite, a major Internet search service. We provide data on: (i) sessions - changes in queries during a session, number of pages viewed, and use of relevance feedback, (ii) queries - the number of search terms, and the use of logic and modifiers, and (iii) terms - their rank/frequency distribution and the most highly used search terms. We then shift the focus of analysis from the query to the user to gain insight to the characteristic of the Web user. With these characteristics as a basis, we then conducted a failure analysis, identifying trends among user mistakes. We conclude with a summary of findings and a discussion of the implications of these finding.  INTRODUCTION  A panel session at the 1997 ACM Special Interest Group on Research Issues In Information Retrieval conference entitled &#034;Real Life Information Retrieval: Commercial Search Engines&#034; included representatives from several Internet search se..."
221,222,growing a language,3210.0,Growing a language,"is not the first till it comes to nought; then the first count is the sum.  4    2    5        0    Four plus two is the same as five plus one, which is the same as six plus nought, which is six.  We shall take the word many to mean more than two in number.  Think of a machine that can keep track of two numbers, and count each one up or down, and test if a number be nought and by such a test choose to do this or that. The list of things that it can do and of choices that it can make must be..."
222,223,packrat parsing simple powerful lazy linear time,3219.0,"Packrat Parsing: Simple, Powerful, Lazy, Linear Time","Packrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(k) or LR(k) grammar can be recognized by a packrat parser, in addition to many languages that conventional linear-time algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a linear-time packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives."
224,225,phobos a frontend approach to extensible compilers,3247.0,Phobos: A front-end approach to extensible compilers,"This paper describes a practical approach for implementing domain-specific languages with extensible compilers. Given a compiler with one or more front-end languages, we introduce the idea of a generic front-end that allows the syntactic and semantic specification of domainspecific languages. Phobos, our generic front-end, offers modular language specification, allowing the programmer to define new syntax and semantics incrementally."
225,226,searching the world wide web,3271.0,Searching the World Wide Web,"The coverage and recency of the major World Wide Web search engines was analyzed, yielding some surprising results. The coverage of any one engine is significantly limited: No single engine indexes more than about one-third of the &#034;indexable Web,&#034; the coverage of the six engines investigated varies by an order of magnitude, and combining the results of the six engines yields about 3.5 times as many documents on average as compared with the results from only one engine. Analysis of the overlap between pairs of engines gives an estimated lower bound on the size of the indexable Web of 320 million pages."
226,227,the pagerank citation ranking bringing order to the web,3283.0,The PageRank Citation Ranking: Bringing Order to the Web,"The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation. 1"
227,228,random boolean network models and the yeast transcriptional network,3284.0,Random Boolean network models and the yeast transcriptional network,"10.1073/pnas.2036429100 The recently measured yeast transcriptional network is analyzed in terms of simplified Boolean network models, with the aim of determining feasible rule structures, given the requirement of stable solutions of the generated Boolean networks. We find that, for ensembles of generated models, those with canalyzing Boolean rules are remarkably stable, whereas those with random Boolean rules are only marginally stable. Furthermore, substantial parts of the generated networks are frozen, in the sense that they reach the same state, regardless of initial state. Thus, our ensemble approach suggests that the yeast network shows highly ordered dynamics."
228,229,organizing programs without classes,3298.0,Organizing Programs Without Classes,". All organizational functions carried out by classes can be accomplished in a simple and natural way by object inheritance in classless languages, with no need for special mechanisms. A single model---dividing types into prototypes and traits---supports sharing of behavior and extending or replacing representations. A natural extension, dynamic object inheritance, can model behavioral modes. Object inheritance can also be used to provide structured name spaces for well-known objects. Classless ..."
229,230,phylogenetic estimation of contextdependent substitution rates by maximum likelihood,3303.0,Phylogenetic estimation of context-dependent substitution rates by maximum likelihood.,"{Nucleotide substitution in both coding and noncoding regions is context-dependent, in the sense that substitution rates depend on the identity of neighboring bases. Context-dependent substitution has been modeled in the case of two sequences and an unrooted phylogenetic tree, but it has only been accommodated in limited ways with more general phylogenies. In this article, extensions are presented to standard phylogenetic models that allow for better handling of context-dependent substitution, yet still permit exact inference at reasonable computational cost. The new models improve goodness of fit substantially for both coding and noncoding data. Considering context dependence leads to much larger improvements than does using a richer substitution model or allowing for rate variation across sites, under the assumption of site independence. The observed improvements appear to derive from three separate properties of the models: their explicit characterization of context-dependent substitution within N-tuples of adjacent sites, their ability to accommodate overlapping N-tuples, and their rich parameterization of the substitution process. Parameter estimation is accomplished using an expectation maximization algorithm, with a quasi-Newton algorithm for the maximization step; this approach is shown to be preferable to ordinary Newton methods for parameter-rich models. Overlapping tuples are efficiently handled by assuming Markov dependence of the observed bases at each site on those at the N - 1 preceding sites, and the required conditional probabilities are computed with an extension of Felsenstein's algorithm. Estimated substitution rates based on a data set of about 160,000 noncoding sites in mammalian genomes indicate a pronounced CpG effect, but they also suggest a complex overall pattern of context-dependent substitution, comprising a variety of subtle effects. Estimates based on about 3 million sites in coding regions demonstrate that amino acid substitution rates can be learned at the nucleotide level, and suggest that context effects across codon boundaries are significant.}"
230,231,types and programming languages,3316.0,Types and programming languages,"{A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems--and of programming languages from a type-theoretic perspective-has important applications in software engineering, language design, high-performance compilers, and security.<br /> <br /> This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.<br /> <br /> The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.}"
231,232,the derivative of a regular type is its type of onehole contexts,3318.0,The derivative of a regular type is its type of one-hole contexts,"Polymorphic regular types are tree-like datatypes generated by polynomial type expressions over a set of free variables and closed under least ﬁxed point. The 'equality types' of Core ML can be expressed in this form. Given such a type expression T with x free, this paper shows a way to represent the one-hole contexts for elements of x within elements of T, together with an operation which will plug an element of x into the hole of such a context. One-hole contexts are given as inhabitants of a regular type dxT, computed generically from the syntactic structure of T by a mechanism better known as partial differentiation. The relevant notion of containment is shown to be appropriately characterized in terms of derivatives and plugging in. The technology is then exploited to give the one-hole contexts for sub-elements of recursive types in a manner similar to Huet's 'zippers.'"
232,233,resource usage analysis,3320.0,Resource usage analysis,"It is an important criterion of program correctness that a program accesses resources in a valid manner. For example, a memory region that has been allocated should be eventually deallocated, and after the deallocation, the region should no longer be accessed. A file that has been opened should be eventually closed. So far, most of the methods to analyze this kind of property have been proposed in rather specific contexts (like studies of memory management and verification of usage of lock primitives), and it was not so clear what is the essence of those methods or how methods proposed for individual problems are related. To remedy this situation, we formalize a general problem of analyzing resource usage as a resource usage analysis problem, and propose a type-based method as a solution to the problem."
233,234,from system f to typed assembly language,3322.0,From system F to typed assembly language,"We motivate the design of a statically typed assembly language (TAL) and present a typepreserving translation from System F to TAL. The TAL we present is based on a conventional RISC assembly language, but its static type system provides support for enforcing high-level language abstractions, such as closures, tuples, and user-defined abstract data types. The type system ensures that well-typed programs cannot violate these abstractions. In addition, the typing constructs admit most low-level..."
234,235,mining specifications,3418.0,Mining specifications,"Program verification is a promising approach to improving program quality, because it can search all possible program executions for specific errors. However, the need to formally describe correct behavior or errors is a major barrier to the widespread adoption of program verification, since programmers historically have been reluctant to write formal specifications. Automating the process of formulating specifications would remove a barrier to program verification and enhance its practicality.This paper describes  specification mining , a machine learning approach to discovering formal specifications of the protocols that code must obey when interacting with an application program interface or abstract data type. Starting from the assumption that a working program is well enough debugged to reveal strong hints of correct protocols, our tool infers a specification by observing program execution and concisely summarizing the frequent interaction patterns as state machines that capture both temporal and data dependences. These state machines can be examined by a programmer, to refine the specification and identify errors, and can be utilized by automatic verification tools, to find bugs.Our preliminary experience with the mining tool has been promising. We were able to learn specifications that not only captured the correct protocol, but also discovered serious bugs."
236,237,distributed snapshots determining global states of distributed systems,3444.0,Distributed snapshots: determining global states of distributed systems,"This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are &ldquo;computation has terminated,&rdquo; &ldquo; the system is deadlocked&rdquo; and &ldquo;all tokens in a token ring have disappeared.&rdquo; The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing."
237,238,dimensions of relevance,3445.0,Dimensions of relevance,"Relevance has become a major area of research in the field of Information Retrieval, despite the fact that the concept relevance is not well understood. This paper models manifestations of relevance within a system of relevance attributes to show that the attributes function in different dimensions for the different manifestations of relevance. It is shown that motivational relevance, as a manifestation of relevance, should not be viewed as part of a linear, objective–subjective scale of relevances, but rather as an attribute of relevance. Similarly, that the manifestation of affective relevance should not be viewed as a discrete category of relevance manifestation, but rather as an influencing factor on the other subjective relevance types. The paper argues a consolidated model of relevance manifestations which includes the notion of socio-cognitive relevance."
239,240,propagation of trust and distrust,3450.0,Propagation of Trust and Distrust,"A (directed) network of people connected by ratings or trust scores, and a model for propagating those trust scores, is a fundamental building block in many of today's most successful e-commerce and recommendation systems. We develop a framework of trust propagation schemes, each of which may be appropriate in certain circumstances, and evaluate the schemes on a large trust network consisting of 800K trust scores expressed among 130K people. We show that a small number of expressed trusts/distrust per individual allows us to predict trust between any two people in the system with high accuracy. Our work appears to be the first to incorporate distrust in a computational trust propagation setting."
240,241,a conceptual framework and a toolkit for supporting the rapid prototyping of contextaware applications,3521.0,A conceptual framework and a toolkit for supporting the rapid prototyping of context-aware applications,"Computing devices and applications are now used beyond the desktop, in diverse environments, and this trend toward ubiquitous computing is accelerating. One challenge that remains in this emerging research field is the ability to enhance the behavior of any application by informing it of the context of its use. By context, we refer to any information that characterizes a situation related to the interaction between humans, applications, and the surrounding environment. Context-aware applications promise richer and easier interaction, but the current state of research in this field is still far removed from that vision. This is due to 3 main problems: (a) the notion of context is still ill defined, (b) there is a lack of conceptual models and methods to help drive the design of context-aware applications, and (c) no tools are available to jump-start the development of context-aware applications. In this anchor article, we address these 3 problems in turn. We first define context, identify categories of contextual information, and characterize context-aware application behavior. Though the full impact of context-aware computing requires understanding very subtle and high-level notions of context, we are focusing our efforts on the pieces of context that can be inferred automatically from sensors in a physical environment. We then present a conceptual framework that separates the acquisition and representation of context from the delivery and reaction to context by a context-aware application. We have built a toolkit, the Context Toolkit, that instantiates this conceptual framework and supports the rapid development of a rich space of context-aware applications. We illustrate the usefulness of the conceptual framework by describing a number of context-aware applications that have been prototyped using the Context Toolkit. We also demonstrate how such a framework can support the investigation of important research challenges in the area of context-aware computing."
247,248,data clustering a review,3561.0,Data clustering: a review,"Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval."
248,249,initialization of iterative refinement clustering algorithms,3562.0,Initialization of Iterative Refinement Clustering Algorithms,"Iterative refinement clustering algorithms (e.g. K-Means, EM) converge to one of numerous local minima. It is known that they are especially sensitive to initial conditions. We present a procedure for computing a refined starting condition from a given initial one that is based on an efficient technique for estimating the modes of a distribution. The refined initial starting condition leads to convergence to &amp;quot;better &amp;quot; local minima. The procedure is applicable to a wide class of clustering algorithms for both discrete and continuous data. We demonstrate the application of this method to the Expectation Maximization (EM) clustering algorithm and show that refined initial points indeed lead to improved solutions. Refinement run time is considerably lower than the time required to cluster the full database. The method is scalable and can be coupled with a scalable clustering algorithm to address the large-scale clustering in data mining."
250,251,the pfam protein families database,3576.0,The Pfam protein families database,"{Pfam is a large collection of protein multiple sequence alignments and profile hidden Markov models. Pfam is available on the World Wide Web in the UK at http://www.sanger.ac.uk/Software/Pfam/, in Sweden at http://www.cgb.ki.se/Pfam/, in France at http://pfam.jouy.inra.fr/ and in the US at http://pfam.wustl.edu/. The latest version (6.6) of Pfam contains 3071 families, which match 69\% of proteins in SWISS-PROT 39 and TrEMBL 14. Structural data, where available, have been utilised to ensure that Pfam families correspond with structural domains, and to improve domain-based annotation. Predictions of non-domain regions are now also included. In addition to secondary structure, Pfam multiple sequence alignments now contain active site residue mark-up. New search tools, including taxonomy search and domain query, greatly add to the functionality and usability of the Pfam resource.}"
253,254,denim finding a tighter fit between tools and practice for web site design,3587.0,DENIM: finding a tighter fit between tools and practice for Web site design,"Through a study of web site design practice, we observed that web site designers design sites at different levels of refinement&mdash;site map, storyboard, and individual page&mdash;and that designers sketch at all levels during the early stages of design. However, existing web design tools do not support these tasks very well. Informed by these observations, we created DENIM, a system that helps web site designers in the early stages of design. DENIM supports sketching input, allows design at different refinement levels, and unifies the levels through zooming. We performed an informal evaluation with seven professional designers and found that they reacted positively to the concept and were interested in using such a system in their work."
256,257,virtual screening of chemical libraries,3911.0,Virtual screening of chemical libraries,"Virtual screening uses computer-based methods to discover new ligands on the basis of biological structures. Although widely heralded in the 1970s and 1980s, the technique has since struggled to meet its initial promise, and drug discovery remains dominated by empirical screening. Recent successes in predicting new ligands and their receptor-bound structures, and better rates of ligand discovery compared to empirical screening, have re-ignited interest in virtual screening, which is now widely used in drug discovery, albeit on a more limited scale than empirical screening."
259,260,chemical space and biology,3916.0,Chemical space and biology,"Chemical space — which encompasses all possible small organic molecules, including those present in biological systems — is vast. So vast, in fact, that so far only a tiny fraction of it has been explored. Nevertheless, these explorations have greatly enhanced our understanding of biology, and have led to the development of many of today's drugs. The discovery of new bioactive molecules, facilitated by a deeper understanding of the nature of the regions of chemical space that are relevant to biology, will advance our knowledge of biological processes and lead to new strategies to treat disease."
268,269,the chatty web emergent semantics through gossiping,4138.0,The chatty web: emergent semantics through gossiping,"This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up, semi-automatic manner without relying on pre-existing, global semantic models. We assume that large amounts of data exist that have been organized and annotated according to local schemas. Seeing semantics as a form of agreement, our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise, local interactions: Participants provide translations between schemas they are interested in and can learn about other translations by routing queries (gossiping). To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria. The assessment process is incremental and the quality ratings are adjusted along with the operation of the system. Ultimately, this process results in global agreement, i.e., the semantics that all participants understand. We discuss strategies to efficiently find translations and provide results from a case study to justify our claims. Our approach applies to any system which provides a communication infrastructure (existing websites or databases, decentralized systems, P2P systems) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties."
269,270,evolution of transcription factors and the gene regulatory network in escherichia coli,4142.0,Evolution of transcription factors and the gene regulatory network in Escherichia coli.,"The most detailed information presently available for an organism's transcriptional regulation network is that for the prokaryote Escherichia coli. In order to gain insight into the evolution of the E.coli regulatory network, we analysed information obtainable for the domains and protein families of the transcription factors and regulated genes. About three-quarters of the 271 transcription factors we identified are two-domain proteins, consisting of a DNA-binding domain along with a regulatory domain. The regulatory domains mainly bind small molecules. Many groups of transcription factors have identical domain architectures, and this implies that roughly three-quarters of the transcription factors have arisen as a consequence of gene duplication. In contrast, there is little evidence of duplication of regulatory regions together with regulated genes or of transcription factors together with regulated genes. Thirty-eight, out of the 121 transcription factors for which one or more regulated genes are known, regulate other transcription factors. This amplification effect, as well as large differences between the numbers of genes directly regulated by transcription factors, means that there are about 10 global regulators which each control many more genes than the other transcription factors."
270,271,natural image statistics and efficient coding,4189.0,Natural image statistics and efficient coding,"doi: 10.1088/0954-898X_7_2_014 Natural images contain characteristic statistical regularities that set them apart from purely random images. Understanding what these regularities are can enable natural images to be coded more efficiently. In this paper, we describe some of the forms of structure that are contained in natural images, and we show how these are related to the response properties of neurons at early stages of the visual system. Many of the important forms of structure require higher-order (i.e. more than linear, pairwise) statistics to characterize, which makes models based on linear Hebbian learning, or principal components analysis, inappropriate for finding efficient codes for natural images. We suggest that a good objective for an efficient coding of natural scenes is to maximize the sparseness of the representation, and we show that a network that learns sparse codes of natural scenes succeeds in developing localized, oriented, bandpass receptive fields similar to those in the mammalian striate cortex.This paper was presented at the Workshop on Information Theory and the Brain, held at the University of Stirling, UK, on 4–5 September 1995."
271,272,the independent components of natural scenes are edge filters,4190.0,The `Independent Components' of Natural Scenes are Edge Filters,"It has previously been suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representation of natural scenes, and it has been reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that a new unsupervised learning algorithm based on information maximization, a nonlinear “infomax” network, when applied to an ensemble of natural scenes produces sets of visual filters that are localized and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximization network. In addition, the outputs of these filters are as independent as possible, since this infomax network performs Independent Components Analysis or ICA, for sparse (super-gaussian) component distributions. We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA). The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes. They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form a natural, information-theoretic coordinate system for natural images."
278,279,filtering algorithms and implementation for very fast publishsubscribe systems,4291.0,Filtering algorithms and implementation for very fast publish/subscribe systems,"Publish/Subscribe is the paradigm in which users express long-term interests (&ldquo;subscriptions&rdquo;) and some agent &ldquo;publishes&rdquo; events (e.g., offers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satisfied by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an offer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a long-lived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish/subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-specific caching policies, and application-specific query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions."
281,282,a scalable contentaddressable network,4300.0,A scalable content-addressable network,"Hash tables – which map “keys ” onto “values ”  – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internetlike scales. The CAN design is scalable, fault-tolerant and completely selforganizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 1"
288,289,statistical analysis of domains in interacting protein pairs,4376.0,Statistical analysis of domains in interacting protein pairs.,"ABSTRACT Motivation: Several methods have recently been developed to analyse large-scale sets of physical interactions between proteins in terms of physical contacts between the constituent domains, often with a view to predicting new pairwise interactions. Our aim is to combine genomic interaction data, in which domain--domain contacts are not explicitly reported, with the domain-level structure of individual proteins, in order to learn about the structure of interacting protein pairs. Our approach is driven by the need to assess the evidence for physical contacts between domains in a statistically rigorous way. Results: We develop a statistical approach that assigns p-values to pairs of domain superfamilies, measuring the strength of evidence within a set of protein interactions that domains from these superfamilies formcontacts. A set of p-values is calculated for SCOP superfamily pairs, based on a pooled data set of interactions from yeast. These p-values can be used to predict which domains come into contact in an interacting protein pair. This predictive scheme is tested against protein complexes in the Protein Quaternary Structure (PQS) database, and is used to predict domain--domain contacts within 705 interacting protein pairs taken from our pooled data set."
292,293,how did alternative splicing evolve,4483.0,How did alternative splicing evolve?,"Alternative splicing creates transcriptome diversification, possibly leading to speciation. A large fraction of the protein-coding genes of multicellular organisms are alternatively spliced, although no regulated splicing has been detected in unicellular eukaryotes such as yeasts. A comparative analysis of unicellular and multicellular eukaryotic 5' splice sites has revealed important differences - the plasticity of the 5' splice sites of multicellular eukaryotes means that these sites can be used in both constitutive and alternative splicing, and for the regulation of the inclusion/skipping ratio in alternative splicing. So, alternative splicing might have originated as a result of relaxation of the 5' splice site recognition in organisms that originally could support only constitutive splicing."
293,294,the google similarity distance,4487.0,The Google Similarity Distance,"Words and phrases acquire meaning from the way they are used in society, from their relative semantics to other words and phrases. For computers, the equivalent of ""society&#8221; is ""database,&#8221; and the equivalent of ""use&#8221; is ""a way to search the database.&#8221; We present a new theory of similarity between words and phrases based on information distance and Kolmogorov complexity. To fix thoughts, we use the World Wide Web (WWW) as the database, and Google as the search engine. The method is also applicable to other search engines and databases. This theory is then applied to construct a method to automatically extract similarity, the Google similarity distance, of words and phrases from the WWW using Google page counts. The WWW is the largest database on earth, and the context information entered by millions of independent users averages out to provide automatic semantics of useful quality. We give applications in hierarchical clustering, classification, and language translation. We give examples to distinguish between colors and numbers, cluster names of paintings by 17th century Dutch masters and names of books by English novelists, the ability to understand emergencies and primes, and we demonstrate the ability to do a simple automatic English-Spanish translation. Finally, we use the WordNet database as an objective baseline against which to judge the performance of our method. We conduct a massive randomized trial in binary classification using support vector machines to learn categories based on our Google distance, resulting in an a mean agreement of 87 percent with the expert crafted WordNet categories."
294,295,reinforcement learning a survey,4578.0,Reinforcement Learning: A Survey,"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."
299,300,accurate multiplex gene synthesis from programmable dna microchips,4659.0,Accurate multiplex gene synthesis from programmable DNA microchips,"Testing the many hypotheses from genomics and systems biology experiments demands accurate and cost-effective gene and genome synthesis. Here we describe a microchip-based technology for multiplex gene synthesis. Pools of thousands of 'construction' oligonucleotides and tagged complementary 'selection' oligonucleotides are synthesized on photo-programmable microfluidic chips1, released, amplified and selected by hybridization to reduce synthesis errors ninefold. A one-step polymerase assembly multiplexing reaction assembles these into multiple genes. This technology enabled us to synthesize all 21 genes that encode the proteins of the Escherichia coli 30S ribosomal subunit, and to optimize their translation efficiency in vitro through alteration of codon bias. This is a significant step towards the synthesis of ribosomes in vitro and should have utility for synthetic biology in general."
312,313,the price dynamics of common trading strategies,4926.0,The price dynamics of common trading strategies,"A deterministic trading strategy can be regarded as as signal processing element that uses external information and past prices as inputs and incorporates them into future prices. This paper uses a market maker based method of price formation to study the price dynamics induced by several commonly used financial trading strategies, showing how they amplify noise, induce structure in prices, and cause phenomena such as excess and clustered volatility."
313,314,wormbase a comprehensive data resource for caenorhabditis biology and genomics,4931.0,WormBase: a comprehensive data resource for Caenorhabditis biology and genomics.,"WormBase (http://www.wormbase.org), the model organism database for information about Caenorhabditis elegans and related nematodes, continues to expand in breadth and depth. Over the past year, WormBase has added multiple large-scale datasets including SAGE, interactome, 3D protein structure datasets and NCBI KOGs. To accommodate this growth, the International WormBase Consortium has improved the user interface by adding new features to aid in navigation, visualization of large-scale datasets, advanced searching and data mining. Internally, we have restructured the database models to rationalize the representation of genes and to prepare the system to accept the genome sequences of three additional Caenorhabditis species over the coming year. 10.1093/nar/gki066"
314,315,a repository of our own the elis eprints archive,4932.0,A repository of our own: the E-LIS e-prints archive,"			This article reviews the E-Prints in Library and Information Science (E-LIS) open access archive. E-LIS is part of the Research in Computing, Library and Information Science (RCLIS) project, an international effort to organize and disseminate scholarly papers in librarianship and related fields. E-LIS uses open source applications, and joins a growing number of OAI-compliant services dedicated to providing free access to scholarly information."
315,316,using terminological feedback for web search refinement a logbased study,4936.0,Using terminological feedback for web search refinement: a log-based study,"Although interactive query reformulation has been actively studied in the laboratory, little is known about the actual behavior of web searchers who are offered terminological feedback along with their search results. We analyze log sessions for two groups of users interacting with variants of the AltaVista search engine - a baseline group given no terminological feedback and a feedback group to whom twelve refinement terms are offered along with the search results. We examine uptake, refinement effectiveness, conditions of use, and refinement type preferences. Although our measure of overall session ""success"" shows no difference between outcomes for the two groups, we find evidence that a subset of those users presented with terminological feedback do make effective use of it on a continuing basis."
316,317,a usercentred approach to functions in excel,4952.0,A User-Centred Approach to Functions in Excel,"We describe extensions to the Excel spreadsheet that integrate userdefined functions into the spreadsheet grid, rather than treating them as a “bolt-on”. Our first objective was to bring the benefits of additional programming language features to a system that is often not recognised as a programming language. Second, in a project involving the evolution of a well-established language, compatibility with previous versions is a major issue, and maintaining this compatibility was our second objective. Third and most important, the commercial success of spreadsheets is largely due to the fact that many people find them more usable than programming languages for programming-like tasks. Thus, our third objective (with resulting constraints) was to maintain this usability advantage. Simply making Excel more like a conventional programming language would not meet these objectives and constraints. We have therefore taken an approach to our design work that emphasises the cognitive requirements of the user as a primary design criterion. The analytic approach that we demonstrate in this project is based on recent developments in the study of programming usability, including the Cognitive Dimensions of Notations and the Attention Investment model of abstraction use. We believe that this approach is also applicable to the design and extension of other programming languages and environments."
317,318,type classes in haskell,4956.0,Type Classes in Haskell,"This article defines a set of type inference rules for resolving overloading introduced by type classes, as used in the functional programming language Haskell. Programs including type classes are transformed into ones which may be typed by standard Hindley-Milner inference rules. In contrast to other work on type classes, the rules presented here relate directly to Haskell programs. An innovative aspect of this work is the use of second-order lambda calculus to record type information in the transformed program."
318,319,how to make adhoc polymorphism less ad hoc,4958.0,How to make ad-hoc polymorphism less ad hoc,"This paper presents  type classes , a new approach to  ad-hoc  polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the &ldquo;eqtype variables&rdquo; of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules."
319,320,representing control a study of the cps transformation,4962.0,Representing Control: A study of the CPS transformation,"This paper investigates the transformation of v-terms into continuation-passing style (CPS). We show that by appropriate j-expansion of Fischer and Plotkin&#039;s two-pass equational specification of the CPS transform, we can obtain a static and context-free separation of the result terms into &amp;quot;essential &amp;quot; and &amp;quot;administrative &amp;quot; constructs. Interpreting the former as syntax builders and the latter as directly executable code, we obtain a simple and efficient one-pass transformation algorithm, easily extended to conditional expressions, recursive definitions, and similar constructs. This new transformation algorithm leads to a simpler proof of Plotkin&#039;s simulation and indifference results. We go on to show how CPS-based control operators similar to, but more general than, Scheme&#039;s call/cc can be naturally accommodated by the new transformation algorithm. To demonstrate the expressive power of these operators, we use them to present an equivalent but even more concise formulation of the efficient CPS transformation algorithm. Finally, we relate the fundamental ideas underlying this derivation to similar concepts from other works on program manipulation; we derive a one-pass CPS transformation of n-terms; and we outline some promising areas for future research."
321,322,stochastic lambda calculus and monads of probability distributions,4973.0,Stochastic lambda calculus and monads of probability distributions,"Probability distributions are useful for expressing the meanings of probabilistic languages, which support formal modeling of and reasoning about uncertainty. Probability distributions form a monad, and the monadic definition leads to a simple, natural semantics for a stochastic lambda calculus, as well as simple, clean implementations of common queries. But the monadic implementation of the expectation query can be much less efficient than current best practices in probabilistic modeling. We therefore present a language of measure terms, which can not only denote discrete probability distributions but can also support the best known modeling techniques. We give a translation of stochastic lambda calculus into measure terms. Whether one translates into the probability monad or into measure terms, the results of the translations denote the same probability distribution."
322,323,the alternative splicing gallery asg bridging the gap between genome and transcriptome,4990.0,The Alternative Splicing Gallery (ASG): bridging the gap between genome and transcriptome,"Alternative splicing essentially increases the diversity of the transcriptome and has important implications for physiology, development and the genesis of diseases. Conventionally, alternative splicing is investigated in a case-by-case fashion, but this becomes cumbersome and error prone if genes show a huge abundance of different splice variants. We use a different approach and integrate all transcripts derived from a gene into a single splicing graph. Each transcript corresponds to a path in the graph, and alternative splicing is displayed by bifurcations. This representation preserves the relationships between different splicing variants and allows us to investigate systematically all possible putative transcripts. We built a database of splicing graphs for human genes, using transcript information from various major sources (Ensembl, RefSeq, STACK, TIGR and UniGene). A Web interface allows users to display the splicing graphs, to interactively assemble transcripts and to access their sequences as well as neighboring genomic regions. We also provide for each gene an exhaustive pre-computed catalog of putative transcripts--in total more than 1.2 million sequences. We found that [~]65% of the investigated genes show evidence for alternative splicing, and in 5% of the cases, a single gene might produce over 100 transcripts. 10.1093/nar/gkh731"
323,324,splicing graphs and est assembly problem,4991.0,Splicing graphs and EST assembly problem,"10.1093/bioinformatics/18.suppl_1.S181 Motivation: The traditional approach to annotate alternative splicing is to investigate every splicing variant of the gene in a case-by-case fashion. This approach, while useful, has some serious shortcomings. Recent studies indicate that alternative splicing is more frequent than previously thought and some genes may produce tens of thousands of different transcripts. A list of alternatively spliced variants for such genes would be difficult to build and hard to analyse. Moreover, such a list does not show the relationships between different transcripts and does not show the overall structure of all transcripts. A better approach would be to represent all splicing variants for a given gene in a way that captures the relationships between different splicing variants.Results: We introduce the notion of the splicing graph that is a natural and convenient representation of all splicing variants. The key difference with the existing approaches is that we abandon the linear (sequence) representation of each transcript and replace it with a graph representation where each transcript corresponds to a path in the graph. We further design an algorithm to assemble EST reads into the splicing graph rather than assembling them into each splicing variant in a case-by-case fashion.Availability: http://www-cse.ucsd.edu/groups/bioinformatics/software.htmlContact: sheber@ucsd.eduKeywords: EST assembly; splicing graph; alternative splicing"
324,325,oaipp a peertopeer network for open archives,4996.0,OAI-P2P: A Peer-to-Peer Network for Open Archives,"OAI is designed with a low-barrier technology approach, thus allowing institutions to provide content metadata with little effort. On the other hand, search capabilities are very limited on OAI data providers, and have to be provided by separate service providers. We propose that data providers form a peer-to-peer network which supports distributed search over all connected metadata repositories. Such an approach is already implemented for learning content metadata (project 'Edutella'). We..."
326,327,landmarc indoor location sensing using active rfid,5787.0,LANDMARC: Indoor Location Sensing Using Active RFID,"Growing convergence among mobile computing devices and embedded technology sparks the development and deployment of “context-aware” applications, where location is the most essential context. In this paper we present LANDMARC, a location sensing prototype system that uses Radio Frequency Identification (RFID) technology for locating objects inside buildings. The major advantage of LANDMARC is that it improves the overall accuracy of locating objects by utilizing the concept of reference tags. Based on experimental analysis, we demonstrate that active RFID is a viable and cost-effective candidate for indoor location sensing. Although RFID is not designed for indoor location sensing, we point out three major features that should be added to make RFID technologies competitive in this new and growing market."
327,328,wireless broadband drivers and their social implications,8851.0,Wireless broadband drivers and their social implications,"Wireless local area networks now offer high-speed Internet access at numerous locations in both public and private environments. Associated with this rapid growth, numerous social implications come to the fore, especially relating to practices, such as the free use and shar- ing of bandwidth. Using case-based comparative analysis, we examine three primary strate- gies involved in providing wireless broadband access. Based on this research, we discuss the future of Wi-Fi growth, emergent competing technologies, and the broad social implications of this phenomenon."
328,329,a tutorial on support vector regression,9972.0,A tutorial on support vector regression,"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for regression and function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization and capacity control from a SV point of view."
329,330,multimodel inference,11258.0,Multimodel Inference,"The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a âsavvyâ prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging."
330,331,greed and grievance in civil war,19773.0,Greed and grievance in civil war,"We investigate the causes of civil war, using a new data set of wars during 1960-99. Rebellion may be explained by atypically severe grievances, such as high inequality, a lack of political rights, or ethnic and religious divisions in society. Alternatively, it might be explained by atypical opportunities for building a rebel organization. While it is difficult to find proxies for grievances and opportunities, we find that political and social variables that are most obviously related to grievances have little explanatory power. By contrast, economic variables, which could proxy some grievances but are perhaps more obviously related to the viability of rebellion, provide considerably more explanatory power. 10.1093/oep/gpf064"
332,333,multimodal video indexing a review of the stateoftheart,23055.0,Multimodal Video Indexing: A Review of the State-of-the-art,"Efficient and effective handling of video documents depends on the availability of indexes. Manual indexing is unfeasible for large video collections. In this paper we survey several methods aiming at automating this time and resource consuming process. Good reviews on single modality based video indexing have appeared in literature. Effective indexing, however, requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion. Therefore, instead of separately treating the different information sources involved, and their specific algorithms, we focus on the similarities and differences between the modalities. To that end we put forward a unifying and multimodal framework, which views a video document from the perspective of its author. This framework forms the guiding principle for identifying index types, for which automatic methods are found in literature. It furthermore forms the basis for categorizing these different methods."
333,334,active sampling for class probability estimation and ranking,25738.0,Active Sampling for Class Probability Estimation and Ranking,"In many cost-sensitive environments class probability estimates are used by decision makers to evaluate the expected utility from a set of alternatives. Supervised learning can be used to build class probability estimates; however, it often is very costly to obtain training data with class labels. Active learning acquires data incrementally, at each phase identifying especially useful additional data for labeling, and can be used to economize on examples needed for learning. We outline the critical features of an active learner and present a sampling-based active learning method for estimating class probabilities and class-based rankings. BOOTSTRAP-LV identifies particularly informative new data for learning based on the variance in probability estimates, and uses weighted sampling to account for a potential example's informative value for the rest of the input space. We show empirically that the method reduces the number of data items that must be obtained and labeled, across a wide variety of domains. We investigate the contribution of the components of the algorithm and show that each provides valuable information to help identify informative examples. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV uses fewer examples to exhibit a certain estimation accuracy and provide insights to the behavior of the algorithms. Finally, we experiment with another new active sampling algorithm drawing from both UNCERTAINTY SAMPLING and BOOTSTRAP-LV and show that it is significantly more competitive with BOOTSTRAP-LV compared to UNCERTAINTY SAMPLING. The analysis suggests more general implications for improving existing active sampling algorithms for classification."
334,335,learning to decode cognitive states from brain images,25742.0,Learning to Decode Cognitive States from Brain Images,"Over the past decade, functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful new instrument to collect vast quantities of data about activity in the human brain. A typical fMRI experiment can produce a three-dimensional image related to the human subject's brain activity every half second, at a spatial resolution of a few millimeters. As in other modern empirical sciences, this new instrumentation has led to a flood of new data, and a corresponding need for new data analysis methods. We describe recent research applying machine learning methods to the problem of classifying the cognitive state of a human subject based on fRMI data observed over a single time interval. In particular, we present case studies in which we have successfully trained classifiers to distinguish cognitive states such as (1) whether the human subject is looking at a picture or a sentence, (2) whether the subject is reading an ambiguous or non-ambiguous sentence, (3) whether the word the subject is viewing is a noun or a verb, and (4) whether the noun the subject is viewing is a word describing food, people, buildings, etc. This learning problem provides an interesting case study of classifier learning from extremely high dimensional ($10^5$ features), extremely sparse (tens of training examples), noisy data. This paper summarizes the results obtained in these four case studies, as well as lessons learned about how to successfully apply machine learning methods to train classifiers in such settings."
336,337,not just piaget not just vygotsky and certainly not vygotsky as alternative to piaget,26513.0,"Not just Piaget; not just Vygotsky, and certainly not Vygotsky as alternative to Piaget","There have been many interpretations published on the relative importance of the work of both Vygotsky and Piaget: often to the detriment of the latter. This article represents an attempt to discover the meaning and intention of the former by going back to the specifics of what he said and wrote. By reference to what they said of each other it is argued that by the early 30s they had reached almost identical positions regarding child development, and that the work of each is complementary to that of the other. The implications of this position for a theory of intervention for cognitive acceleration are then discussed."
337,338,scholarly communication in the digital environment what do authors want,26527.0,Scholarly communication in the digital environment: what do authors want?,"This article reports on a large-scale international survey of authors' perception and experience of the journals system conducted by ciber in association with National Opinion Polls {(NOP).} It explores the factors that inform authors' decisions where to publish and, in particular, which groups of readers they perceive to be most important. It probes readership behaviour and the values that underlie authors' attitudes towards copyright and emerging business models, notably open access. It is concluded that many aspects of author behaviour are highly conservative and that a significant shift towards open access is, in the short to medium term, highly unlikely."
338,339,a modelbased background adjustment for oligonucleotide expression arrays,28493.0,A Model-Based Background Adjustment for Oligonucleotide Expression Arrays,"High-density oligonucleotide expression arrays are widely used in many areas of biomedical research. Affymetrix GeneChip arrays are the most popular. In the Affymetrix system, a fair amount of further preprocessing and data reduction occurs after the image-processing step. Statistical procedures developed by academic groups have been successful in improving the default algorithms provided by the Affymetrix system. In this article we present a solution to one of the preprocessing steps—background adjustment—based on a formal statistical framework. Our solution greatly improves the performance of the technology in various practical applications. These arrays use short oligonucleotides to probe for genes in an RNA sample. Typically, each gene is represented by 11—20 pairs of oligonucleotide probes. The first component of these pairs is referred to as a perfect match probe and is designed to hybridize only with transcripts from the intended gene (i.e., specific hybridization). However, hybridization by other sequences (i.e., nonspecific hybridization) is unavoidable. Furthermore, hybridization strengths are measured by a scanner that introduces optical noise. Therefore, the observed intensities need to be adjusted to give accurate measurements of specific hybridization. We have found that the default ad hoc adjustment, provided as part of the Affymetrix system, can be improved through the use of estimators derived from a statistical model that uses probe sequence information. A final step in preprocessing is to summarize the probe-level data for each gene to define a measure of expression that represents the amount of the corresponding mRNA species. In this article we illustrate the practical consequences of not adjusting appropriately for the presence of nonspecific hybridization and provide a solution based on our background adjustment procedure. Software that computes our adjustment is available as part of the Bioconductor Project (http://www.bioconductor.org)."
340,341,speaking while monitoring addressees for understanding,32136.0,Speaking while monitoring addressees for understanding,"(from the journal abstract) Speakers monitor their own speech and, when they discover problems, make repairs. In the proposal examined here, speakers also monitor addressees for understanding and, when necessary, alter their utterances in progress. Addressees cooperate by displaying and signaling their understanding in progress. Pairs of participants were videotaped as a director instructed a builder in assembling 10 Lego models. In one group, directors could see the builders' workspace; in a second, they could not; in a third, they gave instructions by audiotape. Two partners were much slower when directors could not see the builders' workspace, and they made many more errors when the instructions were audiotaped. When their workspace was visible, builders communicated with directors by exhibiting, poising, pointing at, placing, and orienting blocks, and by eye gaze, head nods, and head shakes, all timed with precision. Directors often responded by altering their utterances midcourse, also timed with precision. (PsycINFO Database Record (c) 2004 APA, all rights reserved) speaking; monitoring addressees; understanding"
341,342,the network paradigm in organizational research a review and typology,32678.0,The Network Paradigm in Organizational Research: A Review and Typology,"In this paper, we review and analyze the emerging network paradigm in organizational research. We begin with a conventional review of recent research organized around recognized research streams. Next, we analyze this research, developing a set of dimensions along which network studies vary, including direction of causality, levels of analysis, explanatory goals, and explanatory mechanisms. We use the latter two dimensions to construct a 2-by-2 table cross-classifying studies of network consequences into four canonical types: structural social capital, social access to resources, contagion, and environmental shaping. We note the rise in popularity of studies with a greater sense of agency than was traditional in network research. 10.1016/S0149-2063_03_00087-4"
342,343,beyond independent components trees and clusters,32849.0,Beyond Independent Components: Trees and Clusters,"We present a generalization of independent component analysis (ICA), where instead of looking  for a linear transform that makes the data components independent, we look for a transform that  makes the data components well fit by a tree-structured graphical model. This tree-dependent  component analysis (TCA) provides a tractable and flexible approach to weakening the assumption  of independence in ICA. In particular, TCA allows the underlying graph to have multiple connected  components, and thus the method is able to find &#034;clusters&#034; of components such that components  are dependent within a cluster and independent between clusters. Finally, we make use of a notion  of graphical models for time series due to Brillinger (1996) to extend these ideas to the temporal  setting. In particular, we are able to fit models that incorporate tree-structured dependencies among  multiple time series."
343,344,transaction cost determinants and ownershipbased entry mode choice a metaanalytical review,33371.0,Transaction cost determinants and ownership-based entry mode choice: a meta-analytical review,"Entry mode choice is a critical ingredient of international entry strategies, and has been voluminously examined in the field. The findings, however, are very mixed, especially with respect to transaction-cost-related factors in determining the ownership-based entry mode choice. This study conducted a meta-analysis to quantitatively summarize the literature and empirically generalize more conclusive findings. Based on the 106 effect sizes of 38 empirical studies, the meta-analysis shows that the findings of the existing studies are moderated to varying degrees by both study-setting factors and statistical artifacts, although the combined overall effects of transaction cost-based determinants are consistent with the predictions of transaction cost economics. We extensively discuss the implications of meta-analytical results, especially moderating effects of location, country of origin, industry type, and statistical artifacts, highlight the measurement adequacy, equivalence, and multidimensionality of transaction cost determinants, and present our suggestions to improve theoretical inquiries and empirical verifications on entry mode choice."
344,345,exploratory data analysis for complex models,36623.0,Exploratory Data Analysis for Complex Models,"''Exploratory'' and ''confirmatory'' data analysis can both be viewed as methods for comparing observed data to what would be obtained under an implicit or explicit statistical model. For example, many of Tukey’s methods can be interpreted as checks against hypothetical linear models and Poisson distributions. In more complex situations, Bayesian methods can be useful for constructing reference distributions for various plots that are useful in exploratory data analysis. This article proposes an approach to unify exploratory data analysis with more formal statistical methods based on probability models. These ideas are developed in the context of examples from fields including psychology, medicine, and social science."
345,346,defaultmode activity during a passive sensory task uncoupled from deactivation but impacting activation,36912.0,Default-Mode Activity during a Passive Sensory Task: Uncoupled from Deactivation but Impacting Activation,"Deactivation refers to increased neural activity during low-demand tasks or rest compared with high-demand tasks. Several groups have reported that a particular set of brain regions, including the posterior cingulate cortex and the medial prefrontal cortex, among others, is consistently deactivated. Taken together, these typically deactivated brain regions appear to constitute a default-mode network of brain activity that predominates in the absence of a demanding external task. Examining a passive, block-design sensory task with a standard deactivation analysis (rest epochs vs. stimulus epochs), we demonstrate that the default-mode network is undetectable in one run and only partially detectable in a second run. Using independent component analysis, however, we were able to detect the full default-mode network in both runs and to demonstrate that, in the majority of subjects, it persisted across both rest and stimulus epochs, uncoupled from the task waveform, and so mostly undetectable as deactivation. We also replicate an earlier finding that the default-mode network includes the hippocampus suggesting that episodic memory is incorporated in default-mode cognitive processing. Furthermore, we show that the more a subject's default-mode activity was correlated with the rest epochs (and ""deactivated"" during stimulus epochs), the greater that subject's activation to the visual and auditory stimuli. We conclude that activity in the default-mode network may persist through both experimental and rest epochs if the experiment is not sufficiently challenging. Time-series analysis of default-mode activity provides a measure of the degree to which a task engages a subject and whether it is sufficient to interrupt the processes--presumably cognitive, internally generated, and involving episodic memory--mediated by the default-mode network."
346,347,mathematical modal logic a view of its evolution,38466.0,Mathematical modal logic: A view of its evolution,"This is a survey of the origins of mathematical interpretations of modal logics, and their development over the last century or so. It focuses on the interconnections between  algebraic  semantics using Boolean algebras with operators and  relational  semantics using structures often called  Kripke models . It reviews the ideas of a number of people who independently contributed to the emergence of relational semantics, and compares them with the work of Kripke. It concludes with an account of several applications of modal model theory to mathematics and theoretical computer science."
347,348,the relationships of communicator style personalitybased learning style and classroom community among online graduate students,39794.0,"The relationships of communicator style, personality-based learning style, and classroom community among online graduate students","This study examined the relationships among communicator style, personality-based learning style, and sense of classroom community among 72 graduate students enrolled in online doctoral coursework. Findings suggested that communicator style patterns were related to learning styles and to classroom community. Moreover, the results of a canonical correlation suggested that friendly and open communicator styles were significantly related to feelings of being connected and the precise communicator style was related to both feelings of connectedness and to feelings that membership in the online learning community fostered educational goal attainment. No significant relationships were found between learning styles and classroom community."
349,350,collaborating with writing tools an instrumental perspective on the problem of computersupported collaborative activities,43375.0,Collaborating with writing tools - An instrumental perspective on the problem of computer-supported collaborative activities,This paper presents an analysis of the modifications that a synchronous computer support for collaborative writing introduces into the organization of co-authors' writing. The analysis is grounded in case studies of different groups of co-authors writing a report together face to face and at a distance through a collaborative writing computer system. Drawing from these studies I suggest that the problems with using a collaborative writing computer system to provide a fully collaborative writing environment derive from underlying assumptions concerning collaboration within the co-authoring activity. I point out that a more thorough understanding of how co-authors organize their writing can provide resources to envisage more radical solutions to the problem of computer support for collaboration. I conclude by considering ways that might be adequate to reconfigure collaborative writing systems in order to provide more satisfactory support for collaboration in writing environments.
350,351,the concept of information overload a review of literature from organization science accounting marketing mis and related disciplines,43867.0,"The Concept of Information Overload: A Review of Literature from Organization Science, Accounting, Marketing, MIS, and Related Disciplines","Based on literature from the domains of organization science, marketing, accounting, and management information systems, this review article examines the theoretical basis of the information overload discourse and presents an overview of the main definitions, situations, causes, effects, and countermeasures. It analyzes the contributions from the last 30 years to consolidate the existing research in a conceptual framework and to identify future research directions."
351,352,higher education management and policy volume issue complete edition,45311.0,Higher Education Management and Policy: Volume 16 Issue 2 Complete Edition,"The Journal of OECD&#39;s Programme on Institutional Management in Higher Education. This issue features article on fair access, assessment of personnel, a merger, private university policy initiatives, accessibility and equity, internationalisation, and enrollment management.  - A Faustian Bargain&#63; Institutional Responses to National and International Rankings -  - &#34;Standards Will Drop&#34; - and Other Fears about the Equality Agenda in Higher Education -"
354,355,becoming an online teacher adapting to a changed environment for teaching and learning in higher education,52185.0,Becoming an Online Teacher: Adapting to a Changed Environment for Teaching and Learning in Higher Education,"Advancements in online technologies have facilitated a convergence of distance and campus-based learning and, thus, offer new opportunities for all students through better access to resources, increased interaction between staff and students and greater flexibility in place and time. However, the transition to online teaching and learning presents new challenges as the roles and expectations of both staff and students evolve. An online teacher must create a coherent learning experience for students with whom they may not meet face-to-face and, therefore, must develop new support strategies that maintain motivation and encourage interaction. Adapting student-centred approaches to the online environment has required the development of new skills and changes to teaching practices. This paper presents an analysis of the changed environment for teachers and learners in a post-graduate coursework programme based on constructivist principles that has moved from predominately on-campus delivery to online mode. The authors examine the impact of changes to teaching and learning over the past 5 years of the programme's development and reflect on the implications of these for becoming an online teacher. <b>Devenir un professeur en ligne : Adaptation à un environnement varié d'enseignement et de formation supérieure.</b> Les progrès de technologie en ligne ont facilité une convergence de formation par correspondance et de formation au campus et ceci offre de nouvelles possibilités pour tous les étudiants par un meilleur accès aux ressources, une interaction augmentée entre le corps enseignant et les étudiants et une plus grande flexibilité temporelle et cohérente. Cependant, la transition vers l'enseignement et la formation en ligne représente de nouveaux défis étant donné que les rôles et les expectatives du corps enseignant ainsi que des étudiants altèrent. Un professeur en ligne doit créer une expérience de formation cohérente pour des étudiants qu'il ne rencontre pas personnellement et par conséquent il doit développer une nouvelle stratégie de soutien qui maintient la motivation et encourage l'interaction. L'adaption des approches concentrées sur les étudiants à un environnement en ligne exige un développement de nouveaux talents et des changements de pratiques d'enseignement. Cet exposé présente une analyse de la convergence d'environnements de formation qui a tourné d'une assistance prédominante du côté du campus à un mode en ligne et de l'effet de cette convergence sur les professeurs et les étudiants d'un programme d'études basé sur des principes constructifs. Les auteurs examinent l'impact des changements d'enseignement et de formation pendant les cinq dernières années du développement de ce programme et considèrent ses répercussions sur l'éducation d'un professeur en ligne. <b>Ausbildung zur Online-Lehrkraft: Anpassung an ein verändertes Lern- und Lehrumfeld im Bereich der Hochschulausbildung.</b> Die Fortschritte in der Online-Technologie haben eine Annäherung zwischen Fernstudium und hochschulbasiertem Lernen ermöglicht, und dies eröffnet neue Möglichkeiten für alle Studenten durch einen besseren Zugang zu Ressourcen, einer verbesserten Interaktion zwischen Lehrerkollegium und Studenten und einer größeren räumlichen und zeitlichen Flexibilität. Der Übergang zum Online-Unterricht und Lernen stellt außerdem neue Herausforderungen dar, da sich die Rolle und die Erwartungen sowohl des Lehrerkollegiums als auch der Studenten verändern. Eine Online-Lehrkraft muss ein schlüssiges Lernerlebnis für Studenten schaffen, die sie gegebenenfalls nie persönlich kennen lernt. Aus diesem Grunde muss sie neue Betreuungsstrategien entwickeln, die die Motivation aufrechterhalten und die Interaktion fördern. Durch die Anpassung der studentischen Lernansätze an ein Online-Umfeld ist die Entwicklung neuer Fähigkeiten und eine Änderung der Lehrpraktiken erforderlich geworden. Dieser Bericht stellt eine Analyse des veränderten Lernumfelds für Lehrer und Studenten in einem auf konstruktivistischen Prinzipien basierten weiterführenden Studienprogramm vor, das von einer überwiegend hochschulbasierten Wissensvermittlung zu Online-Methoden übergeht. Die Autoren untersuchen den Einfluss der Lehr- und Lernveränderungen in den vergangenen fünf Jahren der Programmentwicklung und betrachten deren Auswirkungen auf die Ausbildung einer Online-Lehrkraft."
355,356,why environmental scientists are becoming bayesians,52658.0,Why environmental scientists are becoming Bayesians,"Advances in computational statistics provide a general framework for the high-dimensional models typically needed for ecological inference and prediction. Hierarchical Bayes (HB) represents a modelling structure with capacity to exploit diverse sources of information, to accommodate influences that are unknown (or unknowable), and to draw inference on large numbers of latent variables and parameters that describe complex relationships. Here I summarize the structure of HB and provide examples for common spatiotemporal problems. The flexible framework means that parameters, variables and latent variables can represent broader classes of model elements than are treated in traditional models. Inference and prediction depend on two types of stochasticity, including (1) uncertainty, which describes our knowledge of fixed quantities, it applies to all 'unobservables' (latent variables and parameters), and it declines asymptotically with sample size, and (2) variability, which applies to fluctuations that are not explained by deterministic processes and does not decline asymptotically with sample size. Examples demonstrate how different sources of stochasticity impact inference and prediction and how allowance for stochastic influences can guide research."
356,357,scale and translation invariant collaborative filtering systems,53021.0,Scale and Translation Invariant Collaborative Filtering Systems,"Collaborative filtering systems are prediction algorithms over sparse data sets of user preferences. We modify a wide range of state-of-the-art collaborative filtering systems to make them scale and translation invariant and generally improve their accuracy without increasing their computational cost. Using the EachMovie and the Jester data sets, we show that learning-free constant time scale and translation invariant schemes outperforms other learning-free constant time schemes by at least 3% and perform as well as expensive memory-based schemes (within 4%). Over the Jester data set, we show that a scale and translation invariant Eigentaste algorithm outperforms Eigentaste 2.0 by 20%. These results suggest that scale and translation invariance is a desirable property."
357,358,tropical forest fragments enhance pollinator activity in nearby coffee crops,56279.0,Tropical Forest Fragments Enhance Pollinator Activity in Nearby Coffee Crops,":&#8194; Crop pollination by wild bees is an ecosystem service of enormous value, but it is under increasing threat from agricultural intensification. As with many ecosystem services, the mechanisms, scales, and species through which crop pollination is provided are too poorly understood to inform land-use decisions. I investigated the role of tropical forest remnants as sources of pollinators to surrounding coffee crops in Costa Rica. In 2001 and 2002 I observed bee activity and pollen deposition rates at coffee flowers along distance gradients from two fragments and one narrow riparian strip of forest. Eleven eusocial species were the most common visitors: 10 species of native meliponines and the introduced honeybee, Apis mellifera (hereafter Apis). Bee richness, overall visitation rate, and pollen deposition rate were all significantly higher in sites within approximately 100 m of forest fragments than in sites farther away (maximum distance of 1.6 km). Apis visitation rates were constant across the distance gradient, however, and Apis accounted for &#62;90% of all floral visits in distant sites. The gradient from the riparian strip showed a similar drop in bee species richness with distance, but visitation rates were uniformly low along the gradient. Throughout the study area, Apis abundances declined sharply from 2001 to 2002, reducing visitation rates by over 50% in distant sites (where Apis was almost the only pollinator). In near sites, however, overall visitation rates dropped only 9% because native species almost entirely compensated for the Apis decline. Forest fragments (more so than the riparian strip) thus provided nearby coffee with a diversity of bees that increased both the amount and stability of pollination services by reducing dependence on a single introduced pollinator. Exploring the economic links between forest preservation and coffee cultivation may help align the goals of conservation and agriculture within many regions of global conservation priority."
358,359,analyzing student interactions and meaning construction in computer bulletin board discussions,56532.0,Analyzing student interactions and meaning construction in computer bulletin board discussions,"This case study, based on social constructivist learning theory, investigated the communication patterns and the knowledge construction process of students who used a computer bulletin board system (BBS) to discuss course-related content. Collected data included the outline of BBS postings and transcripts of the BBS messages from three selected weeks during the semester in an advanced communications class. Quantitative analysis was used to examine participation and interaction rates, and qualitative procedures were used to analyze knowledge construction processes and to refine a category system of indicators and descriptors. Results showed that students engaged in a knowledge construction process that was characterized chiefly by clarification, elaboration, and interpretation, and that produced more reflective monologues than dialogical interactions. Findings were related to constructivist theories and to previous analyses of computer conferencing systems, and were used to develop a list of recommendations for practitioners interested in incorporating such systems in their courses."
360,361,regulation of rna polymerase ii transcription by sequencespecific dna binding factors,60145.0,Regulation of RNA Polymerase II Transcription by Sequence-Specific DNA Binding Factors,"In eukaryotes, transcription of the diverse array of tens of thousands of protein-coding genes is carried out by RNA polymerase II. The control of this process is predominantly mediated by a network of thousands of sequence-specific DNA binding transcription factors that interpret the genetic regulatory information, such as in transcriptional enhancers and promoters, and transmit the appropriate response to the RNA polymerase II transcriptional machinery. This review will describe some early advances in the discovery and characterization of the sequence-specific DNA binding transcription factors as well as some of the properties of these regulatory proteins."
361,362,seeing is believing the bicoid morphogen gradient matures,60153.0,Seeing Is Believing - The Bicoid Morphogen Gradient Matures,"Although Cell has a long history of publishing some of the most significant advances in developmental biology, the back to back papers by Driever and Nüsslein-Volhard on the role of the Bicoid gradient in patterning the Drosophila embryo stand out as the first molecular demonstration of two of the longest standing concepts of the field, namely localized cytoplasmic determinants and morphogen gradients. Here we discuss the impact of this ground-breaking work and review recent results on bicoid mRNA localization and the dual role of Bicoid as a transcription and translation factor."
363,364,circular binary segmentation for the analysis of arraybased dna copy number data,62586.0,Circular binary segmentation for the analysis of array-based DNA copy number data.,"DNA sequence copy number is the number of copies of DNA at a region of a genome. Cancer progression often involves alterations in DNA copy number. Newly developed microarray technologies enable simultaneous measurement of copy number at thousands of sites in a genome. We have developed a modification of binary segmentation, which we call circular binary segmentation, to translate noisy intensity measurements into regions of equal copy number. The method is evaluated by simulation and is demonstrated on cell line data with known copy number alterations and on a breast cancer cell line data set."
364,365,gotermfinderopen source software for accessing gene ontology information and finding significantly enriched gene ontology terms associated with a list of genes,63158.0,GO::TermFinder-open source software for accessing Gene Ontology information and finding significantly enriched Gene Ontology terms associated with a list of genes,"SUMMARY: GO::TermFinder comprises a set of object-oriented Perl modules for accessing Gene Ontology (GO) information and evaluating and visualizing the collective annotation of a list of genes to GO terms. It can be used to draw conclusions from microarray and other biological data, calculating the statistical significance of each annotation. GO::TermFinder can be used on any system on which Perl can be run, either as a command line application, in single or batch mode, or as a web-based CGI script. AVAILABILITY: The full source code and documentation for GO::TermFinder are freely available from http://search.cpan.org/dist/GO-TermFinder/."
365,366,medliner an open source library in r for medline literature data mining,63175.0,MedlineR: an open source library in R for Medline literature data mining.,"SUMMARY: We describe an open source library written in the R programming language for Medline literature data mining. This MedlineR library includes programs to query Medline through the NCBI PubMed database; to construct the co-occurrence matrix; and to visualize the network topology of query terms. The open source nature of this library allows users to extend it freely in the statistical programming language of R. To demonstrate its utility, we have built an application to analyze term-association by using only ten lines of code. We provide MedlineR as a library foundation for bioinformaticians and statisticians to build more sophisticated literature data mining applications. AVAILABILITY: The library is available from http://dbsr.duke.edu/pub/MedlineR."
366,367,dagchainer a tool for mining segmental genome duplications and synteny,63180.0,DAGchainer: a tool for mining segmental genome duplications and synteny,"Summary: Given the positions of protein-coding genes along genomic sequence and probability values for protein alignments between genes, DAGchainer identifies chains of gene pairs sharing conserved order between genomic regions, by identifying paths through a directed acyclic graph (DAG). These chains of collinear gene pairs can represent segmentally duplicated regions and genes within a single genome or syntenic regions between related genomes. Automated mining of the Arabidopsis genome for segmental duplications illustrates the use of DAGchainer. 10.1093/bioinformatics/bth397"
367,368,discovering patterns to extract proteinprotein interactions from full texts,63187.0,Discovering patterns to extract protein-protein interactions from full texts.,"MOTIVATION: Although there are several databases storing protein-protein interactions, most such data still exist only in the scientific literature. They are scattered in scientific literature written in natural languages, defying data mining efforts. Much time and labor have to be spent on extracting protein pathways from litera-ture. Our aim is to develop a robust and powerful methodology to mine protein-protein interactions from biomedical texts. RESULTS: We present a novel and robust approach for extracting protein-protein interactions from literature. Our method uses a dynamic programming algorithm to compute distinguishing patterns by aligning relevant sentences and key verbs that describe protein inter-actions. A matching algorithm is designed to extract the interactions between proteins. Equipped only with a dictionary of protein names, our system achieves a recall rate of 80.0% and precision rate of 80.5%. AVAILABILITY: The program is available on request from the authors."
368,369,modeling interactome scalefree or geometric,63197.0,Modeling interactome: scale-free or geometric?,"MOTIVATION: Networks have been used to model many real-world phenomena to better understand the phenomena and to guide experiments in order to predict their behavior. Since incorrect models lead to incorrect predictions, it is vital to have as accurate a model as possible. As a result, new techniques and models for analyzing and modeling real-world networks have recently been introduced. RESULTS: One example of large and complex networks involves protein-protein interaction (PPI) networks. We analyze PPI networks of yeast S. cerevisiae and fruitfly D. melanogaster using a newly introduced measure of local network structure as well as the standardly used measures of global network structure. We examine the fit of four different network models, including Erd?s-R?nyi, scale-free, and geometric random network models, to these PPI networks with respect to the measures of local and global network structure. We demonstrate that the currently accepted scale-free model of PPI networks fails to fit the data in several respects and show that a random geometric model provides a much more accurate model of the PPI data. We hypothesize that only the noise in these networks is scale-free. Conclusions: We systematically evaluate how well different network models fit the PPI networks. We show that the structure of PPI networks is better modeled by a geometric random graph than by a scale-free model. CONTACT: Jurisica, I., Ontario Cancer Institute, Princess Margaret Hospital, University Health Network, Division of Cancer Informatics, 610 University Avenue, Toronto, ON, M5G 2M9, Canada. SUPPLEMENTARY INFORMATION: Supplementary information is available and submitted together with this manuscript."
369,370,blogs and wikis are valuable software tools for communication within research groups,65083.0,Blogs and Wikis Are Valuable Software Tools for Communication Within Research Groups,":&#8194; Appropriate software tools may improve communication and ease access to knowledge for research groups. A weblog is a website which contains periodic, chronologically ordered posts on a common webpage, whereas a wiki is hypertext-based collaborative software that enables documents to be authored collectively using a web browser. Although not primarily intended for use as an intranet-based collaborative knowledge warehouse, both blogs and wikis have the potential to offer all the features of complex and expensive IT solutions. These tools enable the team members to share knowledge simply and quickly&#8212;the collective knowledge base of&#160; the group can be efficiently managed and navigated."
370,371,a review of routing protocols for mobile ad hoc networks,69494.0,A review of routing protocols for mobile ad hoc networks,"The 1990s have seen a rapid growth of research interests in mobile ad hoc networking. The infrastructureless and the dynamic nature of these networks demands new set of networking strategies to be implemented in order to provide efficient end-to-end communication. This, along with the diverse application of these networks in many different scenarios such as battlefield and disaster recovery, have seen MANETs being researched by many different organisations and institutes. MANETs employ the traditional TCP/IP structure to provide end-to-end communication between nodes. However, due to their mobility and the limited resource in wireless networks, each layer in the TCP/IP model require redefinition or modifications to function efficiently in MANETs. One interesting research area in MANET is routing. Routing in the MANETs is a challenging task and has received a tremendous amount of attention from researches. This has led to development of many different routing protocols for MANETs, and each author of each proposed protocol argues that the strategy proposed provides an improvement over a number of different strategies considered in the literature for a given network scenario. Therefore, it is quite difficult to determine which protocols may perform best under a number of different network scenarios, such as increasing node density and traffic. In this paper, we provide an overview of a wide range of routing protocols proposed in the literature. We also provide a performance comparison of all routing protocols and suggest which protocols may perform best in large networks."
372,373,a short survey of noncommutative geometry,70696.0,A Short Survey of Noncommutative Geometry,"We give a survey of selected topics in noncommutative geometry, with some emphasis on those directly related to physics, including our recent work with Dirk Kreimer on renormalization and the Riemann-Hilbert problem. We discuss at length two issues. The first is the relevance of the paradigm of geometric space, based on spectral considerations, which is central in the theory. As a simple illustration of the spectral formulation of geometry in the ordinary commutative case, we give a polynomial equation for geometries on the four dimensional sphere with fixed volume. The equation involves an idempotent e, playing the role of the instanton, and the Dirac operator D. It expresses the gamma five matrix as the pairing between the operator theoretic chern characters of e and D. It is of degree five in the idempotent and four in the Dirac operator which only appears through its commutant with the idempotent. It determines both the sphere and all its metrics with fixed volume form.   We also show using the noncommutative analogue of the Polyakov action, how to obtain the noncommutative metric (in spectral form) on the noncommutative tori from the formal naive metric. We conclude on some questions related to string theory."
377,378,subtyping recursive types,70810.0,Subtyping Recursive Types,"We investigate the interactions of subtyping and recursive types, in a simply typed &lgr;-calculus. The two fundamental questions here are whether two (recursive)types are in the subtype relation and whether a term has a type. To address the first question, we relate various definitions of type equivalence and subtyping that are induced by a model, an ordering on infinite trees, an algorithm, and a set of type rules. We show soundness and completeness among the rules, the algorithm, and the tree semantics. We also prove soundness and a restricted form of completeness for the model. To address the second question, we show that to every pair of types in the subtype relation we can associate a term whose denotation is the uniquely determined coercion map between the two types. Moreover, we derive an algorithm that, when given a term with implicit coercions, can infer its least type whenever possible."
378,379,theorems for free,70811.0,Theorems for Free!,"From the type of a polymorphic function we can derive a theorem that it satisfies. Every function of the same type satisfies the same theorem. This provides a free source of useful theorems, courtesy of Reynolds' abstraction theorem for the polymorphic lambda calculus.  1 Introduction  Write down the definition of a polymorphic function on a piece of paper. Tell me its type, but be careful not to let me see the function's definition. I will tell you a theorem that the function satisfies. The..."
380,381,power laws pareto distributions and zipfs law,70828.0,"Power laws, Pareto distributions and Zipf's law","When the probability of measuring a particular value of some quantity varies inversely as a power of that value, the quantity is said to follow a power law, also known variously as Zipf's law or the Pareto distribution. Power laws appear widely in physics, biology, earth and planetary sciences, economics and finance, computer science, demography and the social sciences. For instance, the distributions of the sizes of cities, earthquakes, forest fires, solar flares, moon craters and people's personal fortunes all appear to follow power laws. The origin of power-law behaviour has been a topic of debate in the scientific community for more than a century. Here we review some of the empirical evidence for the existence of power-law forms and the theories proposed to explain them."
382,383,adoption and focus practical linear types for imperative programming,70952.0,Adoption and focus: practical linear types for imperative programming,"A type system with linearity is useful for checking software protocols and resource management at compile time. Linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. The hard division between linear and nonlinear types forces the programmer to make a trade-off between checking a protocol on an object and aliasing the object. Most onerous is the restriction that any type with a linear component must itself be linear. Because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. We propose a new type system that reduces these restrictions with the adoption and focus constructs. Adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. A programmer can alias data structures that point to linear objects and use focus for safe access to those objects. We discuss how we implemented these ideas in the Vault programming language."
384,385,a taste of linear logic,70958.0,A Taste of Linear Logic,". This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of `Propositions as Types'. The presentation of linear logic is simplified by basing it on the Logic of Unity. An application to the array update problem is briefly discussed. 1 Introduction Some of the best things in life are free; and some are not. Truth is free. Having proved a theorem, you may use this proof as many..."
385,386,once upon a type,70960.0,Once upon a type,"A number of useful optimisations are enabled if we can determine when a value is accessed at most once. We extend the Hindley-Milner type system with uses, yielding a type-inference based program analysis which determines when values are accessed at most once. Our analysis can handle higher-order functions and data structures, and admits principal types for terms. Unlike previous analyses, we prove our analysis sound with respect to call-by-need reduction. Call-by-name reduction does not..."
386,387,social presence in distributed group environments the role of social identity,71181.0,Social presence in distributed group environments: the role of social identity,"This paper argues that to achieve social presence in a distributed environment, it is not necessary to emulate face-to-face conditions of increased cues to the interpersonal. Rather, it is argued, that a sense of belongingness to the group, or perceptual immersion in the group, can be realised through the creation of a shared social identity between group members. From this perspective, social presence is a function of the cognitive representation of the group by group members and not the interpersonal bonds between group members. Furthermore, specific design features and characteristics of the distributed learning environment can be utilised to achieve and maintain this shared group identity. This approach, encapsulated by the SIDE model, is discussed and supported by two case studies of distributed students, each consisting of 10 groups, collaborating for a period of 5 weeks on group projects."
388,389,autonomous gossiping a selforganizing epidemic algorithm for selective,71363.0,Autonomous Gossiping: A self-organizing epidemic algorithm for selective,"We introduce autonomous gossiping (A/G), a new genre epidemic algorithm for selective dissemination of information in contrast to previous usage of epidemic algorithms which flood the whole network. A/G is a paradigm which suits well in a mobile ad-hoc networking (MANET) environment because it does not require any infrastructure or middleware like multicast tree and (un)subscription maintenance for publish/subscribe, but uses ecological and economic principles in a self-organizing manner in..."
389,390,romeo studies the impact of copyright ownership on academic author selfarchiving,71457.0,RoMEO studies 1: the impact of copyright ownership on academic author self-archiving,"			This is the first of a series of studies emanating from the UK JISC-funded RoMEO Project (Rights Metadata for Open-archiving) which investigated the IPR issues relating to academic author self-archiving of research papers. It considers the claims for copyright ownership in research papers by universities, academics, and publishers by drawing on the literature, a survey of 542 academic authors and an analysis of 80 journal publisher copyright transfer agreements. The paper concludes that self-archiving is not best supported by copyright transfer to publishers. It recommends that universities assert their interest in copyright ownership in the long term, that academics retain rights in the short term, and that publishers consider new ways of protecting the value they add through journal publishing."
392,393,continuously adaptive continuous queries over streams,71479.0,Continuously Adaptive Continuous Queries over Streams,"We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators --- both selections and join state --- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches."
393,394,adaptive query processing a survey,71485.0,Adaptive query processing: A survey," In wide-area database systems, which may be running on unpredictable and volatile environments (such as computational grids), it is difficult to produce efficient database query plans based on information available solely at compile time. A solution to this problem is to exploit information that becomes available at query runtime and adapt the query plan to changing conditions during execution. This paper presents a survey on adaptive query processing techniques, examining the opportunities they offer to modify a plan dynamically and classifying them into categories according to the problem they focus on, their objectives, the nature of feedback they collect from the environment, the frequency at which they can adapt, their implementation environment and which component is responsible for taking the adaptation decisions."
394,395,aurora a new model and architecture for data stream management,71486.0,Aurora: a new model and architecture for data stream management,"Abstract.This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators."
395,396,query languages and data models for database,71495.0,Query Languages and Data Models for Database,"We study the fundamental limitations of relational  algebra (RA) and SQL in supporting  sequence and stream queries, and present effective  query language and data model enrichments  to deal with them. We begin by observing  the well-known limitations of SQL in  application domains which are important for  data streams, such as sequence queries and  data mining. Then we present a formal proof  that, for continuous queries on data streams,  SQL su#ers from additional expressive power  problems...."
396,397,queryfree news search,71558.0,Queryfree news search,"Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics."
397,398,random early detection gateways for congestion avoidance,71724.0,Random early detection gateways for congestion avoidance,"This paper presents Random Early Detection (RED) gateways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by computing the average queue size. The gateway could notify connections of congestion either by dropping packets arriving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold, the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a function of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection's share of the bandwidth through the gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP. The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connections decreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways."
398,399,a measurement study of the bittorrent peertopeer filesharing system,71746.0,A measurement study of the bittorrent peer-to-peer file-sharing system,"P2P systems for sharing content have become very popular over the last few  years. However, despite the increasing attention of both the research community  and large numbers of users, the actual behavior of these systems over prolonged periods  of time is still poorly understood. This paper presents a detailed measurement  study over a period of eight months of BitTorrent/Suprnova, a P2P file-sharing  system that is quickly gaining in popularity. In particular, we show measurement  results of..."
399,400,analyzing peertopeer traffic across large networks,71747.0,Analyzing peer-to-peer traffic across large networks,"The use of peer-to-peer (P2P) applications is growing dramatically, particularly for sharing large video/audio files and software. In this paper, we analyze P2P traffic by measuring flow-level information collected at multiple border routers across a large ISP network, and report our investigation of three popular P2P systems-FastTrack, Gnutella, and Direct-Connect. We characterize the P2P traffic observed at a single ISP and its impact on the underlying network. We observe very skewed distribution in the traffic across the network at different levels of spatial aggregation (IP, prefix, AS). All three P2P systems exhibit significant dynamics at short time scale and particularly at the IP address level. Still, the fraction of P2P traffic contributed by each prefix is more stable than the corresponding distribution of either Web traffic or overall traffic. The high volume and good stability properties of P2P traffic suggests that the P2P workload is a good candidate for being managed via application-specific layer-3 traffic engineering in an ISP's network."
400,401,the smallworld phenomenon an algorithmic perspective,71749.0,The Small-World Phenomenon: An Algorithmic Perspective,"Long a matter of folklore, the &amp;quot;small-world phenomenon &amp;quot;--- the principle that we are all linked by short chains of acquaintances--- was inaugurated as an area of experimental study in the social sciences through the pioneering work of Stanley Milgram in the 1960&#039;s. This work was among the first to make the phenomenon quantitative, allowing people to speak of the &amp;quot;six degrees of separation &amp;quot; between any two people in the United States. Since then, a number of network models have been proposed as frameworks in which to study the problem analytically. One of the most refined of these models was formulated in recent work of Watts and Strogatz; their framework provided compelling evidence that the small-world phenomenon is pervasive in a range of networks arising in nature and technology, and a fundamental ingredient in the evolution of the World Wide Web. But existing models are insu#cient to explain the striking algorithmic component of Milgram&#039;s original findings: that individuals using local information are collectively very e#ective at actually constructing short paths between two points in a social network. Although recently proposed network models are rich in short paths, we prove that no decentralized algorithm, operating with local information only, can construct short paths in these networks with non-negligible probability. We then define an infinite family of network models that naturally generalizes the Watts-Strogatz model, and show that for one of these models, there is a decentralized algorithm capable of finding short paths with high probability. More generally, we provide a strong characterization of this family of network models, showing that there is in fact a unique model within the family for which decentralized algorithms are e#ective."
401,402,purely functional data structures,71751.0,Purely functional data structures,"Most books on data structures assume an imperative language such as C or C++. However, data structures for these languages do not always translate well to functional languages such as Standard ML, Haskell, or Scheme. This book describes data structures from the point of view of functional languages, with examples, and presents design techniques that allow programmers to develop their own functional data structures. The author includes both classical data structures, such as red-black trees and binomial queues, and a host of new data structures developed exclusively for functional languages. All source code is given in Standard ML and Haskell, and most of the programs are easily adaptable to other functional languages. This handy reference for professional programmers working with functional languages can also be used as a tutorial or for self-study."
402,403,modular domain specific languages and tools,71752.0,Modular Domain Specific Languages and Tools,"A domain specific language (DSL) allows one to develop software for a particular application domain quickly and effectively, yielding programs that are easy to understand, reason about, and maintain. On the other hand, there may be a significant overhead in creating the infrastructure needed to support a DSL. To solve this problem, a methodology is described for building domain specific embedded languages (DSELs), in which a DSL is designed within an existing, higher-order and typed, programming language such as Haskell or ML. In addition, techniques are described for building modular interpreters and tools for DSELs. The resulting methodology facilitates reuse of syntax, semantics, implementation code, software tools, as well as look-and-feel."
404,405,compiling embedded languages,71762.0,Compiling Embedded Languages,"Functional languages are particularly well-suited to the implementation of interpreters for domain-specific embedded languages (DSELs). We describe an implemented technique for producing optimizing compilers for DSELs, based on Kamin's idea of DSELs for program generation. The technique uses a data type of syntax for basic types, a set of smart constructors that perform rewriting over those types, some code motion transformations, and a back-end code generator. Domain-specific optimization..."
405,406,notable design patterns for domainspecific languages,71764.0,Notable design patterns for domain-specific languages,"The realisation of domain-specific languages ( s) differs in fundamental ways from that of traditional programming languages. We describe eight recurring patterns that we have identified as being used for design and implementation. Existing languages can be extended, restricted, partially used, or become hosts for s. Simple s can be implemented by lexical processing. In addition, s can be used to create front-ends to existing systems or to express complicated data structures. Finally, s can be combined using process pipelines. The patterns described form a pattern language that can be used as a building block for a systematic view of the software development process involving s."
408,409,predicate dispatching a unified theory of dispatch,71834.0,Predicate Dispatching: A Unified Theory of Dispatch,"Predicate dispatching generalizes previous method dispatch  mechanisms by permitting arbitrary predicates to control method applicability  and by using logical implication between predicates as the  overriding relationship. The method selected to handle a message send  can depend not just on the classes of the arguments, as in ordinary  object-oriented dispatch, but also on the classes of subcomponents, on  an argument's state, and on relationships between objects. This simple  mechanism..."
409,410,concurrent haskell,71842.0,Concurrent Haskell,"Some applications are most easily expressed in a programming language that supports concurrency, notably interactive and distributed systems. We propose extensions to the purely-functional language Haskell that allows it to express explicitly concurrent applications; we call the resulting language Concurrent Haskell. The resulting system appears to be both expressive and efficient, and we give a number of examples of useful abstractions that can be built from our primitives. We have developed a freely-available implementation of Concurrent Haskell, and are now using it as a substrate for a graphical user interface toolkit. 1"
411,412,the shifting sands of open access publishing a publishers view,72138.0,"The Shifting Sands of Open Access Publishing, a Publisher's View","This paper sets Open Access (OA) publishing in the context of today's scientific, technical, and medical (STM) publishing trends. Four areas are covered: (a) a brief overview of STM publishing and its value today; (b) OA's place in the industry; (c) the underlying economics of OA, particularly its author-pays model; and (d) directions in moving towards “universal access” to STM information, where both researchers and the public have access to the scientific information they need."
413,414,delivery management and access model for eprints and open access journals,72142.0,"Delivery, Management and Access Model for E_prints and Open Access Journals","A study conducted for the (United Kingdom) Joint Information Systems Committee reviewed possible models for implementing Open Access to research reports in institutional archives and Open Access journals. The conclusion was that a ""harvesting model,"" in which full texts reside on the original servers but metadata are harvested, held, and enhanced by a central service, was preferable to either a centralized national service or a completely decentralized service for the UK. The study included issues of populating institutional archives (IAs) and some form of mandatory archiving for publicly funded research results to obtain a critical mass of Open Access material in such a system."
415,416,the accessimpact problem and the green and gold roads to open access,72145.0,The Access/Impact Problem and the Green and Gold Roads to Open Access,"The research access/impact problem arises because journal articles are not accessible to all of their would-be users; hence, they are losing potential research impact. The solution is to make all articles Open Access {(OA;} i.e., accessible online, free for all). {OA} articles have significantly higher citation impact than {non-OA} articles. There are two roads to {OA:} the ""golden"" road (publish your article in an {OA} journal) and the ""green"" road (publish your article in a {non-OA} journal but also self-archive it in an {OA} archive). Only 5% of journals are gold, but over 90% are already green (i.e., they have given their authors the green light to self-archive); yet only about 10-20% of articles have been self-archived. To reach 100% {OA,} self-archiving needs to be mandated by researchers' employers and funders, as the United Kingdom and the United States have recently recommended, and universities need to implement that mandate."
416,417,hourly analysis of a very large topically categorized web query log,72195.0,Hourly analysis of a very large topically categorized web query log,"We review a query log of hundreds of millions of queries that constitute the total query traffic for an entire week of a general-purpose commercial web search service. Previously, query logs have been studied from a single, cumulative view. In contrast, our analysis shows changes in popularity and uniqueness of topically categorized queries across the hours of the day. We examine query traffic on an hourly basis by matching it against lists of queries that have been topically pre-categorized by human editors. This represents 13% of the query traffic. We show that query traffic from particular topical categories differs both from the query stream as a whole and from other categories. This analysis provides valuable insight for improving retrieval effectiveness and efficiency. It is also relevant to the development of enhanced query disambiguation, routing, and caching algorithms."
417,418,probabilistic query expansion using query logs,72196.0,Probabilistic query expansion using query logs,"Query expansion has long been suggested as an effective way to resolve the short query and word mismatching problems. A number of query expansion methods have been proposed in traditional information retrieval. However, these previous methods do not take into account the specific characteristics of web searching; in particular, of the availability of large amount of user interaction information recorded in the web query logs. In this study, we propose a new method for query expansion based on query logs. The central idea is to extract probabilistic correlations between query terms and document terms by analyzing query logs. These correlations are then used to select high-quality expansion terms for new queries. The experimental results show that our log-based probabilistic query expansion method can greatly improve the search performance and has several advantages over other existing methods."
418,419,assessing computational tools for the discovery of transcription factor binding sites,72879.0,Assessing computational tools for the discovery of transcription factor binding sites,"The prediction of regulatory elements is a problem where computational methods offer great hope. Over the past few years, numerous tools have become available for this task. The purpose of the current assessment is twofold: to provide some guidance to users regarding the accuracy of currently available tools in various settings, and to provide a benchmark of data sets for assessing future tools."
419,420,document clustering by concept factorization,73383.0,Document clustering by concept factorization,"In this paper, we propose a new data clustering method called concept factorization that models each concept as a linear combination of the data points, and each data point as a linear combination of the concepts. With this model, the data clustering task is accomplished by computing the two sets of linear coefficients, and this linear coefficients computation is carried out by finding the non-negative solution that minimizes the reconstruction error of the data points. The cluster label of each data point can be easily derived from the obtained linear coefficients. This method differs from the method of clustering based on non-negative matrix factorization (NMF) in that it can be applied to data containing negative values and the method can be implemented in the kernel space. Our experimental results show that the proposed data clustering method and its variations performs best among 11 algorithms and their variations that we have evaluated on both TDT2 and Reuters-21578 corpus. In addition to its good performance, the new method also has the merit in its easy and reliable derivation of the clustering results"
